<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>Thoughts from the Café</title>
    <link href="http://stappit.github.io/atom.xml" rel="self" />
    <link href="http://stappit.github.io" />
    <id>http://stappit.github.io/atom.xml</id>
    <author>
        <name>Brian</name>
        <email>ha@hahaha.com</email>
    </author>
    <updated>2018-08-27T00:00:00Z</updated>
    <entry>
    <title>BDA3 Chapter 2 Exercise 8</title>
    <link href="http://stappit.github.io/posts/bda3/chapter_02_exercise_08.html" />
    <id>http://stappit.github.io/posts/bda3/chapter_02_exercise_08.html</id>
    <published>2018-08-27T00:00:00Z</published>
    <updated>2018-08-27T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 class="post-title">BDA3 Chapter 2 Exercise 8</h1>

<div class="card post-meta border-0">
  <div class="card-body post-meta">
    <p class="card-text text-muted text-left">
    Posted on August 27, 2018  by Brian </br>
     Tags: <a href="/tags/bda%20chapter%202.html">bda chapter 2</a>, <a href="/tags/bda.html">bda</a>, <a href="/tags/solutions.html">solutions</a>, <a href="/tags/bayes.html">bayes</a>, <a href="/tags/normal.html">normal</a>, <a href="/tags/posterior%20predictive.html">posterior predictive</a> </br>
     Category: <a href="/categories/bda3.html">bda3</a> 
    </p>
  </div>
</div>

<p>Here’s my solution to exercise 8, chapter 2, of <a href="https://andrewgelman.com/">Gelman’s</a> <em>Bayesian Data Analysis</em> (BDA), 3rd edition. There are <a href="http://www.stat.columbia.edu/~gelman/book/solutions.pdf">solutions</a> to some of the exercises on the <a href="http://www.stat.columbia.edu/~gelman/book/">book’s webpage</a>.</p>
<!--more-->
<div style="display:none">
<p class="mathjaxWide"><span class="math inline">\(\DeclareMathOperator{\dbinomial}{binomial} \DeclareMathOperator{\dbern}{Bernoulli} \DeclareMathOperator{\dnorm}{normal} \DeclareMathOperator{\dgamma}{gamma} \DeclareMathOperator{\invlogit}{invlogit} \DeclareMathOperator{\logit}{logit} \DeclareMathOperator{\dbeta}{beta}\)</span></p>
</div>
<p>With prior <span class="math inline">\(\theta \sim \dnorm(180, 40)\)</span>, sampling distribution <span class="math inline">\(y \mid \theta \sim \dnorm(\theta, 20)\)</span>, and <span class="math inline">\(n\)</span> sampled students with average weight <span class="math inline">\(\bar y = 150\)</span>, it follows from 2.11 that the posterior mean is</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
  \mu
  :=
  \mathbb E(\theta \mid \bar y) 
  &amp;=
  \frac{\frac{180}{1600} + \frac{150n}{400}}{\frac{1}{1600} + \frac{n}{400}} 
  \\
  &amp;=
  \frac{60(3 + 10n)}{1600} \cdot \frac{1600}{1 + 4n}
  \\
  &amp;=
  \frac{60(3 + 10n)}{1 + 4n}
  \\
  1 / \sigma^2 
  :=
  1 / \mathbb V (\theta \mid \bar y)
  &amp;=
  \frac{1}{1600} + \frac{n}{400}
  \\
  &amp;=
  \frac{1 + 4n}{1600}
  .
\end{align}
\]</span></p>
<p>So <span class="math inline">\(\theta \mid \bar y ~ \dnorm \left( \frac{60(3 + 10n)}{1 + 4n}, \frac{40}{\sqrt{1 + 4n}} \right)\)</span>.</p>
<p>It follows from the calculations shown in the book that the posterior predictive distribution is <span class="math inline">\(\tilde y \mid y \sim \dnorm(\mu, \sigma + 20)\)</span>.</p>
<p>We can obtain 95% posterior intervals as follows.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mu &lt;-<span class="st"> </span><span class="cf">function</span>(n) <span class="dv">60</span> <span class="op">*</span><span class="st"> </span>(<span class="dv">3</span> <span class="op">+</span><span class="st"> </span><span class="dv">10</span> <span class="op">*</span><span class="st"> </span>n) <span class="op">/</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="dv">4</span> <span class="op">*</span><span class="st"> </span>n)
sigma &lt;-<span class="st"> </span><span class="cf">function</span>(n) <span class="dv">40</span> <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="dv">4</span> <span class="op">*</span><span class="st"> </span>n)

percentiles &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.05</span>, <span class="fl">0.95</span>)

theta_posterior_interval &lt;-<span class="st"> </span><span class="kw">qnorm</span>(percentiles, <span class="kw">mu</span>(<span class="dv">10</span>), <span class="kw">sigma</span>(<span class="dv">10</span>))
y_posterior_interval &lt;-<span class="st"> </span><span class="kw">qnorm</span>(percentiles, <span class="kw">mu</span>(<span class="dv">10</span>), <span class="kw">sigma</span>(<span class="dv">10</span>) <span class="op">+</span><span class="st"> </span><span class="dv">20</span>)</code></pre></div>
<p>With a sample of size of 10, we get θ ϵ [140.5, 161] and <span class="math inline">\(\tilde y\)</span> ϵ [107.6, 193.9].</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">theta_posterior_interval &lt;-<span class="st"> </span><span class="kw">qnorm</span>(percentiles, <span class="kw">mu</span>(<span class="dv">100</span>), <span class="kw">sigma</span>(<span class="dv">100</span>))
y_posterior_interval &lt;-<span class="st"> </span><span class="kw">qnorm</span>(percentiles, <span class="kw">mu</span>(<span class="dv">100</span>), <span class="kw">sigma</span>(<span class="dv">100</span>) <span class="op">+</span><span class="st"> </span><span class="dv">20</span>)</code></pre></div>
<p>With a sample of size of 100, we get θ ϵ [146.8, 153.4] and <span class="math inline">\(\tilde y\)</span> ϵ [113.9, 186.3].</p>

<div id="disqus_thread"></div>
<script>
/**
 *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
 *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
 */
/*
   var disqus_config = function () {
   this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
   this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
   };
 */
(function() {  // DON'T EDIT BELOW THIS LINE
 var d = document, s = d.createElement('script');

 s.src = '//stappit-github-io.disqus.com/embed.js';

 s.setAttribute('data-timestamp', +new Date());
 (d.head || d.body).appendChild(s);
 })();
</script>

<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

<script id="dsq-count-scr" src="//stappit-github-io.disqus.com/count.js" async></script>
]]></summary>
</entry>
<entry>
    <title>BDA3 Chapter 2 Exercise 7</title>
    <link href="http://stappit.github.io/posts/bda3/chapter_02_exercise_07.html" />
    <id>http://stappit.github.io/posts/bda3/chapter_02_exercise_07.html</id>
    <published>2018-08-26T00:00:00Z</published>
    <updated>2018-08-26T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 class="post-title">BDA3 Chapter 2 Exercise 7</h1>

<div class="card post-meta border-0">
  <div class="card-body post-meta">
    <p class="card-text text-muted text-left">
    Posted on August 26, 2018  by Brian </br>
     Tags: <a href="/tags/bda%20chapter%202.html">bda chapter 2</a>, <a href="/tags/bda.html">bda</a>, <a href="/tags/solutions.html">solutions</a>, <a href="/tags/bayes.html">bayes</a>, <a href="/tags/binomial.html">binomial</a>, <a href="/tags/natural%20parameter.html">natural parameter</a>, <a href="/tags/exponential%20family.html">exponential family</a>, <a href="/tags/improper%20prior.html">improper prior</a> </br>
     Category: <a href="/categories/bda3.html">bda3</a> 
    </p>
  </div>
</div>

<p>Here’s my solution to exercise 7, chapter 2, of <a href="https://andrewgelman.com/">Gelman’s</a> <em>Bayesian Data Analysis</em> (BDA), 3rd edition. There are <a href="http://www.stat.columbia.edu/~gelman/book/solutions.pdf">solutions</a> to some of the exercises on the <a href="http://www.stat.columbia.edu/~gelman/book/">book’s webpage</a>.</p>
<!--more-->
<div style="display:none">
<p class="mathjaxWide"><span class="math inline">\(\DeclareMathOperator{\dbinomial}{binomial} \DeclareMathOperator{\dbern}{Bernoulli} \DeclareMathOperator{\dgamma}{gamma} \DeclareMathOperator{\invlogit}{invlogit} \DeclareMathOperator{\logit}{logit} \DeclareMathOperator{\dbeta}{beta}\)</span></p>
</div>
<p>We show that a uniform prior on the natural parameter of a binomial model implies an improper prior under a different parameterisation.</p>
<p>The binomial likelihood can be written as a member of the exponential family as</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
  \dbinomial(y \mid \theta)
  &amp;=
  \binom{n}{y} \theta^y (1 - \theta)^{n - y}
  \\
  &amp;=
  \binom{n}{y} \cdot (1 - \theta)^n \cdot \exp \left(y \log \left(\frac{\theta}{1 - \theta}\right)\right)
  \\
  &amp;=
  f(y) \cdot g(\theta) \cdot \exp (\phi(\theta) \cdot u(y))
  ,
\end{align}
\]</span></p>
<p>where <span class="math inline">\(\phi(\theta) := \log \frac{\theta}{1 - \theta}\)</span>, <span class="math inline">\(u(y) := y\)</span>, <span class="math inline">\(g(\theta) := (1 - \theta)^n\)</span>. Suppose the natural parameter <span class="math inline">\(\phi \sim \dbeta(1, 1)\)</span> is uniformly distributed. Then the distribution of <span class="math inline">\(\theta\)</span> is</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
  p(\theta) 
  &amp;\propto
  p(\phi) \cdot \vert \invlogit^\prime (\phi) \vert^{-1}
  \\
  &amp;=
  \left \vert \frac{1}{1 + \exp(-\phi)}^\prime \right\vert^{-1}
  \\
  &amp;=
  \left \vert \frac{1}{\left(1 + \exp(-\phi)\right)^2} \cdot \exp(-\phi) \right\vert^{-1}
  \\
  &amp;=
  \frac{1 + 2 \exp(-\phi) + \exp(-2\phi)}{\exp(-\phi)}
  \\
  &amp;=
  \exp(\phi) + 2 + \exp(-\phi)
  \\
  &amp;=
  \frac{\theta}{1 - \theta} + 2 + \frac{1 - \theta}{\theta}
  \\
  &amp;=
  \frac{\theta^2 + 2\theta(1 - \theta) + (1 - \theta)^2}{\theta(1 - \theta)}
  \\
  &amp;=
  \frac{1}{\theta(1 - \theta)}
  \\
  &amp;=
  \theta^{-1}(1 - \theta)^{-1}
  \qquad \square
\end{align}
\]</span></p>
<p>This is an improper distribution on <span class="math inline">\(\theta\)</span> because</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
  \int_0^1 \frac{1}{\theta(1 - \theta)}
  &amp;\ge
  \int_0^1 \frac{1}{\theta}
  \\
  &amp;=
  \log\theta \vert_0^1
  \\
  &amp;=
  \infty.
\end{align}
\]</span></p>
<p>When <span class="math inline">\(y = 0\)</span>, then the posterior distribution is <span class="math inline">\(p(\theta \mid y = 0) \propto (1 - \theta)^{n - 1}\theta^{-1}\)</span>. When <span class="math inline">\(y = n\)</span>, then the posterior distribution is <span class="math inline">\(p(\theta \mid y = n) \propto \theta^{n-1}(1 - \theta)^{-1}\)</span>. These two cases are equivalent by the change of variable <span class="math inline">\(\theta \mapsto 1 - \theta\)</span>.</p>
<p>We show that the distribution is improper for <span class="math inline">\(y = 0\)</span> by induction. The case <span class="math inline">\(n = 0\)</span> is shown above (for the prior). Assume the distribution is improper for any integer <span class="math inline">\(k &lt; n\)</span>. Then using integration by parts yields</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
\int_0^1 \theta^{- 1}(1 - \theta)^{n - 1} d\theta
&amp;=
\int_0^1 \theta^{- 1}(1 - \theta)^{n - 2} \cdot (1 - \theta)d\theta
\\
&amp;=
\left[(1 - \theta)^{n-2}(1 - \frac{\theta}{2}) \right]_0^1
+
\int_0^1 \frac{(1 - \theta)^{n - 2}}{\theta}
+
(n-2)(1 - \theta)^{n-3}
-
\frac{(1 - \theta)^{n-2}}{2}
-
(n - 2)\theta\frac{(1 - \theta)^{n-3}}{2}
d\theta
\\
&amp;=
c
+
\int_0^1 \frac{(1 - \theta)^{n - 2}}{\theta} d\theta,
\end{align}
\]</span></p>
<p>where <span class="math inline">\(c &lt; \infty\)</span>. By the induction hypothesis, the integral on the last line is <span class="math inline">\(\infty\)</span>. Therefore, the distribution is also improper for <span class="math inline">\(n\)</span>.</p>

<div id="disqus_thread"></div>
<script>
/**
 *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
 *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
 */
/*
   var disqus_config = function () {
   this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
   this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
   };
 */
(function() {  // DON'T EDIT BELOW THIS LINE
 var d = document, s = d.createElement('script');

 s.src = '//stappit-github-io.disqus.com/embed.js';

 s.setAttribute('data-timestamp', +new Date());
 (d.head || d.body).appendChild(s);
 })();
</script>

<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

<script id="dsq-count-scr" src="//stappit-github-io.disqus.com/count.js" async></script>
]]></summary>
</entry>
<entry>
    <title>BDA3 Chapter 2 Exercise 6</title>
    <link href="http://stappit.github.io/posts/bda3/chapter_02_exercise_06.html" />
    <id>http://stappit.github.io/posts/bda3/chapter_02_exercise_06.html</id>
    <published>2018-08-25T00:00:00Z</published>
    <updated>2018-08-25T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 class="post-title">BDA3 Chapter 2 Exercise 6</h1>

<div class="card post-meta border-0">
  <div class="card-body post-meta">
    <p class="card-text text-muted text-left">
    Posted on August 25, 2018  by Brian </br>
     Tags: <a href="/tags/bda%20chapter%202.html">bda chapter 2</a>, <a href="/tags/bda.html">bda</a>, <a href="/tags/solutions.html">solutions</a>, <a href="/tags/bayes.html">bayes</a>, <a href="/tags/poisson.html">poisson</a>, <a href="/tags/gamma.html">gamma</a>, <a href="/tags/negative%20binomial.html">negative binomial</a> </br>
     Category: <a href="/categories/bda3.html">bda3</a> 
    </p>
  </div>
</div>

<p>Here’s my solution to exercise 6, chapter 2, of <a href="https://andrewgelman.com/">Gelman’s</a> <em>Bayesian Data Analysis</em> (BDA), 3rd edition. There are <a href="http://www.stat.columbia.edu/~gelman/book/solutions.pdf">solutions</a> to some of the exercises on the <a href="http://www.stat.columbia.edu/~gelman/book/">book’s webpage</a>.</p>
<!--more-->
<div style="display:none">
<p class="mathjaxWide"><span class="math inline">\(\DeclareMathOperator{\dbinomial}{binomial} \DeclareMathOperator{\dbern}{Bernoulli} \DeclareMathOperator{\dgamma}{gamma} \DeclareMathOperator{\dpois}{Poisson} \DeclareMathOperator{\dbeta}{beta}\)</span></p>
</div>
<p>Considering the negative binomial variable <span class="math inline">\(y\)</span> as a gamma-Poisson variable, we derive expressions for the mean and variance.</p>
<p>From equation 1.6, <span class="math inline">\(\mathbb E (y) = \mathbb E (\mathbb E(y \mid \theta))\)</span>. Since <span class="math inline">\(y \mid \theta \sim \dpois(10n\theta)\)</span>, it follows that <span class="math inline">\(\mathbb E (y \mid \theta) = 10n\theta\)</span>. The rate <span class="math inline">\(\theta \sim \dgamma(\alpha, \beta)\)</span> so <span class="math inline">\(\mathbb E(\theta) = \frac{\alpha}{\beta}\)</span>. Thus, <span class="math inline">\(\mathbb E(y) = 10n\mathbb E(\theta) = 10n \frac{\alpha}{\beta}\)</span>.</p>
<p>We also have <span class="math inline">\(\mathbb V(\theta) = \frac{\alpha}{\beta^2}\)</span> since <span class="math inline">\(\theta \sim \dgamma(\alpha, \beta)\)</span>, and <span class="math inline">\(\mathbb V(y \mid \theta) = 10n\theta\)</span> since <span class="math inline">\(y \mid \theta \sim \dpois(10n\theta)\)</span>. Thus,</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
  \mathbb V (y) 
  &amp;= 
  \mathbb E(\mathbb V(y \mid \theta)) + \mathbb V (\mathbb E (y \mid \theta)) 
  \\
  &amp;=
  \mathbb E(10n\theta) + \mathbb V (10n\theta)
  \\
  &amp;=
  10n\frac{\alpha}{\beta} + (10n)^2\frac{\alpha}{\beta^2}
  \qquad \square
\end{align}
\]</span></p>

<div id="disqus_thread"></div>
<script>
/**
 *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
 *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
 */
/*
   var disqus_config = function () {
   this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
   this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
   };
 */
(function() {  // DON'T EDIT BELOW THIS LINE
 var d = document, s = d.createElement('script');

 s.src = '//stappit-github-io.disqus.com/embed.js';

 s.setAttribute('data-timestamp', +new Date());
 (d.head || d.body).appendChild(s);
 })();
</script>

<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

<script id="dsq-count-scr" src="//stappit-github-io.disqus.com/count.js" async></script>
]]></summary>
</entry>
<entry>
    <title>BDA3 Chapter 2 Exercise 5</title>
    <link href="http://stappit.github.io/posts/bda3/chapter_02_exercise_05.html" />
    <id>http://stappit.github.io/posts/bda3/chapter_02_exercise_05.html</id>
    <published>2018-08-24T00:00:00Z</published>
    <updated>2018-08-24T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 class="post-title">BDA3 Chapter 2 Exercise 5</h1>

<div class="card post-meta border-0">
  <div class="card-body post-meta">
    <p class="card-text text-muted text-left">
    Posted on August 24, 2018  by Brian </br>
     Tags: <a href="/tags/bayes.html">bayes</a>, <a href="/tags/solutions.html">solutions</a>, <a href="/tags/bda%20chapter%202.html">bda chapter 2</a>, <a href="/tags/bda.html">bda</a>, <a href="/tags/beta.html">beta</a>, <a href="/tags/binomial.html">binomial</a>, <a href="/tags/beta-binomial.html">beta-binomial</a>, <a href="/tags/variance.html">variance</a> </br>
     Category: <a href="/categories/bda3.html">bda3</a> 
    </p>
  </div>
</div>

<p>Here’s my solution to exercise 5, chapter 2, of <a href="https://andrewgelman.com/">Gelman’s</a> <em>Bayesian Data Analysis</em> (BDA), 3rd edition. There are <a href="http://www.stat.columbia.edu/~gelman/book/solutions.pdf">solutions</a> to some of the exercises on the <a href="http://www.stat.columbia.edu/~gelman/book/">book’s webpage</a>.</p>
<!--more-->
<div style="display:none">
<p class="mathjaxWide"><span class="math inline">\(\DeclareMathOperator{\dbinomial}{binomial} \DeclareMathOperator{\dbern}{Bernoulli} \DeclareMathOperator{\dbeta}{beta}\)</span></p>
</div>
<p>Let’s derive the prior predictive distribution of a beta-binomial model with a uniform prior. See <a href="https://math.stackexchange.com/questions/122296/how-to-evaluate-this-integral-relating-to-binomial">stackexchange</a> and <a href="https://en.wikipedia.org/wiki/Beta_function">wikipedia</a> for useful results for solving the integral below.</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
  p(y = k)
  &amp;=
  \int_0^1 p(y = k \mid \theta) p(\theta) d\theta
  \\
  &amp;=
  \binom{n}{k} \cdot \int_0^1 \theta^k (1 - \theta)^{n - k} d\theta
  \\
  &amp;=
  \binom{n}{k} \cdot \frac{1}{\binom{n}{k} \cdot (n + 1)}
  \\
  &amp;=
  \frac{1}{n + 1}
\end{align}
\]</span></p>
<p>Now let’s show that the posterior mean of <span class="math inline">\(\theta\)</span> lies between the prior mean and observed frequency. The posterior is</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
  p(\theta \mid y)
  &amp;\propto
  p(y \mid \theta) \cdot p(\theta)
  \\
  &amp;\propto
  \theta^y (1 - \theta)^{n - y}\cdot \theta^{\alpha - 1} (1 - \theta)^{\beta - 1}
  \\
  &amp;=
  \theta^{y + \alpha - 1} (1 - \theta)^{n + \beta - y - 1}.
\end{align}
\]</span></p>
<p>So <span class="math inline">\(p(\theta \mid y) \sim \dbeta(y + \alpha, n - y + \beta)\)</span>, which has mean <span class="math inline">\(\frac{y + \alpha}{n + \alpha + \beta}\)</span>. Suppose <span class="math inline">\(\frac{y}{n} \le \frac{\alpha}{\alpha + \beta}\)</span>. Then</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
  \frac{y}{n}
  &amp;\le
  \frac{y + \alpha}{n + \alpha + \beta}
  \\
  \Leftrightarrow
  y(n + \alpha + \beta)
  &amp;\le
  n(y + \alpha)
  \\
  \Leftrightarrow
  y(\alpha + \beta)
  &amp;\le
  n\alpha
  \\
  \Leftrightarrow
  \frac{y}{n} 
  &amp;\le 
  \frac{\alpha}{\alpha + \beta}
\end{align}
\]</span></p>
<p>A similar argument shows that <span class="math inline">\(\frac{y + \alpha}{n + \alpha + \beta} \le \frac{\alpha}{\alpha + \beta}\)</span>.</p>
<p>If <span class="math inline">\(\frac{y}{n} \ge \frac{\alpha}{\alpha + \beta}\)</span>, then the analogous argument shows that <span class="math inline">\(\frac{\alpha}{\alpha + \beta} \le \frac{y + \alpha}{n + \alpha + \beta} \le \frac{y}{n}. \square\)</span></p>
<p>The prior variance is <span class="math inline">\(\mathbb V (\theta) = \frac{\alpha \beta}{(\alpha + \beta)^2(\alpha + \beta + 1)}\)</span>. For a uniform prior this is <span class="math inline">\(\frac{1}{4 \cdot 3} = \frac{1}{12}\)</span>. The posterior variance with a uniform prior is <span class="math inline">\(\frac{y + 1}{n + 2} \cdot \frac{n - y + 1}{n + 2} \cdot \frac{1}{n + 3}\)</span>. For <span class="math inline">\(p \in [0, 1]\)</span>, the function <span class="math inline">\(p \mapsto p(1 - p)\)</span> is maximised when <span class="math inline">\(p = 0.5\)</span>. Thus for fixed <span class="math inline">\(n\)</span>, the posterior variance is maximised when <span class="math inline">\(y = \frac{n}{2}\)</span>. This means that the posterior variance is at most <span class="math inline">\(\frac{1}{4} \cdot \frac{1}{n + 3} \le \frac{1}{4n + 12} \le \frac{1}{12}. \square\)</span></p>
<p>Intuitively, the posterior variance should be larger than the prior variance when the observed data is different from what would be expected from the prior distribution. (This can’t happen with a uniform prior because every value is equally likely). Indeed, with prior <span class="math inline">\(\theta \sim \dbeta(1, 9)\)</span> and observed data <span class="math inline">\(y = 9, n = 10\)</span>, we have <span class="math inline">\(\mathbb V(\theta) = \frac{9}{1100}\)</span> and <span class="math inline">\(\mathbb V(\theta \mid y) = \frac{1}{2} \cdot \frac{1}{2} \cdot \frac{1}{21} = \frac{1}{84}\)</span>.</p>

<div id="disqus_thread"></div>
<script>
/**
 *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
 *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
 */
/*
   var disqus_config = function () {
   this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
   this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
   };
 */
(function() {  // DON'T EDIT BELOW THIS LINE
 var d = document, s = d.createElement('script');

 s.src = '//stappit-github-io.disqus.com/embed.js';

 s.setAttribute('data-timestamp', +new Date());
 (d.head || d.body).appendChild(s);
 })();
</script>

<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

<script id="dsq-count-scr" src="//stappit-github-io.disqus.com/count.js" async></script>
]]></summary>
</entry>
<entry>
    <title>BDA3 Chapter 2 Exercise 4</title>
    <link href="http://stappit.github.io/posts/bda3/chapter_02_exercise_04.html" />
    <id>http://stappit.github.io/posts/bda3/chapter_02_exercise_04.html</id>
    <published>2018-08-23T00:00:00Z</published>
    <updated>2018-08-23T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 class="post-title">BDA3 Chapter 2 Exercise 4</h1>

<div class="card post-meta border-0">
  <div class="card-body post-meta">
    <p class="card-text text-muted text-left">
    Posted on August 23, 2018  by Brian </br>
     Tags: <a href="/tags/binomial.html">binomial</a>, <a href="/tags/bayes.html">bayes</a>, <a href="/tags/solutions.html">solutions</a>, <a href="/tags/bda%20chapter%202.html">bda chapter 2</a>, <a href="/tags/bda.html">bda</a>, <a href="/tags/normal%20approximation.html">normal approximation</a>, <a href="/tags/multi-modal.html">multi-modal</a> </br>
     Category: <a href="/categories/bda3.html">bda3</a> 
    </p>
  </div>
</div>

<p>Here’s my solution to exercise 4, chapter 2, of <a href="https://andrewgelman.com/">Gelman’s</a> <em>Bayesian Data Analysis</em> (BDA), 3rd edition. There are <a href="http://www.stat.columbia.edu/~gelman/book/solutions.pdf">solutions</a> to some of the exercises on the <a href="http://www.stat.columbia.edu/~gelman/book/">book’s webpage</a>.</p>
<!--more-->
<div style="display:none">
<p class="mathjaxWide"><span class="math inline">\(\DeclareMathOperator{\dbinomial}{binomial} \DeclareMathOperator{\dbern}{Bernoulli} \DeclareMathOperator{\dbeta}{beta}\)</span></p>
</div>
<p>Consider 1000 rolls of an unfair die, where the probability of a 6 is either 1/4, 1/6, or 1/12. Let’s draw the distribution and the normal approximation.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">N &lt;-<span class="st"> </span><span class="dv">1000</span>
p6 &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span> <span class="op">/</span><span class="st"> </span><span class="dv">4</span>, <span class="dv">1</span> <span class="op">/</span><span class="st"> </span><span class="dv">6</span>, <span class="dv">1</span> <span class="op">/</span><span class="st"> </span><span class="dv">12</span>)

ex4 &lt;-<span class="st"> </span><span class="kw">expand.grid</span>(
    <span class="dt">y =</span> <span class="kw">seq</span>(<span class="dv">0</span>, N),
    <span class="dt">theta =</span> p6
  ) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(
    <span class="dt">mu =</span> N <span class="op">*</span><span class="st"> </span>theta,
    <span class="dt">sigma =</span> <span class="kw">sqrt</span>(N <span class="op">*</span><span class="st"> </span>theta <span class="op">*</span><span class="st"> </span>(<span class="dv">10</span> <span class="op">-</span><span class="st"> </span>theta)),
    <span class="dt">binomial =</span> <span class="kw">dbinom</span>(y, N, theta),
    <span class="dt">normal_approx =</span> <span class="kw">dnorm</span>(y, mu, sigma),
    <span class="dt">theta =</span> scales<span class="op">::</span><span class="kw">percent</span>(<span class="kw">signif</span>(theta))
  ) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">select</span>(<span class="op">-</span>mu, <span class="op">-</span>sigma) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">gather</span>(distribution, probability, binomial, normal_approx) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">spread</span>(theta, probability) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">prior_probability =</span> <span class="fl">0.25</span> <span class="op">*</span><span class="st"> `</span><span class="dt">8.3%</span><span class="st">`</span> <span class="op">+</span><span class="st"> </span><span class="fl">0.5</span> <span class="op">*</span><span class="st"> `</span><span class="dt">16.7%</span><span class="st">`</span> <span class="op">+</span><span class="st"> </span><span class="fl">0.25</span> <span class="op">*</span><span class="st"> `</span><span class="dt">25.0%</span><span class="st">`</span>)</code></pre></div>
<table class="table" style="margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:right;">
y
</th>
<th style="text-align:left;">
distribution
</th>
<th style="text-align:right;">
16.7%
</th>
<th style="text-align:right;">
25.0%
</th>
<th style="text-align:right;">
8.3%
</th>
<th style="text-align:right;">
prior_probability
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
0
</td>
<td style="text-align:left;">
binomial
</td>
<td style="text-align:right;">
0.0e+00
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0.0000000
</td>
<td style="text-align:right;">
0.00e+00
</td>
</tr>
<tr>
<td style="text-align:right;">
0
</td>
<td style="text-align:left;">
normal_approx
</td>
<td style="text-align:right;">
2.1e-06
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0.0002078
</td>
<td style="text-align:right;">
5.30e-05
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
binomial
</td>
<td style="text-align:right;">
0.0e+00
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0.0000000
</td>
<td style="text-align:right;">
0.00e+00
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
normal_approx
</td>
<td style="text-align:right;">
2.3e-06
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0.0002297
</td>
<td style="text-align:right;">
5.86e-05
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:left;">
binomial
</td>
<td style="text-align:right;">
0.0e+00
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0.0000000
</td>
<td style="text-align:right;">
0.00e+00
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:left;">
normal_approx
</td>
<td style="text-align:right;">
2.5e-06
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0.0002536
</td>
<td style="text-align:right;">
6.47e-05
</td>
</tr>
</tbody>
</table>
<figure>
<img src="chapter_02_exercise_04_files/figure-markdown/ex4_plot-1.png" />
</figure>
<p>The normal approximation underestimates the maxima and overestimates the values between the maxima. From the percentiles in the table below, we see that the normal approximation is best near the median but becomes gradually worse towards towards both extremes.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">percentiles &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.05</span>, <span class="fl">0.25</span>, <span class="fl">0.5</span>, <span class="fl">0.75</span>, <span class="fl">0.95</span>)

ex4 <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(distribution) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">arrange</span>(y) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(
    <span class="dt">cdf =</span> <span class="kw">cumsum</span>(prior_probability),
    <span class="dt">percentile =</span> <span class="kw">case_when</span>(
      cdf <span class="op">&lt;=</span><span class="st"> </span><span class="fl">0.05</span> <span class="op">~</span><span class="st"> &#39;05%&#39;</span>,
      cdf <span class="op">&lt;=</span><span class="st"> </span><span class="fl">0.25</span> <span class="op">~</span><span class="st"> &#39;25%&#39;</span>,
      cdf <span class="op">&lt;=</span><span class="st"> </span><span class="fl">0.50</span> <span class="op">~</span><span class="st"> &#39;50%&#39;</span>,
      cdf <span class="op">&lt;=</span><span class="st"> </span><span class="fl">0.75</span> <span class="op">~</span><span class="st"> &#39;75%&#39;</span>,
      cdf <span class="op">&lt;=</span><span class="st"> </span><span class="fl">0.95</span> <span class="op">~</span><span class="st"> &#39;95%&#39;</span>
    )
  ) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">filter</span>(cdf <span class="op">&lt;=</span><span class="st"> </span><span class="fl">0.95</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(distribution, percentile) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">slice</span>(<span class="kw">which.max</span>(cdf)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">select</span>(distribution, percentile, y) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">spread</span>(distribution, y) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">arrange</span>(percentile) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">kable</span>() <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">kable_styling</span>()</code></pre></div>
<table class="table" style="margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
percentile
</th>
<th style="text-align:right;">
binomial
</th>
<th style="text-align:right;">
normal_approx
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
05%
</td>
<td style="text-align:right;">
75
</td>
<td style="text-align:right;">
58
</td>
</tr>
<tr>
<td style="text-align:left;">
25%
</td>
<td style="text-align:right;">
119
</td>
<td style="text-align:right;">
110
</td>
</tr>
<tr>
<td style="text-align:left;">
50%
</td>
<td style="text-align:right;">
166
</td>
<td style="text-align:right;">
164
</td>
</tr>
<tr>
<td style="text-align:left;">
75%
</td>
<td style="text-align:right;">
206
</td>
<td style="text-align:right;">
214
</td>
</tr>
<tr>
<td style="text-align:left;">
95%
</td>
<td style="text-align:right;">
260
</td>
<td style="text-align:right;">
291
</td>
</tr>
</tbody>
</table>

<div id="disqus_thread"></div>
<script>
/**
 *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
 *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
 */
/*
   var disqus_config = function () {
   this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
   this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
   };
 */
(function() {  // DON'T EDIT BELOW THIS LINE
 var d = document, s = d.createElement('script');

 s.src = '//stappit-github-io.disqus.com/embed.js';

 s.setAttribute('data-timestamp', +new Date());
 (d.head || d.body).appendChild(s);
 })();
</script>

<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

<script id="dsq-count-scr" src="//stappit-github-io.disqus.com/count.js" async></script>
]]></summary>
</entry>
<entry>
    <title>BDA3 Chapter 2 Exercise 3</title>
    <link href="http://stappit.github.io/posts/bda3/chapter_02_exercise_03.html" />
    <id>http://stappit.github.io/posts/bda3/chapter_02_exercise_03.html</id>
    <published>2018-08-22T00:00:00Z</published>
    <updated>2018-08-22T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 class="post-title">BDA3 Chapter 2 Exercise 3</h1>

<div class="card post-meta border-0">
  <div class="card-body post-meta">
    <p class="card-text text-muted text-left">
    Posted on August 22, 2018  by Brian </br>
     Tags: <a href="/tags/binomial.html">binomial</a>, <a href="/tags/bayes.html">bayes</a>, <a href="/tags/solutions.html">solutions</a>, <a href="/tags/bda%20chapter%202.html">bda chapter 2</a>, <a href="/tags/bda.html">bda</a>, <a href="/tags/normal%20approximation.html">normal approximation</a> </br>
     Category: <a href="/categories/bda3.html">bda3</a> 
    </p>
  </div>
</div>

<p>Here’s my solution to exercise 3, chapter 2, of <a href="https://andrewgelman.com/">Gelman’s</a> <em>Bayesian Data Analysis</em> (BDA), 3rd edition. There are <a href="http://www.stat.columbia.edu/~gelman/book/solutions.pdf">solutions</a> to some of the exercises on the <a href="http://www.stat.columbia.edu/~gelman/book/">book’s webpage</a>.</p>
<!--more-->
<div style="display:none">
<p class="mathjaxWide"><span class="math inline">\(\DeclareMathOperator{\dbinomial}{binomial} \DeclareMathOperator{\dbern}{Bernoulli} \DeclareMathOperator{\dbeta}{beta}\)</span></p>
</div>
<p>For 1000 rolls of a fair die, The mean number of sixs is 1000/6 = 166.667, the variance is 138.889, and the standard deviation is 11.7851.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">N &lt;-<span class="st"> </span><span class="dv">1000</span>
p &lt;-<span class="st"> </span><span class="dv">1</span> <span class="op">/</span><span class="st"> </span><span class="dv">6</span>
mu &lt;-<span class="st"> </span>N <span class="op">*</span><span class="st"> </span>p
sigma &lt;-<span class="st"> </span><span class="kw">sqrt</span>(N <span class="op">*</span><span class="st"> </span>p <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>p))

ex3 &lt;-<span class="st"> </span><span class="kw">tibble</span>(
    <span class="dt">y =</span> <span class="kw">seq</span>(<span class="dv">0</span>, N),
    <span class="dt">binomial =</span> <span class="kw">dbinom</span>(y, N, p),
    <span class="dt">normal_approx =</span> <span class="kw">dnorm</span>(y, mu, sigma)
  ) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">gather</span>(metric, probability, <span class="op">-</span>y) </code></pre></div>
<table class="table" style="margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:right;">
y
</th>
<th style="text-align:left;">
metric
</th>
<th style="text-align:right;">
probability
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
0
</td>
<td style="text-align:left;">
binomial
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
binomial
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:left;">
binomial
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:right;">
3
</td>
<td style="text-align:left;">
binomial
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:right;">
4
</td>
<td style="text-align:left;">
binomial
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:right;">
5
</td>
<td style="text-align:left;">
binomial
</td>
<td style="text-align:right;">
0
</td>
</tr>
</tbody>
</table>
<figure>
<img src="chapter_02_exercise_03_files/figure-markdown/ex3_plot-1.png" />
</figure>
<p>The two curves are visually indistinguishable. The percentiles are listed in the table below.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">percentiles &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.05</span>, <span class="fl">0.25</span>, <span class="fl">0.5</span>, <span class="fl">0.75</span>, <span class="fl">0.95</span>)

<span class="kw">tibble</span>(
    <span class="dt">percentile =</span> scales<span class="op">::</span><span class="kw">percent</span>(percentiles),
    <span class="dt">binom =</span> <span class="kw">qbinom</span>(percentiles, N, p),
    <span class="dt">norm =</span> <span class="kw">qnorm</span>(percentiles, mu, sigma)
) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">kable</span>() <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">kable_styling</span>()</code></pre></div>
<table class="table" style="margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
percentile
</th>
<th style="text-align:right;">
binom
</th>
<th style="text-align:right;">
norm
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
5%
</td>
<td style="text-align:right;">
147
</td>
<td style="text-align:right;">
147.2819
</td>
</tr>
<tr>
<td style="text-align:left;">
25%
</td>
<td style="text-align:right;">
159
</td>
<td style="text-align:right;">
158.7177
</td>
</tr>
<tr>
<td style="text-align:left;">
50%
</td>
<td style="text-align:right;">
167
</td>
<td style="text-align:right;">
166.6667
</td>
</tr>
<tr>
<td style="text-align:left;">
75%
</td>
<td style="text-align:right;">
175
</td>
<td style="text-align:right;">
174.6156
</td>
</tr>
<tr>
<td style="text-align:left;">
95%
</td>
<td style="text-align:right;">
186
</td>
<td style="text-align:right;">
186.0515
</td>
</tr>
</tbody>
</table>

<div id="disqus_thread"></div>
<script>
/**
 *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
 *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
 */
/*
   var disqus_config = function () {
   this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
   this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
   };
 */
(function() {  // DON'T EDIT BELOW THIS LINE
 var d = document, s = d.createElement('script');

 s.src = '//stappit-github-io.disqus.com/embed.js';

 s.setAttribute('data-timestamp', +new Date());
 (d.head || d.body).appendChild(s);
 })();
</script>

<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

<script id="dsq-count-scr" src="//stappit-github-io.disqus.com/count.js" async></script>
]]></summary>
</entry>
<entry>
    <title>BDA3 Chapter 2 Exercise 2</title>
    <link href="http://stappit.github.io/posts/bda3/chapter_02_exercise_02.html" />
    <id>http://stappit.github.io/posts/bda3/chapter_02_exercise_02.html</id>
    <published>2018-08-21T00:00:00Z</published>
    <updated>2018-08-21T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 class="post-title">BDA3 Chapter 2 Exercise 2</h1>

<div class="card post-meta border-0">
  <div class="card-body post-meta">
    <p class="card-text text-muted text-left">
    Posted on August 21, 2018  by Brian </br>
     Tags: <a href="/tags/stan.html">stan</a>, <a href="/tags/binomial.html">binomial</a>, <a href="/tags/bayes.html">bayes</a>, <a href="/tags/solutions.html">solutions</a>, <a href="/tags/bda%20chapter%202.html">bda chapter 2</a>, <a href="/tags/bda.html">bda</a> </br>
     Category: <a href="/categories/bda3.html">bda3</a> 
    </p>
  </div>
</div>

<p>Here’s my solution to exercise 2, chapter 2, of <a href="https://andrewgelman.com/">Gelman’s</a> <em>Bayesian Data Analysis</em> (BDA), 3rd edition. There are <a href="http://www.stat.columbia.edu/~gelman/book/solutions.pdf">solutions</a> to some of the exercises on the <a href="http://www.stat.columbia.edu/~gelman/book/">book’s webpage</a>.</p>
<!--more-->
<div style="display:none">
<p class="mathjaxWide"><span class="math inline">\(\DeclareMathOperator{\dbinomial}{binomial} \DeclareMathOperator{\dbern}{Bernoulli} \DeclareMathOperator{\dbeta}{beta}\)</span></p>
</div>
<p>We are given the following information about the two coins.</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
  p(C_1) &amp;= 0.5 &amp; p(H \mid C_1) &amp;= \dbern(H \mid 0.6) 
  \\
  p(C_2) &amp;= 0.5 &amp; p(H \mid C_2) &amp;= \dbern(H \mid 0.4)
\end{align}
\]</span></p>
<p>The posterior probability of each coin given two tails is:</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
  p(C_1 \mid TT )
  &amp;\propto
  p(TT \mid C_1) \cdot p(C_1)
  \\
  &amp;=
  \left(\frac{2}{5}\right)^2 \frac{1}{2}
  \\
  &amp;=
  \frac{2}{25}
\end{align}
\]</span> <span class="math display">\[
\begin{align}
  p(C_2 \mid TT )
  &amp;\propto
  p(TT \mid C_2) \cdot p(C_2)
  \\
  &amp;=
  \left(\frac{3}{5}\right)^2 \frac{1}{2}
  \\
  &amp;=
  \frac{9}{50}
\end{align}
\]</span></p>
<p>Both of the previous probabilities are normalised by the same constant. Since <span class="math inline">\(p(C_1 \mid TT) + p(C_2 \mid TT) = 1\)</span>, the normalising constant is <span class="math inline">\(\frac{2}{25} + \frac{9}{50} = \frac{13}{50}\)</span>. Thus</p>
<p class="mathjaxWide"><span class="math display">\[
p(C_1 \mid TT) = \frac{4}{13}
\qquad
\text{and}
\qquad
p(C_2 \mid TT) = \frac{9}{13}.
\]</span></p>
<p>Let <span class="math inline">\(y\)</span> be the number of additional spins until the next head. Conditional on a coin, <span class="math inline">\(y\)</span> is <a href="https://en.wikipedia.org/wiki/Geometric_distribution">geometrically</a> distributed. So the expected number of spins before the next head is:</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
  \mathbb E(y \mid TT)
  &amp;=
  \frac{4}{13}\mathbb E(y \mid C_1)
  +
  \frac{9}{13}\mathbb E(y \mid C_1)
  \\
  &amp;=
  \frac{4}{13}\frac{5}{3}
  +
  \frac{9}{13}\frac{5}{2}
  \\
  &amp;=
  \frac{20}{39}
  +
  \frac{45}{26}
  \\
  &amp;=
  \frac{175}{78},
\end{align}
\]</span></p>
<p>which is 2.24359.</p>

<div id="disqus_thread"></div>
<script>
/**
 *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
 *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
 */
/*
   var disqus_config = function () {
   this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
   this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
   };
 */
(function() {  // DON'T EDIT BELOW THIS LINE
 var d = document, s = d.createElement('script');

 s.src = '//stappit-github-io.disqus.com/embed.js';

 s.setAttribute('data-timestamp', +new Date());
 (d.head || d.body).appendChild(s);
 })();
</script>

<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

<script id="dsq-count-scr" src="//stappit-github-io.disqus.com/count.js" async></script>
]]></summary>
</entry>
<entry>
    <title>BDA3 Chapter 2 Exercise 1</title>
    <link href="http://stappit.github.io/posts/bda3/chapter_02_exercise_01.html" />
    <id>http://stappit.github.io/posts/bda3/chapter_02_exercise_01.html</id>
    <published>2018-08-20T00:00:00Z</published>
    <updated>2018-08-20T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 class="post-title">BDA3 Chapter 2 Exercise 1</h1>

<div class="card post-meta border-0">
  <div class="card-body post-meta">
    <p class="card-text text-muted text-left">
    Posted on August 20, 2018  by Brian </br>
     Tags: <a href="/tags/stan.html">stan</a>, <a href="/tags/beta.html">beta</a>, <a href="/tags/binomial.html">binomial</a>, <a href="/tags/bayes.html">bayes</a>, <a href="/tags/solutions.html">solutions</a>, <a href="/tags/bda%20chapter%202.html">bda chapter 2</a>, <a href="/tags/bda.html">bda</a> </br>
     Category: <a href="/categories/bda3.html">bda3</a> 
    </p>
  </div>
</div>

<p>Here’s my solution to exercise 1, chapter 2, of <a href="https://andrewgelman.com/">Gelman’s</a> <em>Bayesian Data Analysis</em> (BDA), 3rd edition. There are <a href="http://www.stat.columbia.edu/~gelman/book/solutions.pdf">solutions</a> to some of the exercises on the <a href="http://www.stat.columbia.edu/~gelman/book/">book’s webpage</a>.</p>
<!--more-->
<div style="display:none">
<p class="mathjaxWide"><span class="math inline">\(\DeclareMathOperator{\dbinomial}{binomial} \DeclareMathOperator{\dbern}{Bernoulli} \DeclareMathOperator{\dbeta}{beta}\)</span></p>
</div>
<p>Let <span class="math inline">\(H\)</span> be the number of heads in 10 tosses of the coin. With a <span class="math inline">\(\dbeta(4, 4)\)</span> prior on the probability <span class="math inline">\(\theta\)</span> of a head, the posterior after finding out <span class="math inline">\(H \le 2\)</span> is</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
  p(\theta \mid H \le 2)
  &amp;\propto
  p(H \le 2 \mid \theta) \cdot p(\theta)
  \\
  &amp;=
  \dbeta(\theta \mid 4, 4) \sum_{h = 0}^2 \dbinomial(h \mid \theta, 10)
  \\
  &amp;=
  \theta^3 (1 - \theta)^3 \sum_{h = 0}^2 \binom{10}{h} \theta^h (1 - \theta)^{10 - h}.
\end{align}
\]</span></p>
<p>We can plot this unnormalised posterior density from the following dataset.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ex1 &lt;-<span class="st"> </span><span class="kw">tibble</span>(
         <span class="dt">theta =</span> <span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="fl">0.01</span>), 
         <span class="dt">prior =</span> theta<span class="op">^</span><span class="dv">3</span> <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>theta)<span class="op">^</span><span class="dv">3</span>,
         <span class="dt">posterior =</span> prior <span class="op">*</span><span class="st"> </span>(
           <span class="kw">choose</span>(<span class="dv">10</span>, <span class="dv">0</span>) <span class="op">*</span><span class="st"> </span>theta<span class="op">^</span><span class="dv">0</span> <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>theta)<span class="op">^</span><span class="dv">10</span> <span class="op">+</span>
<span class="st">           </span><span class="kw">choose</span>(<span class="dv">10</span>, <span class="dv">1</span>) <span class="op">*</span><span class="st"> </span>theta<span class="op">^</span><span class="dv">1</span> <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>theta)<span class="op">^</span><span class="dv">9</span> <span class="op">+</span>
<span class="st">           </span><span class="kw">choose</span>(<span class="dv">10</span>, <span class="dv">2</span>) <span class="op">*</span><span class="st"> </span>theta<span class="op">^</span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>theta)<span class="op">^</span><span class="dv">8</span> 
         )
       )</code></pre></div>
<figure>
<img src="chapter_02_exercise_01_files/figure-markdown/ex1_plot-1.png" />
</figure>
<p>With the help of <a href="http://mc-stan.org/">Stan</a>, we can obtain the normalised posterior density. We include the information that there are at most 2 heads observed by using the (log) cumulative density function.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m1 &lt;-<span class="st"> </span>rstan<span class="op">::</span><span class="kw">stan_model</span>(<span class="st">&#39;src/ex_02_01.stan&#39;</span>)</code></pre></div>
<pre><code>S4 class stanmodel &#39;ex_02_01&#39; coded as follows:
transformed data {
  int tosses = 10;
  int max_heads = 2;
}

parameters {
  real&lt;lower = 0, upper = 1&gt; theta;
}

model {
  theta ~ beta(4, 4); // prior 
  target += binomial_lcdf(max_heads | tosses, theta); // likelihood
} </code></pre>
<p>The following posterior has the same shape as our exact unnormalised density above. The difference is that we now have a normalised probability distribution without having to work out the maths ourselves.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">f1 &lt;-<span class="st"> </span><span class="kw">sampling</span>(m1, <span class="dt">iter =</span> <span class="dv">40000</span>, <span class="dt">warmup =</span> <span class="dv">500</span>, <span class="dt">chains =</span> <span class="dv">1</span>)</code></pre></div>
<figure>
<img src="chapter_02_exercise_01_files/figure-markdown/ex1_stan_plot-1.png" />
</figure>

<div id="disqus_thread"></div>
<script>
/**
 *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
 *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
 */
/*
   var disqus_config = function () {
   this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
   this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
   };
 */
(function() {  // DON'T EDIT BELOW THIS LINE
 var d = document, s = d.createElement('script');

 s.src = '//stappit-github-io.disqus.com/embed.js';

 s.setAttribute('data-timestamp', +new Date());
 (d.head || d.body).appendChild(s);
 })();
</script>

<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

<script id="dsq-count-scr" src="//stappit-github-io.disqus.com/count.js" async></script>
]]></summary>
</entry>
<entry>
    <title>Understanding the hazard function</title>
    <link href="http://stappit.github.io/posts/survival_models/hazard.html" />
    <id>http://stappit.github.io/posts/survival_models/hazard.html</id>
    <published>2018-08-05T00:00:00Z</published>
    <updated>2018-08-05T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 class="post-title">Understanding the hazard function</h1>

<div class="card post-meta border-0">
  <div class="card-body post-meta">
    <p class="card-text text-muted text-left">
    Posted on August  5, 2018  by Brian </br>
     Tags: <a href="/tags/hazard.html">hazard</a>, <a href="/tags/censoring.html">censoring</a>, <a href="/tags/survival.html">survival</a>, <a href="/tags/exponential.html">exponential</a>, <a href="/tags/poisson.html">poisson</a> </br>
     Category: <a href="/categories/survival_models.html">survival_models</a> 
    </p>
  </div>
</div>

<p>Suppose you have fit a distribution to your censored survival times as in the <a href="./censoring.html">previous post</a> and now want to quantify the intuition of an event being imminent. For example, being able to characterise precisely when your customers are about to churn can help identify problem areas to improve on. This notion is called the <em>hazard</em>. We’ll take a look at some of its main properties and how it related to survival analysis.</p>
<!--more-->
<h2 id="definition">Definition</h2>
<p>Start with a small <span class="math inline">\(\delta\)</span>-interval around t and and consider the average probability density of an event ocurring in that interval given that it occurs after t: <span class="math inline">\(\frac{\mathbb P(t \le T &lt; t + \delta \mid t \le T)}{\delta}\)</span>, where <span class="math inline">\(\mathbb P\)</span> is the probability function. To get rid of the arbitrary choice of <span class="math inline">\(\delta\)</span>, we take the limit as <span class="math inline">\(\delta\)</span> goes to zero.</p>
<p class="mathjaxWide"><span class="math display">\[
h(t)
:=
\lim_{\delta \rightarrow 0} \frac{\mathbb P(t \le T &lt; t + \delta \mid t \le T)}{\delta}
.
\]</span></p>
<p>This is well-defined whenever the CDF is differentiable. Although it is defined in terms of probabilities, we will show below that the hazard is not itself a probability density function.</p>
<h2 id="identities">Identities</h2>
<p>There are a number of useful properties of the hazard function that make it convenient to work with in survival analysis.</p>
<h3 id="equivalent-definition">Equivalent definition</h3>
<p>The above definition helps us understand the intuition behind the hazard function but there’s an equivalent formulation that can be easier to work with. Using</p>
<ul>
<li>the definition of conditional probabilities,</li>
<li>the definition of a derivative, and</li>
<li>that <span class="math inline">\(F&#39;(t) = f(t)\)</span> where <span class="math inline">\(F\)</span> is the CDF and <span class="math inline">\(f\)</span> the probability function,</li>
</ul>
<p>we can show that</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
  h(t)
  &amp;=
  \lim_{\delta \rightarrow 0} \frac{\mathbb P(t \le T &lt; t + \delta \mid t \le T)}{\delta}
  \\
  &amp;=\lim_{\delta \rightarrow 0} \frac{\mathbb P(t \le T &lt; t + \delta)}{\mathbb P(t \le T)\delta}
  \\
  &amp;=\lim_{\delta \rightarrow 0} \frac{F(t + \delta) - F(t)}{S(t)\delta}
  \\
  &amp;=\lim_{\delta \rightarrow 0} \frac{F(t + \delta) - F(t)}{\delta} \frac{1}{S(t)}
  \\
  &amp;= F&#39;(t) \frac{1}{S(t)}
  \\
  &amp;= \frac{f(t)}{S(t)}.
\end{align}
\]</span></p>
<p>We will show below how to use this to simplify the likelihood in the case of censored observations for measuring time to an event of interest.</p>
<h3 id="relation-with-the-survival-function">Relation with the survival function</h3>
<p>Using the identity above, we can rewrite the hazard as a derivative of the survival function:</p>
<p class="mathjaxWide"><span class="math display">\[
h(t)
=
\frac{f(t)}{S(t)}
=
-\frac{d}{dt} \log S(t)
.
\]</span></p>
<p>It then follows from the <a href="https://en.wikipedia.org/wiki/Fundamental_theorem_of_calculus">first fundamental theorem of calculus</a> that</p>
<p class="mathjaxWide"><span class="math display">\[
S(t) = e^{-\int_0^t h(s) ds}.
\]</span></p>
<p>In other words, the hazard function completely determines the survival function (and therefore also the mass/density function).</p>
<p>Since the integral of the hazard appears in the above equation, we can give it a definition for easier reference. We define the <em>cumulative hazard</em> as</p>
<p class="mathjaxWide"><span class="math display">\[
H(t) 
:=
\int_0^t h(s) ds
.
\]</span></p>
<p>Since <span class="math inline">\(\lim_{t \rightarrow \infty} S(t) = 0\)</span>, it follows that <span class="math inline">\(\lim_{t \rightarrow \infty} H(t) = -\lim \log S(t) = \infty\)</span>. In particular, this means that the hazard function is NOT a probability density function!</p>
<h3 id="example">Example</h3>
<p>The exponential distribution has constant hazard. To see this, suppose <span class="math inline">\(h(t) = \lambda\)</span>. Then <span class="math inline">\(S(t) = \exp(-\int_0^t \lambda ds) = \exp(-\lambda t)\)</span> so that <span class="math inline">\(f(t) = -S&#39;(t) = \lambda \exp(-\lambda t)\)</span>, which is the probability function for the <a href="https://en.wikipedia.org/wiki/Exponential_distribution">exponential distribution</a>.</p>
<h2 id="hazard-in-censored-survival-analysis">Hazard in censored survival analysis</h2>
<p>In <a href="./censoring.html">the previous post</a>, we motivated the following likelihood in the case of censored survival times:</p>
<p class="mathjaxWide"><span class="math display">\[
L(\theta) 
:= 
\prod_{i = 1}^N \delta_i f(t_i \mid \theta)
\times
\prod_{i = 1}^N (1 - \delta_i) S(t_i \mid \theta)
\]</span></p>
<p>where <span class="math inline">\(\delta_i\)</span> is 1 if the event is observed and 0 if it is censored, and <span class="math inline">\(\theta\)</span> is the vector of parameters of the distribution of survival times. Since <span class="math inline">\(f(t) = h(t) S(t)\)</span>, we can rewrite this as</p>
<p class="mathjaxWide"><span class="math display">\[
L(\theta) 
:= 
\prod_{i = 1}^N h(t_i \mid \theta)^{\delta_i}S(t_i \mid \theta).
\]</span></p>
<h3 id="example-1">Example</h3>
<p>Assuming that survival times follow an exponential distribution, the hazard <span class="math inline">\(h(t) = \lambda\)</span> is constant, and the likelihood is</p>
<p class="mathjaxWide"><span class="math display">\[
  L(\lambda)
  = 
  \prod_{i = 1}^N \lambda^{\delta_i} e^{-\lambda t_i}
  =
  \lambda^D e^{-\lambda T}
\]</span></p>
<p>where <span class="math inline">\(D := \sum_1^N \delta_i\)</span> is the total number of events observed and <span class="math inline">\(T := \sum_1^N t_i\)</span> is the total observation time. This expression has an interesting interpretation as a <a href="https://en.wikipedia.org/wiki/Poisson_distribution">Poisson</a> likelihood. To see this, first note that <span class="math inline">\(T\)</span> and <span class="math inline">\(D\)</span> can be considered constant in our likelihood because they don’t depend on our only parameter <span class="math inline">\(\lambda\)</span>. We can consider <span class="math inline">\(D\)</span> as a Poisson variable with rate <span class="math inline">\(\lambda\)</span> and exposure <span class="math inline">\(T\)</span>:</p>
<p class="mathjaxWide"><span class="math display">\[
D \mid \lambda \sim \text{Poisson}(\lambda T),
\]</span></p>
<p>which gives probability of observing <span class="math inline">\(D\)</span> events as</p>
<p class="mathjaxWide"><span class="math display">\[
\text{Poisson}(D \mid \lambda T)
=
(\lambda T)^D e^{-\lambda T}
=
T^D L(\lambda)
.
\]</span></p>
<p>However, likelihoods are equivalent up to a multiplicative constant. Since <span class="math inline">\(T^D\)</span> is constant we can treat our likelihood, <span class="math inline">\(L\)</span>, as Poisson.</p>

<div id="disqus_thread"></div>
<script>
/**
 *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
 *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
 */
/*
   var disqus_config = function () {
   this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
   this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
   };
 */
(function() {  // DON'T EDIT BELOW THIS LINE
 var d = document, s = d.createElement('script');

 s.src = '//stappit-github-io.disqus.com/embed.js';

 s.setAttribute('data-timestamp', +new Date());
 (d.head || d.body).appendChild(s);
 })();
</script>

<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

<script id="dsq-count-scr" src="//stappit-github-io.disqus.com/count.js" async></script>
]]></summary>
</entry>
<entry>
    <title>How to deal with right-censored observations</title>
    <link href="http://stappit.github.io/posts/survival_models/censoring.html" />
    <id>http://stappit.github.io/posts/survival_models/censoring.html</id>
    <published>2018-08-04T00:00:00Z</published>
    <updated>2018-08-04T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 class="post-title">How to deal with right-censored observations</h1>

<div class="card post-meta border-0">
  <div class="card-body post-meta">
    <p class="card-text text-muted text-left">
    Posted on August  4, 2018  by Brian </br>
     Tags: <a href="/tags/censoring.html">censoring</a>, <a href="/tags/stan.html">stan</a>, <a href="/tags/likelihood.html">likelihood</a>, <a href="/tags/survival.html">survival</a>, <a href="/tags/mle.html">mle</a>, <a href="/tags/poisson.html">poisson</a> </br>
     Category: <a href="/categories/survival_models.html">survival_models</a> 
    </p>
  </div>
</div>

<p>I’ve recently been interested in understanding survival models, which model the time to an event of interest (<code>tte</code>) but where we are not always able to wait until that event occurs. This happens, for example, when modelling the time until a customer <a href="https://en.wikipedia.org/wiki/Churn_rate">churns</a>: some of your customers may have cancelled their subscriptions but many hopefully haven’t. Those that haven’t are said to be <code>censored</code> because we haven’t observed them cancel their subscription yet.</p>
<!--more-->
<p>As a first step in that direction, we’ll take a look at modelling censoring when the <code>tte</code> has a Poisson distribution (minor modifications can be made to extend to other distributions). We’ll use <a href="http://mc-stan.org/">Stan</a> to implement our model since Bayesian notation very nicely reflects our statistical understanding. Don’t worry if you aren’t familiar with Stan or Bayesian inference - it should be possible to follow along regardless.</p>
<p>You can download the <a href="./censoring.Rmd">R markdown</a> and the <a href="./censored_poisson.stan">stan model</a> to try it out.</p>
<h2 id="some-theory">Some theory</h2>
<h3 id="the-problem">The Problem</h3>
<p>Let’s generate some data. We will assume that the time to event (<code>tte</code>) is poisson distributed with mean <span class="math inline">\(\mu = 10\)</span>. However, we will also assume that we don’t get to observe the event of interest in every case, i.e. some cases are censored. What we measure is the time to observation (<code>tto</code>).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">N &lt;-<span class="st"> </span><span class="dv">10000</span>
mu &lt;-<span class="st"> </span><span class="dv">10</span>

df &lt;-<span class="st"> </span><span class="kw">tibble</span>(
  <span class="dt">id =</span> <span class="dv">1</span><span class="op">:</span>N,
  <span class="dt">tte =</span> <span class="kw">rpois</span>(N, mu),
  <span class="dt">tto =</span> <span class="kw">pmin</span>(<span class="kw">rpois</span>(N, <span class="dv">12</span>), tte),
  <span class="dt">censored =</span> tto <span class="op">&lt;</span><span class="st"> </span>tte
)

df <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">head</span>() <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">kable</span>() <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">kable_styling</span>()</code></pre></div>
<table class="table" style="margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:right;">
id
</th>
<th style="text-align:right;">
tte
</th>
<th style="text-align:right;">
tto
</th>
<th style="text-align:left;">
censored
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
12
</td>
<td style="text-align:right;">
11
</td>
<td style="text-align:left;">
TRUE
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
7
</td>
<td style="text-align:right;">
7
</td>
<td style="text-align:left;">
FALSE
</td>
</tr>
<tr>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:left;">
FALSE
</td>
</tr>
<tr>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
13
</td>
<td style="text-align:right;">
8
</td>
<td style="text-align:left;">
TRUE
</td>
</tr>
<tr>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:left;">
FALSE
</td>
</tr>
<tr>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
15
</td>
<td style="text-align:right;">
8
</td>
<td style="text-align:left;">
TRUE
</td>
</tr>
</tbody>
</table>
<figure>
<img src="censoring_files/figure-markdown/censoring_count-1.png" />
</figure>
<p>Note that we observe <code>tto</code> but not <code>tte</code>. How might we estimate <span class="math inline">\(\mu\)</span>? One way is to take the mean.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">df <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">select</span>(tte, tto) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">summarise_all</span>(mean) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">kable</span>() <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">kable_styling</span>()</code></pre></div>
<table class="table" style="margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:right;">
tte
</th>
<th style="text-align:right;">
tto
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
9.9916
</td>
<td style="text-align:right;">
8.9525
</td>
</tr>
</tbody>
</table>
<p>This estimate is fairly good for <code>tte</code> but is too low for <code>tto</code>. This was to be expected because we know that <code>tto</code> is smaller than <code>tte</code> for censored observations.</p>
<p>It’s not possible to just filter out the censored values as this also gives biased estimates. In fact, it makes our estimate worse in this case.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">df <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">filter</span>(<span class="op">!</span>censored) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">select</span>(tte, tto) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">summarise_all</span>(mean) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">kable</span>() <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">kable_styling</span>()</code></pre></div>
<table class="table" style="margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:right;">
tte
</th>
<th style="text-align:right;">
tto
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
8.916655
</td>
<td style="text-align:right;">
8.916655
</td>
</tr>
</tbody>
</table>
<h3 id="a-more-sophisticated-way-to-be-wrong">A more sophisticated way to be wrong</h3>
<p>So how can we estimate μ using just <code>tto</code>? The first step is to reinterperet the mean as the estimator that maximises a <a href="https://khakieconomics.github.io/2018/07/14/What-is-a-likelihood-anyway.html">likelihood</a> (the maximum likelihood estimator, or MLE). The likelihood is defined as the probability of the data given the estimate. Under the true model, this probability is <span class="math inline">\(f(tte_i \mid \mu) = \text{Poisson}(tte_i \mid \mu)\)</span> for the <span class="math inline">\(i\)</span>th case, giving the likelihood of the whole dataset as:</p>
<p class="mathjaxWide"><span class="math display">\[
L(\mu) := \prod_{i = 1}^N f(tte_i | \mu)
.
\]</span></p>
<p>The mean maximises this likelihood, which is why the mean of <code>tte</code> is close to the true value.</p>
<p>To further illustrate this point, note that this is the estimate we get when regressing <code>tte</code> on a constant.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">df <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">glm</span>(
    <span class="dt">formula =</span> tte <span class="op">~</span><span class="st"> </span><span class="dv">1</span>,
    <span class="dt">family =</span> <span class="kw">poisson</span>(<span class="dt">link =</span> <span class="st">&#39;log&#39;</span>),
    <span class="dt">data =</span> .
  ) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">tidy</span>() <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">pull</span>(estimate) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">exp</span>()</code></pre></div>
<pre><code>[1] 9.9916</code></pre>
<p>However, we don’t observe <code>tte</code>; we observe <code>tto</code>. Simply replacing <code>tte</code> with <code>tto</code> and maximising</p>
<p class="mathjaxWide"><span class="math display">\[
L(\mu) := \prod_{i = 1}^N f(tto_i \mid \mu)
\]</span></p>
<p>gives us the mean of <code>tto</code>, which is a bad estimate because this ‘likelihood’ does not take censoring into account.</p>
<h3 id="the-correct-likelihood">The correct likelihood</h3>
<p>So what likelihood can we use? Note that in uncensored cases, <span class="math inline">\(f(tto_i \mid \mu) = f(tte_i \mid \mu)\)</span>, just like above.</p>
<p>In the censored cases, all we know is that <code>tte</code> must be larger than what was observed. This means that we need to sum over the probabilities of all possibilities: <span class="math inline">\(S(tto_i \mid \mu) := \sum_{t &gt; tto_i} f(t \mid \mu)\)</span>. The full likelihood is then</p>
<p class="mathjaxWide"><span class="math display">\[
L(\mu) 
:= 
\prod_{i = 1}^N \delta_i f(tto_i \mid \mu)
\times
\prod_{i = 1}^N (1 - \delta_i) S(tto_i \mid \mu)
,
\]</span></p>
<p>where <span class="math inline">\(\delta_i\)</span> is 1 if the event was observed and 0 if censored. Although we’ll stick to the Poisson model in this post, we can use these ideas to create a likelihood for many different choices of distribution by using the appropriate probability/survival functions <span class="math inline">\(f\)</span>, <span class="math inline">\(S\)</span>.</p>
<h2 id="implementation">Implementation</h2>
<p>We will fit this model using Stan because it is relatively easy to write a Bayesian model once we have understood the data generating process. This will require us to define prior distributions (on <span class="math inline">\(\mu\)</span>) just like with any Bayesian method. Since we are mostly interested in understanding the likelihood here, we will not give much consideration to the prior. However, if applying this to a real problem, it would be a good idea to give this more thought in a <a href="https://betanalpha.github.io/assets/case_studies/principled_bayesian_workflow.html">principled Bayesian workflow</a>.</p>
<h3 id="terminology">Terminology</h3>
<p>Stan makes the following abbreviations:</p>
<dl>
<dt><code>pmf</code></dt>
<dd>probability mass function, <span class="math inline">\(f(tto_i \mid \mu) = \text{Poisson}(tto_i \mid \mu)\)</span>
</dd>
<dt><code>lpmf</code></dt>
<dd>log(<code>pmf</code>)
</dd>
<dt><code>ccdf</code></dt>
<dd>survival function, a.k.a. complementary cumulative distribution function, <span class="math inline">\(S(tto_i | \mu) := \sum_{t &gt; tto_i} \text{Poisson}(t | \mu)\)</span>
</dd>
<dt><code>lccdf</code></dt>
<dd>log(<code>ccdf</code>)
</dd>
<dt><code>target</code></dt>
<dd>log(posterior probability density) = log(likelihood x prior).
</dd>
</dl>
<p>Stan uses the log-scale for its calculations, so we will need the log-likelihood:</p>
<p class="mathjaxWide"><span class="math display">\[
\log L(\mu) 
:= 
\sum_{i = 1}^N \delta_i \log f(tto_i \mid \mu)
+
\sum_{i = 1}^N (1 - \delta_i) \log S(tto_i \mid \mu)
.
\]</span></p>
<h3 id="the-model">The model</h3>
<p>In <a href="./censored_poisson.stan">our model</a>, we add the <code>lccdf</code> if the observation is censored and <code>lpmf</code> if not censored. Let’s load our model and take a look.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">model &lt;-<span class="st"> </span><span class="kw">stan_model</span>(<span class="st">&#39;censored_poisson.stan&#39;</span>)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">model</code></pre></div>
<pre><code>S4 class stanmodel &#39;censored_poisson&#39; coded as follows:
data {
  // the input data
  int&lt;lower = 1&gt; n;                      // number of observations
  int&lt;lower = 0&gt; tto[n];                 // tto is a list of ints
  int&lt;lower = 0, upper = 1&gt; censored[n]; // list of 0s and 1s
  
  // parameters for the prior
  real&lt;lower = 0&gt; shape;
  real&lt;lower = 0&gt; rate;
}

parameters {
  // the parameters used in our model
  real&lt;lower = 0&gt; mu; 
}

model {
  // posterior = prior * likelihood
  
  // prior
  mu ~ gamma(shape, rate);
  
  // likelihood
  for (i in 1:n) {
    if (censored[i]) {
      target += poisson_lccdf(tto[i] | mu);  
    } else {
      target += poisson_lpmf(tto[i] | mu);
    }
  }
  
} </code></pre>
<p>The language used in the Stan model is slightly different from R-notation but hopefully intuitive enough to convince yourself that it’s the same model we described above.</p>
<p>Now we can sample from the posterior of our model.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit &lt;-<span class="st"> </span>model <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">sampling</span>(
    <span class="dt">data =</span> <span class="kw">compose_data</span>(df, <span class="dt">shape =</span> <span class="dv">2</span>, <span class="dt">rate =</span> <span class="fl">0.05</span>),
    <span class="dt">iter =</span> <span class="dv">2000</span>,
    <span class="dt">warmup =</span> <span class="dv">500</span>
  ) 

fit</code></pre></div>
<pre><code>Inference for Stan model: censored_poisson.
4 chains, each with iter=2000; warmup=500; thin=1; 
post-warmup draws per chain=1500, total post-warmup draws=6000.

          mean se_mean   sd      2.5%       25%       50%       75%
mu        9.98    0.00 0.03      9.92      9.96      9.98     10.01
lp__ -19613.55    0.01 0.69 -19615.54 -19613.71 -19613.29 -19613.09
         97.5% n_eff Rhat
mu       10.05  2002    1
lp__ -19613.04  2924    1

Samples were drawn using NUTS(diag_e) at Sat Aug  4 15:49:36 2018.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).</code></pre>
<p>Stan has an amazing array of diagnostics to check the quality of the fitted model. Since our model is fairly simple and all checks are in order, I won’t describe them here.</p>
<p>The point estimate for <code>mu</code> is 9.98 and the true value is contained within the 95% credible interval [9.92, 10.05]. We can also plot all the samples from our posterior.</p>
<figure>
<img src="censoring_files/figure-markdown/estimate_histogram-1.png" />
</figure>
<h2 id="conclusion">Conclusion</h2>
<p>We have seen how to change the likelihood to take censored observations into account. Moreover, the same process works for most distributions, so you can swap out the Poisson for Weibull/gamma/lognormal or whatever you want. Using the Bayesian modelling language, Stan, makes it super easy to test your statistical intuitions by turning them into a workable model, so I’ll definitely be exploring Stan more in the future.</p>

<div id="disqus_thread"></div>
<script>
/**
 *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
 *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
 */
/*
   var disqus_config = function () {
   this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
   this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
   };
 */
(function() {  // DON'T EDIT BELOW THIS LINE
 var d = document, s = d.createElement('script');

 s.src = '//stappit-github-io.disqus.com/embed.js';

 s.setAttribute('data-timestamp', +new Date());
 (d.head || d.body).appendChild(s);
 })();
</script>

<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

<script id="dsq-count-scr" src="//stappit-github-io.disqus.com/count.js" async></script>
]]></summary>
</entry>

</feed>
