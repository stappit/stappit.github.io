<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>Thoughts from the Café</title>
    <link href="http://stappit.github.io/atom.xml" rel="self" />
    <link href="http://stappit.github.io" />
    <id>http://stappit.github.io/atom.xml</id>
    <author>
        <name>Brian</name>
        <email>ha@hahaha.com</email>
    </author>
    <updated>2018-09-03T00:00:00Z</updated>
    <entry>
    <title>BDA3 Chapter 2 Exercise 14</title>
    <link href="http://stappit.github.io/posts/bda3/chapter_02_exercise_14.html" />
    <id>http://stappit.github.io/posts/bda3/chapter_02_exercise_14.html</id>
    <published>2018-09-03T00:00:00Z</published>
    <updated>2018-09-03T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 class="post-title">BDA3 Chapter 2 Exercise 14</h1>

<div class="card post-meta border-0">
  <div class="card-body post-meta">
    <p class="card-text text-muted text-left">
    Posted on September  3, 2018  by Brian </br>
     Tags: <a href="/tags/bda%20chapter%202.html">bda chapter 2</a>, <a href="/tags/solutions.html">solutions</a>, <a href="/tags/bayes.html">bayes</a>, <a href="/tags/normal.html">normal</a>, <a href="/tags/conjugate%20prior.html">conjugate prior</a> </br>
     Category: <a href="/categories/bda3.html">bda3</a> 
    </p>
  </div>
</div>

<p>Here’s my solution to exercise 14, chapter 2, of <a href="https://andrewgelman.com/">Gelman’s</a> <em>Bayesian Data Analysis</em> (BDA), 3rd edition. There are <a href="http://www.stat.columbia.edu/~gelman/book/solutions.pdf">solutions</a> to some of the exercises on the <a href="http://www.stat.columbia.edu/~gelman/book/">book’s webpage</a>.</p>
<!--more-->
<div style="display:none">
<p class="mathjaxWide"><span class="math inline">\(\DeclareMathOperator{\dbinomial}{binomial}  \DeclareMathOperator{\dbern}{Bernoulli}  \DeclareMathOperator{\dpois}{Poisson}  \DeclareMathOperator{\dnorm}{normal}  \DeclareMathOperator{\dcauchy}{Cauchy}  \DeclareMathOperator{\dgamma}{gamma}  \DeclareMathOperator{\invlogit}{invlogit}  \DeclareMathOperator{\logit}{logit}  \DeclareMathOperator{\dbeta}{beta}\)</span></p>
</div>
<p>Suppose we have a normal prior <span class="math inline">\(\theta \sim \dnorm (\mu_0, \frac{1}{\tau_0})\)</span> and a normal sampling distribution <span class="math inline">\(y \mid \theta \sim \dnorm(\theta, \sigma)\)</span>, where the variance is known. We will show by induction that the posterior is <span class="math inline">\(\theta \mid y_1, \dotsc, y_{n} \sim \dnorm(\mu_{n}, \frac{1}{\tau_{n}})\)</span> where</p>
<p class="mathjaxWide"><span class="math display">\[
  \frac{1}{\tau_{n}^2} = \frac{1}{\tau_0^2} + \frac{n}{\sigma^2}
  \quad
  \text{and}
  \quad
  \mu_n = \frac{\frac{1}{\tau_0^2}\mu_0 + \frac{n}{\sigma^2}\bar y_n}{\frac{1}{\tau_0^2} + \frac{n}{\sigma^2}}
\]</span></p>
<p>for <span class="math inline">\(n = 1, \dotsc, \infty\)</span>.</p>
<h2 id="base-case">Base case</h2>
<p>The case <span class="math inline">\(n = 1\)</span> can be reexpressed as</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
  \frac{1}{\tau_1^2} = \frac{\sigma^2 + \tau_0^2}{\sigma^2\tau_0^2}
  \quad
  \text{and}
  \quad
  \mu_1 = \frac{\sigma_0^2\mu_0 + \tau_0^2y}{\sigma^2 + \tau_0^2}
  .
\end{align}
\]</span></p>
<p>Now we can combine the fractions in the exponent, expand the brackets, collect the terms as <span class="math inline">\(\theta\)</span>-coefficients, rewrite the coefficients in terms of <span class="math inline">\(\mu_1\)</span> and <span class="math inline">\(\tau_1\)</span>, then complete the square in terms of <span class="math inline">\(\theta\)</span>:</p>
$$
<span class="math display">\[\begin{align}
  \frac{(y - \theta)^2}{\sigma^2}
  +
  \frac{(\theta - \mu_0)^2}{\tau_0^2}
  
  &amp;=
  \frac{(y^2 + \theta^2 - 2\theta y) \tau_0^2 + (\theta^2 + \mu_0^2 - 2 \theta \mu_0) \sigma^2}{\sigma^2 \tau_0^2}
  
  \\
  &amp;=
  \frac{\theta^2 (\tau_0^2 + \sigma^2) -2 \theta (\sigma^2\mu_0 + \tau_0^2y) + (y^2\tau_0^2 + \mu_0^2\sigma^2)}{\sigma^2\tau_0^2}
  \\
  &amp;=
  \frac{\theta^2 (\tau_0^2 + \sigma^2) -2 \theta \mu_1 (\sigma^2 + \tau_0^2) + \mu_1 (\sigma^2 + \tau_0^2) }{\sigma^2\tau_0^2}
  \\
  &amp;=
  \theta^2\frac{1}{\tau_1^2} -2\theta \mu_1 \frac{1}{\tau_1^2} + \mu_1 \frac{1}{\tau_1^2}
  \\
  &amp;=
  \frac{\theta^2 -2 \mu_1 \theta + \mu_1^2}{\tau_1^2} 
  \\
  &amp;=
  \frac{(\theta - \mu_1)^2}{\tau_1^2} 
  .
\end{align}\]</span>
<p>$$</p>
<h2 id="induction-step">Induction step</h2>
<p>The induction hypothesis is that the variance and mean are given by</p>
<p class="mathjaxWide"><span class="math display">\[
  \frac{1}{\tau_n^2} = \frac{1}{\tau_0^2} + \frac{n}{\sigma^2}
  \quad
  \text{and}
  \quad
  \mu_n = \frac{\frac{1}{\tau_0^2}\mu_0 + \frac{n}{\sigma^2}\bar y}{\frac{1}{\tau_0^2} + \frac{n}{\sigma^2}}
\]</span></p>
<p>for some <span class="math inline">\(n \ge 1\)</span>.</p>
<p>Starting with the variance, the base step can be reexpressed as</p>
<p class="mathjaxWide"><span class="math display">\[
\frac{1}{\tau_{n+1}^2} 
=
\frac{1}{\tau_n^2} + \frac{1}{\sigma^2}
.
\]</span></p>
<p>Now apply the induction hypothesis to get</p>
<p class="mathjaxWide"><span class="math display">\[
\frac{1}{\tau_{n+1}^2} 
=
\frac{1}{\tau_0^2} + \frac{n}{\sigma^2} + \frac{1}{\sigma^2}
=
\frac{1}{\tau_0^2} + \frac{n+1}{\sigma^2}
.
\]</span></p>
<p>For the mean we apply the same strategy. The base step can be reexpressed as</p>
<p class="mathjaxWide"><span class="math display">\[
\mu_{n+1}
=
\frac{
  \frac{1}{\tau_n^2}\mu_n + \frac{1}{\sigma^2}y_{n+1}
}{
  \frac{1}{\tau_n^2} + \frac{1}{\sigma^2}
}
.
\]</span></p>
<p>Applying the induction hypothesis then gives</p>
<span class="math display">\[\begin{align}
  \mu_{n+1}
  &amp;=
  \frac{
    \frac{1}{\tau_n^2} \left( \frac{\frac{1}{\tau_0^2}\mu_0 + \frac{n}{\sigma^2}\bar y_n}{\frac{1}{\tau_0^2} + \frac{n}{\sigma^2}} \right) + \frac{1}{\sigma^2}y_{n+1}
  }{
    \frac{1}{\tau_0^2} + \frac{n+1}{\sigma^2}}
  
  \\
  &amp;=
  \frac{
    \frac{1}{\tau_0^2}\mu_0 + \frac{n}{\sigma^2}\bar y_n + \frac{1}{\sigma^2}y_{n+1}
  }{
    \frac{1}{\tau_0^2} + \frac{n+1}{\sigma^2}
  }
  
  \\
  &amp;=
  \frac{
    \frac{1}{\tau_0^2}\mu_0 + \frac{n+1}{\sigma^2}\bar y_{n+1}
  }{
    \frac{1}{\tau_0^2} + \frac{n+1}{\sigma^2}
  }
  ,
\end{align}\]</span>
<p>since</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
(n + 1) \bar y_{n + 1}
&amp;:=
\sum_1^{n+1} y_i
\\
&amp;=
y_{n+1} + \sum_1^n y_i
\\
&amp;=
y_{n+1} + n\bar y_n
.
\end{align}
\]</span></p>

<div id="disqus_thread"></div>
<script>
/**
 *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
 *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
 */
/*
   var disqus_config = function () {
   this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
   this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
   };
 */
(function() {  // DON'T EDIT BELOW THIS LINE
 var d = document, s = d.createElement('script');

 s.src = '//stappit-github-io.disqus.com/embed.js';

 s.setAttribute('data-timestamp', +new Date());
 (d.head || d.body).appendChild(s);
 })();
</script>

<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

<script id="dsq-count-scr" src="//stappit-github-io.disqus.com/count.js" async></script>
]]></summary>
</entry>
<entry>
    <title>BDA3 Chapter 2 Exercise 13</title>
    <link href="http://stappit.github.io/posts/bda3/chapter_02_exercise_13.html" />
    <id>http://stappit.github.io/posts/bda3/chapter_02_exercise_13.html</id>
    <published>2018-09-02T00:00:00Z</published>
    <updated>2018-09-02T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 class="post-title">BDA3 Chapter 2 Exercise 13</h1>

<div class="card post-meta border-0">
  <div class="card-body post-meta">
    <p class="card-text text-muted text-left">
    Posted on September  2, 2018  by Brian </br>
     Tags: <a href="/tags/bda%20chapter%202.html">bda chapter 2</a>, <a href="/tags/solutions.html">solutions</a>, <a href="/tags/bayes.html">bayes</a>, <a href="/tags/poisson.html">poisson</a>, <a href="/tags/gamma.html">gamma</a>, <a href="/tags/jeffrey%20prior.html">jeffrey prior</a>, <a href="/tags/posterior%20predictive.html">posterior predictive</a> </br>
     Category: <a href="/categories/bda3.html">bda3</a> 
    </p>
  </div>
</div>

<p>Here’s my solution to exercise 13, chapter 2, of <a href="https://andrewgelman.com/">Gelman’s</a> <em>Bayesian Data Analysis</em> (BDA), 3rd edition. There are <a href="http://www.stat.columbia.edu/~gelman/book/solutions.pdf">solutions</a> to some of the exercises on the <a href="http://www.stat.columbia.edu/~gelman/book/">book’s webpage</a>.</p>
<!--more-->
<div style="display:none">
<p class="mathjaxWide"><span class="math inline">\(\DeclareMathOperator{\dbinomial}{binomial} \DeclareMathOperator{\dbern}{Bernoulli} \DeclareMathOperator{\dpois}{Poisson} \DeclareMathOperator{\dnorm}{normal} \DeclareMathOperator{\dcauchy}{Cauchy} \DeclareMathOperator{\dgamma}{gamma} \DeclareMathOperator{\invlogit}{invlogit} \DeclareMathOperator{\logit}{logit} \DeclareMathOperator{\dbeta}{beta}\)</span></p>
</div>
<p>We are given data on airline deaths and asked to fit various models to that data.</p>
<h2 id="the-data">The data</h2>
<p>We are given the data shown below. The data didn’t seem to be available anywhere so I <a href="/data/bda3_chapter_02_exercise_13.csv">created the csv file</a> myself.</p>
<table class="table table-striped table-hover table-condensed table-responsive" style="margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:right;">
year
</th>
<th style="text-align:right;">
fatal_accidents
</th>
<th style="text-align:right;">
passenger_deaths
</th>
<th style="text-align:right;">
death_rate
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
1976
</td>
<td style="text-align:right;">
24
</td>
<td style="text-align:right;">
734
</td>
<td style="text-align:right;">
0.19
</td>
</tr>
<tr>
<td style="text-align:right;">
1977
</td>
<td style="text-align:right;">
25
</td>
<td style="text-align:right;">
516
</td>
<td style="text-align:right;">
0.12
</td>
</tr>
<tr>
<td style="text-align:right;">
1978
</td>
<td style="text-align:right;">
31
</td>
<td style="text-align:right;">
754
</td>
<td style="text-align:right;">
0.15
</td>
</tr>
<tr>
<td style="text-align:right;">
1979
</td>
<td style="text-align:right;">
31
</td>
<td style="text-align:right;">
877
</td>
<td style="text-align:right;">
0.16
</td>
</tr>
<tr>
<td style="text-align:right;">
1980
</td>
<td style="text-align:right;">
22
</td>
<td style="text-align:right;">
814
</td>
<td style="text-align:right;">
0.14
</td>
</tr>
<tr>
<td style="text-align:right;">
1981
</td>
<td style="text-align:right;">
21
</td>
<td style="text-align:right;">
362
</td>
<td style="text-align:right;">
0.06
</td>
</tr>
<tr>
<td style="text-align:right;">
1982
</td>
<td style="text-align:right;">
26
</td>
<td style="text-align:right;">
764
</td>
<td style="text-align:right;">
0.13
</td>
</tr>
<tr>
<td style="text-align:right;">
1983
</td>
<td style="text-align:right;">
20
</td>
<td style="text-align:right;">
809
</td>
<td style="text-align:right;">
0.13
</td>
</tr>
<tr>
<td style="text-align:right;">
1984
</td>
<td style="text-align:right;">
16
</td>
<td style="text-align:right;">
223
</td>
<td style="text-align:right;">
0.03
</td>
</tr>
<tr>
<td style="text-align:right;">
1985
</td>
<td style="text-align:right;">
22
</td>
<td style="text-align:right;">
1066
</td>
<td style="text-align:right;">
0.15
</td>
</tr>
</tbody>
</table>
<p>Let’s get acquainted with the data by plotting it as a timeseries.</p>
<figure>
<img src="chapter_02_exercise_13_files/figure-markdown/data_plot-1..svg" />
</figure>
<h2 id="part-a">Part a</h2>
<p>We model the number of fatal accidents as poisson <span class="math inline">\(y \mid \theta \sim \dpois(\theta)\)</span>, where we put a <span class="math inline">\(\theta \sim \dgamma(\alpha, \beta)\)</span> prior on the parameter. I don’t really have any strong prior knowledge about the number of annual fatal flight accidents. I’ll use the gamma approximation to Jeffrey’s prior from the <a href="./chapter_02_exercise_12.html">previous exercise</a>, even though it places probability on very extreme values. We’ll stick with this prior throughout.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">shape &lt;-<span class="st"> </span><span class="fl">0.5</span>
rate &lt;-<span class="st"> </span>.Machine<span class="op">$</span>double.xmin</code></pre></div>
<figure>
<img src="chapter_02_exercise_13_files/figure-markdown/prior_a-1..svg" />
</figure>
<p>The posterior is <span class="math inline">\(\dgamma(0.5 + n\bar y, n) = \dgamma(0.5 + 238, 10)\)</span>.</p>
<figure>
<img src="chapter_02_exercise_13_files/figure-markdown/posterior_a-1..svg" />
</figure>
<p>To obtain a 95% posterior predictive interval, we draw <span class="math inline">\(\theta\)</span> from its posterior, then draw <span class="math inline">\(y\)</span> from the corresponding Poisson distribution. With these draws, we can obtain the necessary quantiles.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">n_draws &lt;-<span class="st"> </span><span class="dv">50000</span>

theta_posterior_a &lt;-<span class="st"> </span><span class="kw">rgamma</span>(n_draws, 
                            shape <span class="op">+</span><span class="st"> </span>sum_fatal_accidents, 
                            rate <span class="op">+</span><span class="st"> </span>n_observations
                           ) 
y_pp_a &lt;-<span class="st"> </span><span class="kw">rpois</span>(n_draws, theta_posterior_a) 

mu_a &lt;-<span class="st"> </span><span class="kw">mean</span>(y_pp_a)
ci_a &lt;-<span class="st"> </span><span class="kw">quantile</span>(y_pp_a, <span class="kw">c</span>(<span class="fl">0.05</span>, <span class="fl">0.95</span>))

ci_a</code></pre></div>
<pre><code> 5% 95% 
 16  32 </code></pre>
<figure>
<img src="chapter_02_exercise_13_files/figure-markdown/pp_plot_a-1..svg" />
</figure>
<h2 id="part-b">Part b</h2>
<p>In part a, we ignored how many flights there are. We can incorporate this information into our model by using <code>passenger_miles</code> as a measure of exposure. The parameter <span class="math inline">\(\theta\)</span> is now the rate of fatal accidents per year per 100 million passenger miles. Note that this rate is over an order of magnitude smaller than the death rate in the table because the number of fatal accidents is an order of magnitude smaller than the number of passenger deaths. The posterior is <span class="math inline">\(\theta \mid y \sim \dgamma(0.5 + 238, 57158.69)\)</span>.</p>
<figure>
<img src="chapter_02_exercise_13_files/figure-markdown/posterior_b-1..svg" />
</figure>
<p>The 95% posterior predictive interval seems to be shifted upwards compared to the interval in part a.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">theta_posterior_b &lt;-<span class="st"> </span><span class="kw">rgamma</span>(n_draws, 
                            shape <span class="op">+</span><span class="st"> </span>sum_fatal_accidents, 
                            rate <span class="op">+</span><span class="st"> </span>sum_passenger_miles
                           )
y_pp_b &lt;-<span class="st"> </span><span class="kw">rpois</span>(n_draws, theta_posterior_b <span class="op">*</span><span class="st"> </span><span class="dv">8000</span>)

mu_b &lt;-<span class="st"> </span><span class="kw">mean</span>(y_pp_b)

ci_b &lt;-<span class="st"> </span><span class="kw">quantile</span>(y_pp_b, <span class="kw">c</span>(<span class="fl">0.05</span>, <span class="fl">0.95</span>))

ci_b</code></pre></div>
<pre><code> 5% 95% 
 24  44 </code></pre>
<figure>
<img src="chapter_02_exercise_13_files/figure-markdown/pp_plot_b-1..svg" />
</figure>
<h2 id="part-c">Part c</h2>
<p>Here we use the same model as in part a but for the number of passenger deaths instead of fatal accidents.</p>
<figure>
<img src="chapter_02_exercise_13_files/figure-markdown/posterior_c-1..svg" />
</figure>
<p>Only 1 of the 10 observations in the dataset lie within the 95% posterior predictive interval.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">theta_posterior_c &lt;-<span class="st"> </span><span class="kw">rgamma</span>(n_draws, 
                            shape <span class="op">+</span><span class="st"> </span>sum_passenger_deaths, 
                            rate <span class="op">+</span><span class="st"> </span>n_observations
                           )
y_pp_c &lt;-<span class="st"> </span><span class="kw">rpois</span>(n_draws, theta_posterior_c) 

mu_c &lt;-<span class="st"> </span><span class="kw">mean</span>(y_pp_c)
ci_c &lt;-<span class="st"> </span><span class="kw">quantile</span>(y_pp_c, <span class="kw">c</span>(<span class="fl">0.05</span>, <span class="fl">0.95</span>))

ci_c</code></pre></div>
<pre><code> 5% 95% 
647 738 </code></pre>
<figure>
<img src="chapter_02_exercise_13_files/figure-markdown/pp_plot_c-1..svg" />
</figure>
<h2 id="part-d">Part d</h2>
<p>Now we use the same model as in part b but for passenger deaths instead of fatal accidents. The posterior is <span class="math inline">\(\dgamma(0.5 + 238, 57158.69)\)</span>.</p>
<figure>
<img src="chapter_02_exercise_13_files/figure-markdown/posterior_d-1..svg" />
</figure>
<p>None of the observed values falls into the 95% posterior predictive interval.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">theta_posterior_d &lt;-<span class="st"> </span><span class="kw">rgamma</span>(n_draws, 
                            shape <span class="op">+</span><span class="st"> </span>sum_passenger_deaths, 
                            rate <span class="op">+</span><span class="st"> </span>sum_passenger_miles
                           )
y_pp_d &lt;-<span class="st"> </span><span class="kw">rpois</span>(n_draws, theta_posterior_d <span class="op">*</span><span class="st"> </span><span class="dv">8000</span>)

mu_d &lt;-<span class="st"> </span><span class="kw">mean</span>(y_pp_d)

ci_d &lt;-<span class="st"> </span><span class="kw">quantile</span>(y_pp_d, <span class="kw">c</span>(<span class="fl">0.05</span>, <span class="fl">0.95</span>))

ci_d</code></pre></div>
<pre><code>  5%  95% 
 914 1023 </code></pre>
<figure>
<img src="chapter_02_exercise_13_files/figure-markdown/pp_plot_d-1..svg" />
</figure>
<h2 id="part-e">Part e</h2>
<p>There are a number of issues to consider that are not mentioned in the question or suggested by the data. The number of fatal accidents depends on the number of miles flown by airplanes: if there are more flights, there will likely be more accidents. However, the number of flights isn’t directly accounted for in the number of passenger miles since the number of passengers per flight can vary from year to year. In any case, the number of passenger deaths per year is not independent because passengers on the same flight will have more similar survival chances.</p>

<div id="disqus_thread"></div>
<script>
/**
 *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
 *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
 */
/*
   var disqus_config = function () {
   this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
   this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
   };
 */
(function() {  // DON'T EDIT BELOW THIS LINE
 var d = document, s = d.createElement('script');

 s.src = '//stappit-github-io.disqus.com/embed.js';

 s.setAttribute('data-timestamp', +new Date());
 (d.head || d.body).appendChild(s);
 })();
</script>

<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

<script id="dsq-count-scr" src="//stappit-github-io.disqus.com/count.js" async></script>
]]></summary>
</entry>
<entry>
    <title>BDA3 Chapter 2 Exercise 12</title>
    <link href="http://stappit.github.io/posts/bda3/chapter_02_exercise_12.html" />
    <id>http://stappit.github.io/posts/bda3/chapter_02_exercise_12.html</id>
    <published>2018-09-01T00:00:00Z</published>
    <updated>2018-09-01T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 class="post-title">BDA3 Chapter 2 Exercise 12</h1>

<div class="card post-meta border-0">
  <div class="card-body post-meta">
    <p class="card-text text-muted text-left">
    Posted on September  1, 2018  by Brian </br>
     Tags: <a href="/tags/bda%20chapter%202.html">bda chapter 2</a>, <a href="/tags/solutions.html">solutions</a>, <a href="/tags/bayes.html">bayes</a>, <a href="/tags/poisson.html">poisson</a>, <a href="/tags/jeffrey%20prior.html">jeffrey prior</a>, <a href="/tags/gamma.html">gamma</a> </br>
     Category: <a href="/categories/bda3.html">bda3</a> 
    </p>
  </div>
</div>

<p>Here’s my solution to exercise 12, chapter 2, of <a href="https://andrewgelman.com/">Gelman’s</a> <em>Bayesian Data Analysis</em> (BDA), 3rd edition. There are <a href="http://www.stat.columbia.edu/~gelman/book/solutions.pdf">solutions</a> to some of the exercises on the <a href="http://www.stat.columbia.edu/~gelman/book/">book’s webpage</a>.</p>
<!--more-->
<div style="display:none">
<p class="mathjaxWide"><span class="math inline">\(\DeclareMathOperator{\dbinomial}{binomial} \DeclareMathOperator{\dbern}{Bernoulli} \DeclareMathOperator{\dnorm}{normal} \DeclareMathOperator{\dcauchy}{Cauchy} \DeclareMathOperator{\dgamma}{gamma} \DeclareMathOperator{\invlogit}{invlogit} \DeclareMathOperator{\logit}{logit} \DeclareMathOperator{\dbeta}{beta}\)</span></p>
</div>
<p>Suppose <span class="math inline">\(\theta\)</span> has a Poisson likelihood so that <span class="math inline">\(\log p(y \mid \theta) \propto y \log(\theta) - \theta\)</span>. We will find Jeffrey’s prior for <span class="math inline">\(\theta\)</span> and the gamma distribution that most closely approximates it.</p>
<p>The derivative of the log likelihood is <span class="math inline">\(\frac{y}{\theta} - 1\)</span> and the second derivative is <span class="math inline">\(-\frac{y}{\theta^2}\)</span>. It follows that the Fisher information for <span class="math inline">\(\theta\)</span> is</p>
<p class="mathjaxWide"><span class="math display">\[
J(\theta)
=
\mathbb E \left( \frac{y}{\theta^2} \right)
=
\frac{1}{\theta}
,
\]</span></p>
<p>so Jeffrey’s prior is <span class="math inline">\(p(\theta) \propto \frac{1}{\sqrt{\theta}}\)</span>. This is an improper prior because</p>
<p class="mathjaxWide"><span class="math display">\[
\int_0^\infty \theta^{-\frac{1}{2}} d\theta
=
\left[ 2\theta^{\frac{1}{2}} \right]_0^\infty
=
\infty
.
\]</span></p>
<p>Since Jeffrey’s prior is improper, we can try approximate it with a gamma prior. Let <span class="math inline">\(\alpha, \beta \in (0, \infty)\)</span> be the shape and rate parameters of a gamma distribution. Then</p>
<p class="mathjaxWide"><span class="math display">\[
\dgamma(\theta \mid \alpha, \beta) 
\propto
x^{\alpha - 1}e^{-\beta x}
.
\]</span></p>
<p>Choosing <span class="math inline">\(\alpha = \frac{1}{2}\)</span> and <span class="math inline">\(\beta = 0\)</span> yields Jeffrey’s prior. However, <span class="math inline">\(\beta\)</span> must be positive for the gamma distribution to be proper, so we can choose <span class="math inline">\(\beta = \epsilon\)</span> sufficiently small. We’ll use the <a href="https://stat.ethz.ch/R-manual/R-devel/library/base/html/zMachine.html">smallest positive float</a> representable in R.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">epsilon &lt;-<span class="st"> </span>.Machine<span class="op">$</span>double.xmin

upper_limit &lt;-<span class="st"> </span><span class="dv">100000000</span>
step &lt;-<span class="st"> </span>upper_limit <span class="op">/</span><span class="st"> </span><span class="dv">10000</span>

prior &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">theta =</span> <span class="kw">seq</span>(<span class="dv">0</span>, upper_limit, step)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">density =</span> <span class="kw">dgamma</span>(theta, <span class="fl">0.5</span>, epsilon)) </code></pre></div>
<figure>
<img src="chapter_02_exercise_12_files/figure-markdown/gamma_jeffrey_plot-1.png" />
</figure>

<div id="disqus_thread"></div>
<script>
/**
 *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
 *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
 */
/*
   var disqus_config = function () {
   this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
   this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
   };
 */
(function() {  // DON'T EDIT BELOW THIS LINE
 var d = document, s = d.createElement('script');

 s.src = '//stappit-github-io.disqus.com/embed.js';

 s.setAttribute('data-timestamp', +new Date());
 (d.head || d.body).appendChild(s);
 })();
</script>

<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

<script id="dsq-count-scr" src="//stappit-github-io.disqus.com/count.js" async></script>
]]></summary>
</entry>
<entry>
    <title>BDA3 Chapter 2 Exercise 11</title>
    <link href="http://stappit.github.io/posts/bda3/chapter_02_exercise_11.html" />
    <id>http://stappit.github.io/posts/bda3/chapter_02_exercise_11.html</id>
    <published>2018-09-01T00:00:00Z</published>
    <updated>2018-09-01T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 class="post-title">BDA3 Chapter 2 Exercise 11</h1>

<div class="card post-meta border-0">
  <div class="card-body post-meta">
    <p class="card-text text-muted text-left">
    Posted on September  1, 2018  by Brian </br>
     Tags: <a href="/tags/bda%20chapter%202.html">bda chapter 2</a>, <a href="/tags/solutions.html">solutions</a>, <a href="/tags/bayes.html">bayes</a>, <a href="/tags/cauchy.html">cauchy</a>, <a href="/tags/posterior%20predictive.html">posterior predictive</a>, <a href="/tags/grid%0Aapproximation.html">grid
approximation</a> </br>
     Category: <a href="/categories/bda3.html">bda3</a> 
    </p>
  </div>
</div>

<p>Here’s my solution to exercise 11, chapter 2, of <a href="https://andrewgelman.com/">Gelman’s</a> <em>Bayesian Data Analysis</em> (BDA), 3rd edition. There are <a href="http://www.stat.columbia.edu/~gelman/book/solutions.pdf">solutions</a> to some of the exercises on the <a href="http://www.stat.columbia.edu/~gelman/book/">book’s webpage</a>.</p>
<!--more-->
<div style="display:none">
<p class="mathjaxWide"><span class="math inline">\(\DeclareMathOperator{\dbinomial}{binomial} \DeclareMathOperator{\dbern}{Bernoulli} \DeclareMathOperator{\dnorm}{normal} \DeclareMathOperator{\dcauchy}{Cauchy} \DeclareMathOperator{\dgamma}{gamma} \DeclareMathOperator{\invlogit}{invlogit} \DeclareMathOperator{\logit}{logit} \DeclareMathOperator{\dbeta}{beta}\)</span></p>
</div>
<p>Assume the sampling distribution is <span class="math inline">\(\dcauchy(y \mid \theta, 1)\)</span> with uniform prior <span class="math inline">\(p(\theta) \propto 1\)</span> on <span class="math inline">\([0, 100]\)</span>. Given observations <span class="math inline">\(y\)</span>, we can approximate the posterior for <span class="math inline">\(\theta\)</span> by dividing the interval <span class="math inline">\([0, 100]\)</span> into partitions of length <span class="math inline">\(\frac{1}{m}\)</span>. The unnormalised posterior for <span class="math inline">\(\theta\)</span> on this grid is then computed as follows.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># observations</span>
y &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">43</span>, <span class="dv">44</span>, <span class="dv">45</span>, <span class="fl">46.5</span>, <span class="fl">47.5</span>) 

<span class="co"># grid granularity</span>
m &lt;-<span class="st"> </span><span class="dv">100</span>

<span class="co"># L(θ) := p(y | θ)</span>
likelihood &lt;-<span class="st"> </span><span class="cf">function</span>(theta) 
  y <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">map</span>(dcauchy, theta, <span class="dv">1</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">reduce</span>(prod)

<span class="co"># unnormalised posterior grid</span>
posterior_unnorm &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">theta =</span> <span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">100</span>, <span class="dv">1</span> <span class="op">/</span><span class="st"> </span>m)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">density =</span> <span class="kw">map</span>(theta, likelihood) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">unlist</span>())</code></pre></div>
<p>We can approximate the normalising constant by summing the approximate area on each partition. Each partition has width <span class="math inline">\(\frac{1}{m}\)</span> and approximate height given by the density, so the approximate area is the multiple of the two.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># grid approx to area under curve</span>
normalising_constant &lt;-<span class="st"> </span>posterior_unnorm <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">summarise</span>(<span class="kw">sum</span>(density) <span class="op">/</span><span class="st"> </span>m) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">pull</span>()

<span class="co"># normalised posterior grid</span>
posterior &lt;-<span class="st"> </span>posterior_unnorm <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">density =</span> density <span class="op">/</span><span class="st"> </span>normalising_constant)

normalising_constant</code></pre></div>
<pre><code>[1] 3.418359e-05</code></pre>
<figure>
<img src="chapter_02_exercise_11_files/figure-markdown/posterior_plot-1.png" />
</figure>
<p>Let’s zoom in on the region <span class="math inline">\([40, 50]\)</span> where most of the density lies.</p>
<figure>
<img src="chapter_02_exercise_11_files/figure-markdown/posterior_plot_zoomed-1.png" />
</figure>
<p>Sampling from this posterior yields a histogram with a similar shape.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">posterior_draws &lt;-<span class="st"> </span>posterior <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">sample_n</span>(<span class="dv">1000</span>, <span class="dt">replace =</span> <span class="ot">TRUE</span>, <span class="dt">weight =</span> density) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">select</span>(theta)</code></pre></div>
<figure>
<img src="chapter_02_exercise_11_files/figure-markdown/posterior_sample_plot-1.png" />
</figure>
<p>We can draw from the posterior predictive distribution by first drawing <span class="math inline">\(\tilde\theta\)</span> from the posterior of <span class="math inline">\(\theta\)</span>, then drawing <span class="math inline">\(\tilde y\)</span> from <span class="math inline">\(\dcauchy(\tilde\theta, 1)\)</span>. The tails of the posterior predictive distribution are much wider than for <span class="math inline">\(\theta\)</span> so we plot this histogram on the interval <span class="math inline">\([10, 90]\)</span> (although there are a few observations outside this interval).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">posterior_predictive &lt;-<span class="st"> </span>posterior_draws <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">pp =</span> <span class="kw">rcauchy</span>(<span class="kw">n</span>(), theta, <span class="dv">1</span>)) </code></pre></div>
<figure>
<img src="chapter_02_exercise_11_files/figure-markdown/posterior_predictive_plot-1.png" />
</figure>

<div id="disqus_thread"></div>
<script>
/**
 *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
 *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
 */
/*
   var disqus_config = function () {
   this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
   this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
   };
 */
(function() {  // DON'T EDIT BELOW THIS LINE
 var d = document, s = d.createElement('script');

 s.src = '//stappit-github-io.disqus.com/embed.js';

 s.setAttribute('data-timestamp', +new Date());
 (d.head || d.body).appendChild(s);
 })();
</script>

<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

<script id="dsq-count-scr" src="//stappit-github-io.disqus.com/count.js" async></script>
]]></summary>
</entry>
<entry>
    <title>BDA3 Chapter 2 Exercise 10</title>
    <link href="http://stappit.github.io/posts/bda3/chapter_02_exercise_10.html" />
    <id>http://stappit.github.io/posts/bda3/chapter_02_exercise_10.html</id>
    <published>2018-09-01T00:00:00Z</published>
    <updated>2018-09-01T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 class="post-title">BDA3 Chapter 2 Exercise 10</h1>

<div class="card post-meta border-0">
  <div class="card-body post-meta">
    <p class="card-text text-muted text-left">
    Posted on September  1, 2018  by Brian </br>
     Tags: <a href="/tags/bda%20chapter%202.html">bda chapter 2</a>, <a href="/tags/solutions.html">solutions</a>, <a href="/tags/bayes.html">bayes</a>, <a href="/tags/geometric.html">geometric</a>, <a href="/tags/noninformative%20prior.html">noninformative prior</a>, <a href="/tags/jeffrey%20prior.html">jeffrey prior</a> </br>
     Category: <a href="/categories/bda3.html">bda3</a> 
    </p>
  </div>
</div>

<p>Here’s my solution to exercise 10, chapter 2, of <a href="https://andrewgelman.com/">Gelman’s</a> <em>Bayesian Data Analysis</em> (BDA), 3rd edition. There are <a href="http://www.stat.columbia.edu/~gelman/book/solutions.pdf">solutions</a> to some of the exercises on the <a href="http://www.stat.columbia.edu/~gelman/book/">book’s webpage</a>.</p>
<!--more-->
<div style="display:none">
<p class="mathjaxWide"><span class="math inline">\(\DeclareMathOperator{\dbinomial}{binomial} \DeclareMathOperator{\dbern}{Bernoulli} \DeclareMathOperator{\dnorm}{normal} \DeclareMathOperator{\dgamma}{gamma} \DeclareMathOperator{\invlogit}{invlogit} \DeclareMathOperator{\logit}{logit} \DeclareMathOperator{\dbeta}{beta}\)</span></p>
</div>
<h2 id="the-posterior-density">The posterior density</h2>
<p>There an N cars labelled 1 to N and we observe a random car labelled 203. With a geometric prior with mean 100, the unnormalised posterior is</p>
<p class="mathjaxWide"><span class="math display">\[
p(N \mid y = 203)
\propto
p(y = 203 \mid N) \cdot p (N)
= 
\left.
  \begin{cases}
    \frac{1}{N} \cdot \frac{1}{100} \cdot \left( \frac{99}{100} \right)^{N - 1} 
    &amp; 
    \text{for } 203 \le N 
    \\
    0
    &amp;
    \text{otherwise}
  \end{cases}
\right\}
\]</span></p>
<p>We find the normalising constant in two steps. First set <span class="math inline">\(x := \frac{99}{100}\)</span> and use the <a href="https://en.wikipedia.org/wiki/Taylor_series#Natural_logarithm">Taylor series</a> of <span class="math inline">\(\log(1 - x)\)</span> to show</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
  \sum_1^\infty \frac{1}{N} \cdot \frac{1}{100}\cdot \left( \frac{99}{100} \right)^{N - 1}
  &amp;=
  \frac{1}{100} \left( 1 + \frac{x}{2} + \frac{x^2}{3} + \dotsc + \frac{x^k}{k + 1} + \dotsc \right)
  \\
  &amp;=
  \frac{1}{100} \frac{1}{x} \left(  x + \frac{x^2}{2} + \frac{x^3}{3} + \dotsc + \frac{x^k}{k} + \dotsc   \right)
  \\
  &amp;=
  -\frac{1}{100} \frac{1}{x} \log (1 - x)
  \\
  &amp;=
  \frac{\log 100}{99} 
  .
\end{align}
\]</span></p>
<p>The normalising constant <span class="math inline">\(c\)</span> is then</p>
<p class="mathjaxWide"><span class="math display">\[
  c
  =
  \frac{\log 100}{99} 
  -
  \sum_1^{202} \frac{1}{N} \cdot \frac{1}{100}\cdot \left( \frac{99}{100} \right)^{N - 1}
\]</span></p>
<p>which we approximate with the following computation.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># value of the Nth term in the sum</span>
term &lt;-<span class="st"> </span><span class="cf">function</span>(N) 
  (<span class="dv">1</span> <span class="op">/</span><span class="st"> </span>N) <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">/</span><span class="st"> </span><span class="dv">100</span>) <span class="op">*</span><span class="st"> </span>(<span class="dv">99</span> <span class="op">/</span><span class="st"> </span><span class="dv">100</span>)<span class="op">^</span>(N <span class="op">-</span><span class="st"> </span><span class="dv">1</span>) 

<span class="co"># left hand side</span>
c0 &lt;-<span class="st"> </span><span class="kw">log</span>(<span class="dv">100</span>) <span class="op">/</span><span class="st"> </span><span class="dv">99</span>

<span class="co"># right hand side (the sum)</span>
c1 &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="dv">202</span> <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">map</span>(term) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">reduce</span>(sum)
  
c &lt;-<span class="st"> </span>c0 <span class="op">-</span><span class="st"> </span>c1

c</code></pre></div>
<pre><code>[1] 0.0004705084</code></pre>
<h2 id="the-posterior-moments">The posterior moments</h2>
<p>The posterior mean is</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
  \mathbb E(N \mid y = 203)
  &amp;=
  \frac{1}{c}\sum_{203}^\infty \frac{N}{N} \cdot \frac{1}{100} \cdot \left( \frac{99}{100} \right)^{N - 1}
  \\
  &amp;=
  \frac{1}{100c} \left( x^{202} + x^{203} + \dotsc \right), \qquad x := \frac{99}{100}
  \\
  &amp;=
  \frac{1}{100c} \left( \frac{1}{1 - x} - (1 + x + x^2 + \dotsc + x^{201}) \right)
  \\
  &amp;=
  \frac{1}{100c} \left( \frac{1}{1 - x} - \frac{1 - x^{202}}{1 - x} \right)
  \\
  &amp;=
  \frac{1}{100c} \frac{x^{202}}{1 - x}
  \\
  &amp;=
  \frac{1}{c}\left( \frac{99}{100} \right)^{202}
\end{align}
,
\]</span></p>
<p>which is approximately</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mu &lt;-<span class="st"> </span>(<span class="dv">1</span> <span class="op">/</span><span class="st"> </span>c) <span class="op">*</span><span class="st"> </span>(<span class="dv">99</span> <span class="op">/</span><span class="st"> </span><span class="dv">100</span>)<span class="op">^</span><span class="dv">202</span>
mu</code></pre></div>
<pre><code>[1] 279.0885</code></pre>
<p>This is larger than the prior mean of 100.</p>
<p>The second moment is</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
  \mathbb E(N^2 \mid y = 203)
  &amp;=
  \frac{1}{c}\sum_{203}^\infty \frac{N^2}{N} \cdot \frac{1}{100} \cdot \left( \frac{99}{100} \right)^{N - 1}
  \\
  &amp;=
  \frac{1}{100c} \left( 203x^{202} + 204x^{203} + \dotsc \right), \qquad x := \frac{99}{100}
  \\
  &amp;=
  \frac{1}{100c} \left( \frac{1}{(1 - x)^2} - \left(1 + 2x + 3x^2 + \dotsc + 202 x^{201} \right) \right)
  \\
  &amp;=
  \frac{1}{100c} \left( 100^2 - \left(1 + 2x + 3x^2 + \dotsc + 202 x^{201} \right) \right)
  \\
  &amp;=
  \frac{100}{c}  - \frac{1 + 2x + 3x^2 + \dotsc + 202 x^{201} }{100c}
\end{align}
,
\]</span></p>
<p>which we approximate with the following code.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">EN2_left &lt;-<span class="st"> </span><span class="dv">100</span> <span class="op">/</span><span class="st"> </span>c

EN2_right &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="dv">201</span> <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">map</span>(<span class="cf">function</span>(N) N <span class="op">*</span><span class="st"> </span>(<span class="dv">99</span> <span class="op">/</span><span class="st"> </span><span class="dv">100</span>)<span class="op">^</span>(N <span class="op">-</span><span class="st"> </span><span class="dv">1</span>)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">reduce</span>(sum) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  `</span><span class="dt">/</span><span class="st">`</span>(<span class="dv">100</span> <span class="op">*</span><span class="st"> </span>c)

EN2 &lt;-<span class="st"> </span>EN2_left <span class="op">-</span><span class="st"> </span>EN2_right

EN2</code></pre></div>
<pre><code>[1] 84854.18</code></pre>
<p>It follows that the posterior variance and standard deviation is approximately</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">v &lt;-<span class="st"> </span>EN2 <span class="op">-</span><span class="st"> </span>mu<span class="op">^</span><span class="dv">2</span>
sigma &lt;-<span class="st"> </span><span class="kw">sqrt</span>(v)
<span class="kw">c</span>(v, sigma)</code></pre></div>
<pre><code>[1] 6963.78665   83.44931</code></pre>
<p>These are smaller than the prior variance and standard deviation, respectively:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">v_prior &lt;-<span class="st"> </span>(<span class="dv">99</span> <span class="op">/</span><span class="st"> </span><span class="dv">100</span>) <span class="op">/</span><span class="st"> </span>(<span class="fl">0.01</span><span class="op">^</span><span class="dv">2</span>)
sigma_prior &lt;-<span class="st"> </span><span class="kw">sqrt</span>(v_prior)
<span class="kw">c</span>(v_prior, sigma_prior)</code></pre></div>
<pre><code>[1] 9900.00000   99.49874</code></pre>
<h2 id="a-non-informative-prior">A non-informative prior</h2>
<p>There is no proper uniform density over the positive integers. A uniform prior also leaves us with an improper posterior.</p>
<p>Jeffrey’s prior is <span class="math inline">\(p(N) \propto \frac{1}{N}\)</span>, which is also improper. However, it yields the following (unnormalised) posterior</p>
<p class="mathjaxWide"><span class="math display">\[
p(N \mid y = 203)
\propto
\left.
  \begin{cases}
    \frac{1}{N^2} 
    &amp; 
    \text{for } 203 \le N 
    \\
    0
    &amp;
    \text{otherwise}
  \end{cases}
\right\}
\]</span></p>
<p>which is proper.</p>
<p>Using the <a href="https://en.wikipedia.org/wiki/Basel_problem">Basel problem</a> we can calculate the normalising constant</p>
<p class="mathjaxWide"><span class="math display">\[
c
=
\sum_1^\infty \frac{1}{N^2}
-
\sum_1^{202} \frac{1}{N^2}
=
\frac{\pi^2}{6}
-
\sum_1^{202} \frac{1}{N^2}
\]</span></p>
<p>which is approximately</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">c_left &lt;-<span class="st"> </span>pi<span class="op">^</span><span class="dv">2</span> <span class="op">/</span><span class="st"> </span><span class="dv">6</span>

c_right &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="dv">202</span> <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">map</span>(<span class="cf">function</span>(N) <span class="dv">1</span> <span class="op">/</span><span class="st"> </span>N<span class="op">^</span><span class="dv">2</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">reduce</span>(sum)

c &lt;-<span class="st"> </span>c_left <span class="op">-</span><span class="st"> </span>c_right

c</code></pre></div>
<pre><code>[1] 0.004938262</code></pre>
<p>The posterior mean is not well-defined since</p>
<p class="mathjaxWide"><span class="math display">\[
\mathbb E(N \mid y = 203)
=
\frac{1}{c}\sum_{203}^\infty \frac{1}{N} 
=
\infty
.
\]</span></p>
<p>The posterior variance and standard deviation are also not well-defined.</p>

<div id="disqus_thread"></div>
<script>
/**
 *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
 *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
 */
/*
   var disqus_config = function () {
   this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
   this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
   };
 */
(function() {  // DON'T EDIT BELOW THIS LINE
 var d = document, s = d.createElement('script');

 s.src = '//stappit-github-io.disqus.com/embed.js';

 s.setAttribute('data-timestamp', +new Date());
 (d.head || d.body).appendChild(s);
 })();
</script>

<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

<script id="dsq-count-scr" src="//stappit-github-io.disqus.com/count.js" async></script>
]]></summary>
</entry>
<entry>
    <title>BDA3 Chapter 2 Exercise 9</title>
    <link href="http://stappit.github.io/posts/bda3/chapter_02_exercise_09.html" />
    <id>http://stappit.github.io/posts/bda3/chapter_02_exercise_09.html</id>
    <published>2018-08-29T00:00:00Z</published>
    <updated>2018-08-29T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 class="post-title">BDA3 Chapter 2 Exercise 9</h1>

<div class="card post-meta border-0">
  <div class="card-body post-meta">
    <p class="card-text text-muted text-left">
    Posted on August 29, 2018  by Brian </br>
     Tags: <a href="/tags/bda%20chapter%202.html">bda chapter 2</a>, <a href="/tags/solutions.html">solutions</a>, <a href="/tags/bayes.html">bayes</a>, <a href="/tags/beta.html">beta</a>, <a href="/tags/prior%20sensitivity.html">prior sensitivity</a> </br>
     Category: <a href="/categories/bda3.html">bda3</a> 
    </p>
  </div>
</div>

<p>Here’s my solution to exercise 9, chapter 2, of <a href="https://andrewgelman.com/">Gelman’s</a> <em>Bayesian Data Analysis</em> (BDA), 3rd edition. There are <a href="http://www.stat.columbia.edu/~gelman/book/solutions.pdf">solutions</a> to some of the exercises on the <a href="http://www.stat.columbia.edu/~gelman/book/">book’s webpage</a>.</p>
<!--more-->
<div style="display:none">
<p class="mathjaxWide"><span class="math inline">\(\DeclareMathOperator{\dbinomial}{binomial} \DeclareMathOperator{\dbern}{Bernoulli} \DeclareMathOperator{\dnorm}{normal} \DeclareMathOperator{\dgamma}{gamma} \DeclareMathOperator{\invlogit}{invlogit} \DeclareMathOperator{\logit}{logit} \DeclareMathOperator{\dbeta}{beta}\)</span></p>
</div>
<p>The data show 650 people in support of the death penalty and 350 against. We explore the effect of different priors on the posterior.</p>
<p>First let’s find the prior with a mean of 0.6 and standard deviation 0.3. The mean of the <span class="math inline">\(\dbeta(\alpha, \beta)\)</span> distribution is</p>
<p class="mathjaxWide"><span class="math display">\[
\frac{3}{5}
=
\frac{\alpha}{\alpha + \beta}
\]</span></p>
<p>which implies that <span class="math inline">\(\alpha = 1.5 \beta\)</span>. The variance is</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
  \frac{9}{100}
  &amp;=
  \frac{\alpha \beta}{(\alpha + \beta)^2 (\alpha + \beta  + 1)}
  \\
  &amp;=
  \frac{3}{2} \frac{\beta^2}{\frac{25}{4}\beta^2 \frac{5\beta + 2}{2}}
  \\
  &amp;=
  \frac{3}{2}\frac{4}{25}\frac{2}{5\beta + 2}
  \\
  &amp;=
  \frac{12}{25(5\beta + 2)}
  \\
  &amp;\Leftrightarrow
  \\
  5\beta + 2
  &amp;=
  \frac{12}{25}\frac{100}{9}
  \\
  &amp;=
  4,
\end{align}
\]</span></p>
<p>which implies that <span class="math inline">\(\beta = \frac{2}{5}\)</span>. Thus <span class="math inline">\(\alpha = \frac{3}{5}\)</span>. Since both parameters are below 1, we see maxima near 0 and 1.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">α &lt;-<span class="st"> </span><span class="dv">3</span> <span class="op">/</span><span class="st"> </span><span class="dv">5</span>
β &lt;-<span class="st"> </span><span class="dv">2</span> <span class="op">/</span><span class="st"> </span><span class="dv">5</span>

<span class="kw">tibble</span>(<span class="dt">x =</span> <span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="fl">0.001</span>), <span class="dt">y =</span> <span class="kw">dbeta</span>(x, α, β)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">ggplot</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">aes</span>(x, y) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_area</span>(<span class="dt">fill =</span> <span class="st">&#39;skyblue&#39;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(
    <span class="dt">x =</span> <span class="st">&#39;x&#39;</span>,
    <span class="dt">y =</span> <span class="st">&#39;beta(x | α, β)&#39;</span>,
    <span class="dt">title =</span> <span class="st">&#39;Beta prior with mean 0.3 and standard deviation 0.6&#39;</span>,
    <span class="dt">subtitle =</span> <span class="kw">str_glue</span>(<span class="st">&#39;α = {α}, β = {β}&#39;</span>)
  )</code></pre></div>
<figure>
<img src="chapter_02_exercise_09_files/figure-markdown/prior-1.png" />
</figure>
<p>The beta distribution is self-conjugate so the posterior is <span class="math inline">\(\dbeta(0.6 + 650, 0.4 + 350)\)</span>.</p>
<p>Let’s plot the posterior with priors of different strength. We can increase the strength of the prior whilst keeping the mean constant by multiplying <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> by the same constant c. We will use <span class="math inline">\(c \in \{ 1, 10, 100, 1000\}\)</span>. In the plot below, we have restricted the x-axis to focus on the differences in the shape of the posteriors.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">support &lt;-<span class="st"> </span><span class="dv">650</span>
against &lt;-<span class="st"> </span><span class="dv">350</span>

<span class="kw">expand.grid</span>(<span class="dt">magnitude =</span> <span class="dv">0</span><span class="op">:</span><span class="dv">3</span>, <span class="dt">x =</span> <span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="fl">0.001</span>)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">as_tibble</span>() <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(
    <span class="dt">c =</span> <span class="dv">10</span><span class="op">^</span>magnitude,
    <span class="dt">a_prior =</span> α <span class="op">*</span><span class="st"> </span>c,
    <span class="dt">b_prior =</span> β <span class="op">*</span><span class="st"> </span>c,
    <span class="dt">y =</span> <span class="kw">dbeta</span>(x, support <span class="op">+</span><span class="st"> </span>a_prior, against <span class="op">+</span><span class="st"> </span>b_prior),
    <span class="dt">prior_magnitude =</span> <span class="kw">factor</span>(<span class="kw">as.character</span>(<span class="dv">10</span><span class="op">^</span>magnitude))
  ) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">aes</span>(x, y, <span class="dt">colour =</span> prior_magnitude) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_x_continuous</span>(<span class="dt">limits =</span> <span class="kw">c</span>(<span class="fl">0.55</span>, <span class="fl">0.75</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(
    <span class="dt">x =</span> <span class="st">&#39;x&#39;</span>,
    <span class="dt">y =</span> <span class="st">&#39;beta(x | support + a, against + b)&#39;</span>,
    <span class="dt">title =</span> <span class="st">&#39;Beta posterior with different priors&#39;</span>,
    <span class="dt">subtitle =</span> <span class="kw">str_glue</span>(<span class="kw">paste</span>(
      <span class="st">&#39;a = {α} * 10^magnitude, b = {β} * 10^magnitude&#39;</span>,
      <span class="st">&#39;support = 650, against = 350&#39;</span>,
      <span class="dt">sep =</span> <span class="st">&#39;</span><span class="ch">\n</span><span class="st">&#39;</span>
    )),
    <span class="dt">colour =</span> <span class="st">&#39;Magnitude of the prior&#39;</span>
  )</code></pre></div>
<figure>
<img src="chapter_02_exercise_09_files/figure-markdown/posterior-1.png" />
</figure>
<p>Magnitudes 1 and 10 give very similar results close to the maximum likelihood estimate of 65%. The higher magnitudes pull the mean towards the prior mean of 60%.</p>

<div id="disqus_thread"></div>
<script>
/**
 *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
 *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
 */
/*
   var disqus_config = function () {
   this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
   this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
   };
 */
(function() {  // DON'T EDIT BELOW THIS LINE
 var d = document, s = d.createElement('script');

 s.src = '//stappit-github-io.disqus.com/embed.js';

 s.setAttribute('data-timestamp', +new Date());
 (d.head || d.body).appendChild(s);
 })();
</script>

<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

<script id="dsq-count-scr" src="//stappit-github-io.disqus.com/count.js" async></script>
]]></summary>
</entry>
<entry>
    <title>BDA3 Chapter 2 Exercise 8</title>
    <link href="http://stappit.github.io/posts/bda3/chapter_02_exercise_08.html" />
    <id>http://stappit.github.io/posts/bda3/chapter_02_exercise_08.html</id>
    <published>2018-08-27T00:00:00Z</published>
    <updated>2018-08-27T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 class="post-title">BDA3 Chapter 2 Exercise 8</h1>

<div class="card post-meta border-0">
  <div class="card-body post-meta">
    <p class="card-text text-muted text-left">
    Posted on August 27, 2018  by Brian </br>
     Tags: <a href="/tags/bda%20chapter%202.html">bda chapter 2</a>, <a href="/tags/bda.html">bda</a>, <a href="/tags/solutions.html">solutions</a>, <a href="/tags/bayes.html">bayes</a>, <a href="/tags/normal.html">normal</a>, <a href="/tags/posterior%20predictive.html">posterior predictive</a> </br>
     Category: <a href="/categories/bda3.html">bda3</a> 
    </p>
  </div>
</div>

<p>Here’s my solution to exercise 8, chapter 2, of <a href="https://andrewgelman.com/">Gelman’s</a> <em>Bayesian Data Analysis</em> (BDA), 3rd edition. There are <a href="http://www.stat.columbia.edu/~gelman/book/solutions.pdf">solutions</a> to some of the exercises on the <a href="http://www.stat.columbia.edu/~gelman/book/">book’s webpage</a>.</p>
<!--more-->
<div style="display:none">
<p class="mathjaxWide"><span class="math inline">\(\DeclareMathOperator{\dbinomial}{binomial} \DeclareMathOperator{\dbern}{Bernoulli} \DeclareMathOperator{\dnorm}{normal} \DeclareMathOperator{\dgamma}{gamma} \DeclareMathOperator{\invlogit}{invlogit} \DeclareMathOperator{\logit}{logit} \DeclareMathOperator{\dbeta}{beta}\)</span></p>
</div>
<p>With prior <span class="math inline">\(\theta \sim \dnorm(180, 40)\)</span>, sampling distribution <span class="math inline">\(y \mid \theta \sim \dnorm(\theta, 20)\)</span>, and <span class="math inline">\(n\)</span> sampled students with average weight <span class="math inline">\(\bar y = 150\)</span>, it follows from 2.11 that the posterior mean is</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
  \mu
  :=
  \mathbb E(\theta \mid \bar y) 
  &amp;=
  \frac{\frac{180}{1600} + \frac{150n}{400}}{\frac{1}{1600} + \frac{n}{400}} 
  \\
  &amp;=
  \frac{60(3 + 10n)}{1600} \cdot \frac{1600}{1 + 4n}
  \\
  &amp;=
  \frac{60(3 + 10n)}{1 + 4n}
  \\
  1 / \sigma^2 
  :=
  1 / \mathbb V (\theta \mid \bar y)
  &amp;=
  \frac{1}{1600} + \frac{n}{400}
  \\
  &amp;=
  \frac{1 + 4n}{1600}
  .
\end{align}
\]</span></p>
<p>So <span class="math inline">\(\theta \mid \bar y ~ \dnorm \left( \frac{60(3 + 10n)}{1 + 4n}, \frac{40}{\sqrt{1 + 4n}} \right)\)</span>.</p>
<p>It follows from the calculations shown in the book that the posterior predictive distribution is <span class="math inline">\(\tilde y \mid y \sim \dnorm(\mu, \sigma + 20)\)</span>.</p>
<p>We can obtain 95% posterior intervals as follows.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mu &lt;-<span class="st"> </span><span class="cf">function</span>(n) <span class="dv">60</span> <span class="op">*</span><span class="st"> </span>(<span class="dv">3</span> <span class="op">+</span><span class="st"> </span><span class="dv">10</span> <span class="op">*</span><span class="st"> </span>n) <span class="op">/</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="dv">4</span> <span class="op">*</span><span class="st"> </span>n)
sigma &lt;-<span class="st"> </span><span class="cf">function</span>(n) <span class="dv">40</span> <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="dv">4</span> <span class="op">*</span><span class="st"> </span>n)

percentiles &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.05</span>, <span class="fl">0.95</span>)

theta_posterior_interval &lt;-<span class="st"> </span><span class="kw">qnorm</span>(percentiles, <span class="kw">mu</span>(<span class="dv">10</span>), <span class="kw">sigma</span>(<span class="dv">10</span>))
y_posterior_interval &lt;-<span class="st"> </span><span class="kw">qnorm</span>(percentiles, <span class="kw">mu</span>(<span class="dv">10</span>), <span class="kw">sigma</span>(<span class="dv">10</span>) <span class="op">+</span><span class="st"> </span><span class="dv">20</span>)</code></pre></div>
<p>With a sample of size of 10, we get θ ϵ [140.5, 161] and <span class="math inline">\(\tilde y\)</span> ϵ [107.6, 193.9].</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">theta_posterior_interval &lt;-<span class="st"> </span><span class="kw">qnorm</span>(percentiles, <span class="kw">mu</span>(<span class="dv">100</span>), <span class="kw">sigma</span>(<span class="dv">100</span>))
y_posterior_interval &lt;-<span class="st"> </span><span class="kw">qnorm</span>(percentiles, <span class="kw">mu</span>(<span class="dv">100</span>), <span class="kw">sigma</span>(<span class="dv">100</span>) <span class="op">+</span><span class="st"> </span><span class="dv">20</span>)</code></pre></div>
<p>With a sample of size of 100, we get θ ϵ [146.8, 153.4] and <span class="math inline">\(\tilde y\)</span> ϵ [113.9, 186.3].</p>

<div id="disqus_thread"></div>
<script>
/**
 *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
 *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
 */
/*
   var disqus_config = function () {
   this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
   this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
   };
 */
(function() {  // DON'T EDIT BELOW THIS LINE
 var d = document, s = d.createElement('script');

 s.src = '//stappit-github-io.disqus.com/embed.js';

 s.setAttribute('data-timestamp', +new Date());
 (d.head || d.body).appendChild(s);
 })();
</script>

<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

<script id="dsq-count-scr" src="//stappit-github-io.disqus.com/count.js" async></script>
]]></summary>
</entry>
<entry>
    <title>BDA3 Chapter 2 Exercise 7</title>
    <link href="http://stappit.github.io/posts/bda3/chapter_02_exercise_07.html" />
    <id>http://stappit.github.io/posts/bda3/chapter_02_exercise_07.html</id>
    <published>2018-08-26T00:00:00Z</published>
    <updated>2018-08-26T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 class="post-title">BDA3 Chapter 2 Exercise 7</h1>

<div class="card post-meta border-0">
  <div class="card-body post-meta">
    <p class="card-text text-muted text-left">
    Posted on August 26, 2018  by Brian </br>
     Tags: <a href="/tags/bda%20chapter%202.html">bda chapter 2</a>, <a href="/tags/bda.html">bda</a>, <a href="/tags/solutions.html">solutions</a>, <a href="/tags/bayes.html">bayes</a>, <a href="/tags/binomial.html">binomial</a>, <a href="/tags/natural%20parameter.html">natural parameter</a>, <a href="/tags/exponential%20family.html">exponential family</a>, <a href="/tags/improper%20prior.html">improper prior</a> </br>
     Category: <a href="/categories/bda3.html">bda3</a> 
    </p>
  </div>
</div>

<p>Here’s my solution to exercise 7, chapter 2, of <a href="https://andrewgelman.com/">Gelman’s</a> <em>Bayesian Data Analysis</em> (BDA), 3rd edition. There are <a href="http://www.stat.columbia.edu/~gelman/book/solutions.pdf">solutions</a> to some of the exercises on the <a href="http://www.stat.columbia.edu/~gelman/book/">book’s webpage</a>.</p>
<!--more-->
<div style="display:none">
<p class="mathjaxWide"><span class="math inline">\(\DeclareMathOperator{\dbinomial}{binomial} \DeclareMathOperator{\dbern}{Bernoulli} \DeclareMathOperator{\dgamma}{gamma} \DeclareMathOperator{\invlogit}{invlogit} \DeclareMathOperator{\logit}{logit} \DeclareMathOperator{\dbeta}{beta}\)</span></p>
</div>
<p>We show that a uniform prior on the natural parameter of a binomial model implies an improper prior under a different parameterisation.</p>
<p>The binomial likelihood can be written as a member of the exponential family as</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
  \dbinomial(y \mid \theta)
  &amp;=
  \binom{n}{y} \theta^y (1 - \theta)^{n - y}
  \\
  &amp;=
  \binom{n}{y} \cdot (1 - \theta)^n \cdot \exp \left(y \log \left(\frac{\theta}{1 - \theta}\right)\right)
  \\
  &amp;=
  f(y) \cdot g(\theta) \cdot \exp (\phi(\theta) \cdot u(y))
  ,
\end{align}
\]</span></p>
<p>where <span class="math inline">\(\phi(\theta) := \log \frac{\theta}{1 - \theta}\)</span>, <span class="math inline">\(u(y) := y\)</span>, <span class="math inline">\(g(\theta) := (1 - \theta)^n\)</span>. Suppose the natural parameter <span class="math inline">\(\phi \sim \dbeta(1, 1)\)</span> is uniformly distributed. Then the distribution of <span class="math inline">\(\theta\)</span> is</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
  p(\theta) 
  &amp;\propto
  p(\phi) \cdot \vert \invlogit^\prime (\phi) \vert^{-1}
  \\
  &amp;=
  \left \vert \frac{1}{1 + \exp(-\phi)}^\prime \right\vert^{-1}
  \\
  &amp;=
  \left \vert \frac{1}{\left(1 + \exp(-\phi)\right)^2} \cdot \exp(-\phi) \right\vert^{-1}
  \\
  &amp;=
  \frac{1 + 2 \exp(-\phi) + \exp(-2\phi)}{\exp(-\phi)}
  \\
  &amp;=
  \exp(\phi) + 2 + \exp(-\phi)
  \\
  &amp;=
  \frac{\theta}{1 - \theta} + 2 + \frac{1 - \theta}{\theta}
  \\
  &amp;=
  \frac{\theta^2 + 2\theta(1 - \theta) + (1 - \theta)^2}{\theta(1 - \theta)}
  \\
  &amp;=
  \frac{1}{\theta(1 - \theta)}
  \\
  &amp;=
  \theta^{-1}(1 - \theta)^{-1}
  \qquad \square
\end{align}
\]</span></p>
<p>This is an improper distribution on <span class="math inline">\(\theta\)</span> because</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
  \int_0^1 \frac{1}{\theta(1 - \theta)}
  &amp;\ge
  \int_0^1 \frac{1}{\theta}
  \\
  &amp;=
  \log\theta \vert_0^1
  \\
  &amp;=
  \infty.
\end{align}
\]</span></p>
<p>When <span class="math inline">\(y = 0\)</span>, then the posterior distribution is <span class="math inline">\(p(\theta \mid y = 0) \propto (1 - \theta)^{n - 1}\theta^{-1}\)</span>. When <span class="math inline">\(y = n\)</span>, then the posterior distribution is <span class="math inline">\(p(\theta \mid y = n) \propto \theta^{n-1}(1 - \theta)^{-1}\)</span>. These two cases are equivalent by the change of variable <span class="math inline">\(\theta \mapsto 1 - \theta\)</span>.</p>
<p>We show that the distribution is improper for <span class="math inline">\(y = 0\)</span> by induction. The case <span class="math inline">\(n = 0\)</span> is shown above (for the prior). Assume the distribution is improper for any integer <span class="math inline">\(k &lt; n\)</span>. Then using integration by parts yields</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
\int_0^1 \theta^{- 1}(1 - \theta)^{n - 1} d\theta
&amp;=
\int_0^1 \theta^{- 1}(1 - \theta)^{n - 2} \cdot (1 - \theta)d\theta
\\
&amp;=
\left[(1 - \theta)^{n-2}(1 - \frac{\theta}{2}) \right]_0^1
+
\int_0^1 \frac{(1 - \theta)^{n - 2}}{\theta}
+
(n-2)(1 - \theta)^{n-3}
-
\frac{(1 - \theta)^{n-2}}{2}
-
(n - 2)\theta\frac{(1 - \theta)^{n-3}}{2}
d\theta
\\
&amp;=
c
+
\int_0^1 \frac{(1 - \theta)^{n - 2}}{\theta} d\theta,
\end{align}
\]</span></p>
<p>where <span class="math inline">\(c &lt; \infty\)</span>. By the induction hypothesis, the integral on the last line is <span class="math inline">\(\infty\)</span>. Therefore, the distribution is also improper for <span class="math inline">\(n\)</span>.</p>

<div id="disqus_thread"></div>
<script>
/**
 *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
 *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
 */
/*
   var disqus_config = function () {
   this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
   this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
   };
 */
(function() {  // DON'T EDIT BELOW THIS LINE
 var d = document, s = d.createElement('script');

 s.src = '//stappit-github-io.disqus.com/embed.js';

 s.setAttribute('data-timestamp', +new Date());
 (d.head || d.body).appendChild(s);
 })();
</script>

<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

<script id="dsq-count-scr" src="//stappit-github-io.disqus.com/count.js" async></script>
]]></summary>
</entry>
<entry>
    <title>BDA3 Chapter 2 Exercise 6</title>
    <link href="http://stappit.github.io/posts/bda3/chapter_02_exercise_06.html" />
    <id>http://stappit.github.io/posts/bda3/chapter_02_exercise_06.html</id>
    <published>2018-08-25T00:00:00Z</published>
    <updated>2018-08-25T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 class="post-title">BDA3 Chapter 2 Exercise 6</h1>

<div class="card post-meta border-0">
  <div class="card-body post-meta">
    <p class="card-text text-muted text-left">
    Posted on August 25, 2018  by Brian </br>
     Tags: <a href="/tags/bda%20chapter%202.html">bda chapter 2</a>, <a href="/tags/bda.html">bda</a>, <a href="/tags/solutions.html">solutions</a>, <a href="/tags/bayes.html">bayes</a>, <a href="/tags/poisson.html">poisson</a>, <a href="/tags/gamma.html">gamma</a>, <a href="/tags/negative%20binomial.html">negative binomial</a> </br>
     Category: <a href="/categories/bda3.html">bda3</a> 
    </p>
  </div>
</div>

<p>Here’s my solution to exercise 6, chapter 2, of <a href="https://andrewgelman.com/">Gelman’s</a> <em>Bayesian Data Analysis</em> (BDA), 3rd edition. There are <a href="http://www.stat.columbia.edu/~gelman/book/solutions.pdf">solutions</a> to some of the exercises on the <a href="http://www.stat.columbia.edu/~gelman/book/">book’s webpage</a>.</p>
<!--more-->
<div style="display:none">
<p class="mathjaxWide"><span class="math inline">\(\DeclareMathOperator{\dbinomial}{binomial} \DeclareMathOperator{\dbern}{Bernoulli} \DeclareMathOperator{\dgamma}{gamma} \DeclareMathOperator{\dpois}{Poisson} \DeclareMathOperator{\dbeta}{beta}\)</span></p>
</div>
<p>Considering the negative binomial variable <span class="math inline">\(y\)</span> as a gamma-Poisson variable, we derive expressions for the mean and variance.</p>
<p>From equation 1.6, <span class="math inline">\(\mathbb E (y) = \mathbb E (\mathbb E(y \mid \theta))\)</span>. Since <span class="math inline">\(y \mid \theta \sim \dpois(10n\theta)\)</span>, it follows that <span class="math inline">\(\mathbb E (y \mid \theta) = 10n\theta\)</span>. The rate <span class="math inline">\(\theta \sim \dgamma(\alpha, \beta)\)</span> so <span class="math inline">\(\mathbb E(\theta) = \frac{\alpha}{\beta}\)</span>. Thus, <span class="math inline">\(\mathbb E(y) = 10n\mathbb E(\theta) = 10n \frac{\alpha}{\beta}\)</span>.</p>
<p>We also have <span class="math inline">\(\mathbb V(\theta) = \frac{\alpha}{\beta^2}\)</span> since <span class="math inline">\(\theta \sim \dgamma(\alpha, \beta)\)</span>, and <span class="math inline">\(\mathbb V(y \mid \theta) = 10n\theta\)</span> since <span class="math inline">\(y \mid \theta \sim \dpois(10n\theta)\)</span>. Thus,</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
  \mathbb V (y) 
  &amp;= 
  \mathbb E(\mathbb V(y \mid \theta)) + \mathbb V (\mathbb E (y \mid \theta)) 
  \\
  &amp;=
  \mathbb E(10n\theta) + \mathbb V (10n\theta)
  \\
  &amp;=
  10n\frac{\alpha}{\beta} + (10n)^2\frac{\alpha}{\beta^2}
  \qquad \square
\end{align}
\]</span></p>

<div id="disqus_thread"></div>
<script>
/**
 *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
 *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
 */
/*
   var disqus_config = function () {
   this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
   this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
   };
 */
(function() {  // DON'T EDIT BELOW THIS LINE
 var d = document, s = d.createElement('script');

 s.src = '//stappit-github-io.disqus.com/embed.js';

 s.setAttribute('data-timestamp', +new Date());
 (d.head || d.body).appendChild(s);
 })();
</script>

<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

<script id="dsq-count-scr" src="//stappit-github-io.disqus.com/count.js" async></script>
]]></summary>
</entry>
<entry>
    <title>BDA3 Chapter 2 Exercise 5</title>
    <link href="http://stappit.github.io/posts/bda3/chapter_02_exercise_05.html" />
    <id>http://stappit.github.io/posts/bda3/chapter_02_exercise_05.html</id>
    <published>2018-08-24T00:00:00Z</published>
    <updated>2018-08-24T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 class="post-title">BDA3 Chapter 2 Exercise 5</h1>

<div class="card post-meta border-0">
  <div class="card-body post-meta">
    <p class="card-text text-muted text-left">
    Posted on August 24, 2018  by Brian </br>
     Tags: <a href="/tags/bayes.html">bayes</a>, <a href="/tags/solutions.html">solutions</a>, <a href="/tags/bda%20chapter%202.html">bda chapter 2</a>, <a href="/tags/bda.html">bda</a>, <a href="/tags/beta.html">beta</a>, <a href="/tags/binomial.html">binomial</a>, <a href="/tags/beta-binomial.html">beta-binomial</a>, <a href="/tags/variance.html">variance</a> </br>
     Category: <a href="/categories/bda3.html">bda3</a> 
    </p>
  </div>
</div>

<p>Here’s my solution to exercise 5, chapter 2, of <a href="https://andrewgelman.com/">Gelman’s</a> <em>Bayesian Data Analysis</em> (BDA), 3rd edition. There are <a href="http://www.stat.columbia.edu/~gelman/book/solutions.pdf">solutions</a> to some of the exercises on the <a href="http://www.stat.columbia.edu/~gelman/book/">book’s webpage</a>.</p>
<!--more-->
<div style="display:none">
<p class="mathjaxWide"><span class="math inline">\(\DeclareMathOperator{\dbinomial}{binomial} \DeclareMathOperator{\dbern}{Bernoulli} \DeclareMathOperator{\dbeta}{beta}\)</span></p>
</div>
<p>Let’s derive the prior predictive distribution of a beta-binomial model with a uniform prior. See <a href="https://math.stackexchange.com/questions/122296/how-to-evaluate-this-integral-relating-to-binomial">stackexchange</a> and <a href="https://en.wikipedia.org/wiki/Beta_function">wikipedia</a> for useful results for solving the integral below.</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
  p(y = k)
  &amp;=
  \int_0^1 p(y = k \mid \theta) p(\theta) d\theta
  \\
  &amp;=
  \binom{n}{k} \cdot \int_0^1 \theta^k (1 - \theta)^{n - k} d\theta
  \\
  &amp;=
  \binom{n}{k} \cdot \frac{1}{\binom{n}{k} \cdot (n + 1)}
  \\
  &amp;=
  \frac{1}{n + 1}
\end{align}
\]</span></p>
<p>Now let’s show that the posterior mean of <span class="math inline">\(\theta\)</span> lies between the prior mean and observed frequency. The posterior is</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
  p(\theta \mid y)
  &amp;\propto
  p(y \mid \theta) \cdot p(\theta)
  \\
  &amp;\propto
  \theta^y (1 - \theta)^{n - y}\cdot \theta^{\alpha - 1} (1 - \theta)^{\beta - 1}
  \\
  &amp;=
  \theta^{y + \alpha - 1} (1 - \theta)^{n + \beta - y - 1}.
\end{align}
\]</span></p>
<p>So <span class="math inline">\(p(\theta \mid y) \sim \dbeta(y + \alpha, n - y + \beta)\)</span>, which has mean <span class="math inline">\(\frac{y + \alpha}{n + \alpha + \beta}\)</span>. Suppose <span class="math inline">\(\frac{y}{n} \le \frac{\alpha}{\alpha + \beta}\)</span>. Then</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
  \frac{y}{n}
  &amp;\le
  \frac{y + \alpha}{n + \alpha + \beta}
  \\
  \Leftrightarrow
  y(n + \alpha + \beta)
  &amp;\le
  n(y + \alpha)
  \\
  \Leftrightarrow
  y(\alpha + \beta)
  &amp;\le
  n\alpha
  \\
  \Leftrightarrow
  \frac{y}{n} 
  &amp;\le 
  \frac{\alpha}{\alpha + \beta}
\end{align}
\]</span></p>
<p>A similar argument shows that <span class="math inline">\(\frac{y + \alpha}{n + \alpha + \beta} \le \frac{\alpha}{\alpha + \beta}\)</span>.</p>
<p>If <span class="math inline">\(\frac{y}{n} \ge \frac{\alpha}{\alpha + \beta}\)</span>, then the analogous argument shows that <span class="math inline">\(\frac{\alpha}{\alpha + \beta} \le \frac{y + \alpha}{n + \alpha + \beta} \le \frac{y}{n}. \square\)</span></p>
<p>The prior variance is <span class="math inline">\(\mathbb V (\theta) = \frac{\alpha \beta}{(\alpha + \beta)^2(\alpha + \beta + 1)}\)</span>. For a uniform prior this is <span class="math inline">\(\frac{1}{4 \cdot 3} = \frac{1}{12}\)</span>. The posterior variance with a uniform prior is <span class="math inline">\(\frac{y + 1}{n + 2} \cdot \frac{n - y + 1}{n + 2} \cdot \frac{1}{n + 3}\)</span>. For <span class="math inline">\(p \in [0, 1]\)</span>, the function <span class="math inline">\(p \mapsto p(1 - p)\)</span> is maximised when <span class="math inline">\(p = 0.5\)</span>. Thus for fixed <span class="math inline">\(n\)</span>, the posterior variance is maximised when <span class="math inline">\(y = \frac{n}{2}\)</span>. This means that the posterior variance is at most <span class="math inline">\(\frac{1}{4} \cdot \frac{1}{n + 3} \le \frac{1}{4n + 12} \le \frac{1}{12}. \square\)</span></p>
<p>Intuitively, the posterior variance should be larger than the prior variance when the observed data is different from what would be expected from the prior distribution. (This can’t happen with a uniform prior because every value is equally likely). Indeed, with prior <span class="math inline">\(\theta \sim \dbeta(1, 9)\)</span> and observed data <span class="math inline">\(y = 9, n = 10\)</span>, we have <span class="math inline">\(\mathbb V(\theta) = \frac{9}{1100}\)</span> and <span class="math inline">\(\mathbb V(\theta \mid y) = \frac{1}{2} \cdot \frac{1}{2} \cdot \frac{1}{21} = \frac{1}{84}\)</span>.</p>

<div id="disqus_thread"></div>
<script>
/**
 *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
 *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
 */
/*
   var disqus_config = function () {
   this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
   this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
   };
 */
(function() {  // DON'T EDIT BELOW THIS LINE
 var d = document, s = d.createElement('script');

 s.src = '//stappit-github-io.disqus.com/embed.js';

 s.setAttribute('data-timestamp', +new Date());
 (d.head || d.body).appendChild(s);
 })();
</script>

<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

<script id="dsq-count-scr" src="//stappit-github-io.disqus.com/count.js" async></script>
]]></summary>
</entry>

</feed>
