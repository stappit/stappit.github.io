<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>Thoughts from the Café</title>
    <link href="http://stappit.github.io/atom.xml" rel="self" />
    <link href="http://stappit.github.io" />
    <id>http://stappit.github.io/atom.xml</id>
    <author>
        <name>Brian</name>
        <email>ha@hahaha.com</email>
    </author>
    <updated>2018-11-10T00:00:00Z</updated>
    <entry>
    <title>BDA3 Chapter 5 Exercise 4</title>
    <link href="http://stappit.github.io/posts/bda3/chapter_05_exercise_04.html" />
    <id>http://stappit.github.io/posts/bda3/chapter_05_exercise_04.html</id>
    <published>2018-11-10T00:00:00Z</published>
    <updated>2018-11-10T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 class="post-title">BDA3 Chapter 5 Exercise 4</h1>

<div class="card post-meta border-0">
  <div class="card-body post-meta">
    <p class="card-text text-muted text-left">
    Posted on November 10, 2018  by Brian </br>
     Tags: <a href="/tags/bda%20chapter%205.html">bda chapter 5</a>, <a href="/tags/solutions.html">solutions</a>, <a href="/tags/bayes.html">bayes</a>, <a href="/tags/de%20finetti.html">de finetti</a>, <a href="/tags/exchangeability.html">exchangeability</a>, <a href="/tags/counter%0Aexample.html">counter
example</a> </br>
     Category: <a href="/categories/bda3.html">bda3</a> 
    </p>
  </div>
</div>

<p>Here’s my solution to exercise 4, chapter 5, of <a href="https://andrewgelman.com/">Gelman’s</a> <em>Bayesian Data Analysis</em> (BDA), 3rd edition. There are <a href="http://www.stat.columbia.edu/~gelman/book/solutions.pdf">solutions</a> to some of the exercises on the <a href="http://www.stat.columbia.edu/~gelman/book/">book’s webpage</a>.</p>
<!--more-->
<div style="display:none">
<p class="mathjaxWide"><span class="math inline">\(\DeclareMathOperator{\dbinomial}{Binomial} \DeclareMathOperator{\dbern}{Bernoulli} \DeclareMathOperator{\dpois}{Poisson} \DeclareMathOperator{\dnorm}{Normal} \DeclareMathOperator{\dt}{t} \DeclareMathOperator{\dcauchy}{Cauchy} \DeclareMathOperator{\dexponential}{Exp} \DeclareMathOperator{\duniform}{Uniform} \DeclareMathOperator{\dgamma}{Gamma} \DeclareMathOperator{\dinvgamma}{InvGamma} \DeclareMathOperator{\invlogit}{InvLogit} \DeclareMathOperator{\dinvchi}{InvChi2} \DeclareMathOperator{\dsinvchi}{SInvChi2} \DeclareMathOperator{\dchi}{Chi2} \DeclareMathOperator{\dnorminvchi}{NormInvChi2} \DeclareMathOperator{\logit}{Logit} \DeclareMathOperator{\ddirichlet}{Dirichlet} \DeclareMathOperator{\dbeta}{Beta}\)</span></p>
</div>
<p>Suppose we have <span class="math inline">\(2J\)</span> parameters, of which <span class="math inline">\(J\)</span> are <span class="math inline">\(\dnorm(1, 1)\)</span> and the remaining <span class="math inline">\(J\)</span> are <span class="math inline">\(\dnorm(-1, 1)\)</span>. These parameters are exchangeable since we have no way of knowing which are in the former group in which in the latter. Their exchangeable joint probability distribution would be the product of the <span class="math inline">\(2J\)</span> independent normals averaged over all possible permutations.</p>
<p>Suppose we could write the joint distribution as a mixture of iid components</p>
<p class="mathjaxWide"><span class="math display">\[
p(\theta)
=
\int \prod_{j = 1}^J p(\theta_j \mid \phi) p(\phi) d\phi
.
\]</span></p>
<p>I won’t prove mathematically that his assumption leads to a contradiction, but rather simulate values to illustrate the point.</p>
<p>First, <a href="chapter_05_exercise_05.html">the solution to exercise 5</a> shows that the covariance of <span class="math inline">\(\theta_i, \theta_j\)</span>, <span class="math inline">\(i \ne j\)</span>, would have to be non-negative. So let’s simulate some values and take a look at the covariance.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">rbern &lt;-<span class="st"> </span><span class="cf">function</span>(n, p) <span class="dv">2</span> <span class="op">*</span><span class="st"> </span><span class="kw">rbinom</span>(n, <span class="dv">1</span>, p) <span class="op">-</span><span class="st"> </span><span class="dv">1</span>

simulate &lt;-<span class="st"> </span><span class="cf">function</span>(J, <span class="dt">iter =</span> <span class="dv">10000</span>) {
  <span class="kw">tibble</span>(
      <span class="dt">mu1 =</span> <span class="kw">rbern</span>(iter, <span class="fl">0.5</span>),
      <span class="dt">flip =</span> <span class="kw">rbern</span>(iter, (J <span class="op">-</span><span class="st"> </span><span class="dv">1</span>) <span class="op">/</span><span class="st"> </span>(<span class="dv">2</span><span class="op">*</span>J <span class="op">-</span><span class="st"> </span><span class="dv">1</span>)),
      <span class="dt">mu2 =</span> mu1 <span class="op">*</span><span class="st"> </span>flip,
      <span class="dt">theta1 =</span> <span class="kw">rnorm</span>(iter, mu1, <span class="dv">1</span>),
      <span class="dt">theta2 =</span> <span class="kw">rnorm</span>(iter, mu2, <span class="dv">1</span>)
    )
}</code></pre></div>
<p>In our <code>simulate</code> function, the <code>flip</code> variable indicates whether <span class="math inline">\(\theta_2\)</span> has a mean that is the negative of the mean of <span class="math inline">\(\theta_1\)</span>. Notice that the probability of a flip depends on <span class="math inline">\(J\)</span>. In particular, when <span class="math inline">\(J = 1\)</span> this probability is 1. For larger values of <span class="math inline">\(J\)</span>, the probability of a flip is strictly less than 0.5. As <span class="math inline">\(J \to \infty\)</span>, this probability converges to 0.5, as if <span class="math inline">\(\theta_1\)</span> and <span class="math inline">\(\theta_2\)</span> were independent.</p>
<figure>
<img src="chapter_05_exercise_04_files/figure-markdown/unnamed-chunk-2-1.png" />
</figure>
<p>For this case where <span class="math inline">\(J = 1\)</span>, the correlation is clearly negative. We can also see two clusters forming around <span class="math inline">\((1, -1)\)</span> and <span class="math inline">\((-1, 1)\)</span>. This seems reasonable since the mean of one is necessarily the negative of the mean of the other.</p>
<p>To see what happens when <span class="math inline">\(J \to \infty\)</span>, let’s calculate the covariance for a range of values of <span class="math inline">\(J\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">covariance &lt;-<span class="st"> </span><span class="cf">function</span>(sims)
  sims <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">summarise</span>(<span class="kw">cov</span>(theta1, theta2)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">pull</span>()

covariances &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">J =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">500</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(
    <span class="dt">sim =</span> <span class="kw">map</span>(J, simulate),
    <span class="dt">cov_theta1_theta2 =</span> <span class="kw">map_dbl</span>(sim, covariance)
  )</code></pre></div>
<p>We can see below that the covariance is quite negative for small values of <span class="math inline">\(J\)</span>, and it seems to converge very quickly to 0. I suspect the positive values that appear are due to sampling variance.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">covariances <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">ggplot</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">aes</span>(J, cov_theta1_theta2) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">alpha =</span> <span class="fl">0.4</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">y =</span> <span class="st">&#39;Covariance&#39;</span>)</code></pre></div>
<figure>
<img src="chapter_05_exercise_04_files/figure-markdown/unnamed-chunk-4-1.png" />
</figure>
<p>Finally, let’s plot the samples for a large value of <span class="math inline">\(J\)</span> to verify that there is (almost) no covariance.</p>
<figure>
<img src="chapter_05_exercise_04_files/figure-markdown/unnamed-chunk-5-1.png" />
</figure>
<p>The covariance is negative for finite <span class="math inline">\(J\)</span> since a large value of <span class="math inline">\(\theta_1\)</span> implies it is most likely from <span class="math inline">\(\dnorm(1, 1)\)</span>, which implies that <span class="math inline">\(\theta_2\)</span> is most likely from <span class="math inline">\(\dnorm(-1, 1)\)</span> and would have a smaller value (and vice versa). This contradicts the assumption that we can write the joint distribution as a mixture of iids.</p>
<p>This doesn’t lead to a contradiction to de Finetti’s theorem by letting <span class="math inline">\(J \to \infty\)</span> because the covariances converge to 0.</p>

<div id="disqus_thread"></div>
<script>
/**
 *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
 *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
 */
/*
   var disqus_config = function () {
   this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
   this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
   };
 */
(function() {  // DON'T EDIT BELOW THIS LINE
 var d = document, s = d.createElement('script');

 s.src = '//stappit-github-io.disqus.com/embed.js';

 s.setAttribute('data-timestamp', +new Date());
 (d.head || d.body).appendChild(s);
 })();
</script>

<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

<script id="dsq-count-scr" src="//stappit-github-io.disqus.com/count.js" async></script>
]]></summary>
</entry>
<entry>
    <title>BDA3 Chapter 5 Exercise 3</title>
    <link href="http://stappit.github.io/posts/bda3/chapter_05_exercise_03.html" />
    <id>http://stappit.github.io/posts/bda3/chapter_05_exercise_03.html</id>
    <published>2018-11-10T00:00:00Z</published>
    <updated>2018-11-10T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 class="post-title">BDA3 Chapter 5 Exercise 3</h1>

<div class="card post-meta border-0">
  <div class="card-body post-meta">
    <p class="card-text text-muted text-left">
    Posted on November 10, 2018  by Brian </br>
     Tags: <a href="/tags/bda%20chapter%205.html">bda chapter 5</a>, <a href="/tags/solutions.html">solutions</a>, <a href="/tags/bayes.html">bayes</a>, <a href="/tags/hierarchical%20model.html">hierarchical model</a>, <a href="/tags/eight%20schools.html">eight schools</a>, <a href="/tags/pooling.html">pooling</a> </br>
     Category: <a href="/categories/bda3.html">bda3</a> 
    </p>
  </div>
</div>

<p>Here’s my solution to exercise 3, chapter 5, of <a href="https://andrewgelman.com/">Gelman’s</a> <em>Bayesian Data Analysis</em> (BDA), 3rd edition. There are <a href="http://www.stat.columbia.edu/~gelman/book/solutions.pdf">solutions</a> to some of the exercises on the <a href="http://www.stat.columbia.edu/~gelman/book/">book’s webpage</a>.</p>
<!--more-->
<div style="display:none">
<p class="mathjaxWide"><span class="math inline">\(\DeclareMathOperator{\dbinomial}{Binomial} \DeclareMathOperator{\dbern}{Bernoulli} \DeclareMathOperator{\dpois}{Poisson} \DeclareMathOperator{\dnorm}{Normal} \DeclareMathOperator{\dt}{t} \DeclareMathOperator{\dcauchy}{Cauchy} \DeclareMathOperator{\dexponential}{Exp} \DeclareMathOperator{\duniform}{Uniform} \DeclareMathOperator{\dgamma}{Gamma} \DeclareMathOperator{\dinvgamma}{InvGamma} \DeclareMathOperator{\invlogit}{InvLogit} \DeclareMathOperator{\logit}{Logit} \DeclareMathOperator{\ddirichlet}{Dirichlet} \DeclareMathOperator{\dbeta}{Beta}\)</span></p>
</div>
<p>We’ll reproduce some of the calculations with different priors for the eight schools example. Here is the <a href="data/eight_schools.csv">eight schools dataset</a>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">df &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="st">&#39;data/eight_schools.csv&#39;</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">school =</span> <span class="kw">factor</span>(school))</code></pre></div>
<table class="table table-striped table-hover table-responsive" style="margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
school
</th>
<th style="text-align:right;">
y
</th>
<th style="text-align:right;">
std
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
A
</td>
<td style="text-align:right;">
25
</td>
<td style="text-align:right;">
15
</td>
</tr>
<tr>
<td style="text-align:left;">
B
</td>
<td style="text-align:right;">
8
</td>
<td style="text-align:right;">
10
</td>
</tr>
<tr>
<td style="text-align:left;">
C
</td>
<td style="text-align:right;">
-3
</td>
<td style="text-align:right;">
16
</td>
</tr>
<tr>
<td style="text-align:left;">
D
</td>
<td style="text-align:right;">
7
</td>
<td style="text-align:right;">
11
</td>
</tr>
<tr>
<td style="text-align:left;">
E
</td>
<td style="text-align:right;">
-1
</td>
<td style="text-align:right;">
19
</td>
</tr>
<tr>
<td style="text-align:left;">
F
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
11
</td>
</tr>
<tr>
<td style="text-align:left;">
G
</td>
<td style="text-align:right;">
18
</td>
<td style="text-align:right;">
10
</td>
</tr>
<tr>
<td style="text-align:left;">
H
</td>
<td style="text-align:right;">
12
</td>
<td style="text-align:right;">
18
</td>
</tr>
</tbody>
</table>
<h2 id="uniform-priors">Uniform priors</h2>
<p>We’ll use <a href="http://mc-stan.org/">Stan</a> to calculate the correct posterior for us. Note that Stan will assume a uniform prior (on the domain of the parameter) unless otherwise specified.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">model &lt;-<span class="st"> </span>rstan<span class="op">::</span><span class="kw">stan_model</span>(<span class="st">&#39;src/ex_05_03.stan&#39;</span>)</code></pre></div>
<pre><code>S4 class stanmodel &#39;ex_05_03&#39; coded as follows:
data {
  int&lt;lower = 0&gt; J; // number of schools 
  vector[J] y; // estimated treatment effects
  vector&lt;lower = 0&gt;[J] sigma; // standard errors
}

parameters {
  real mu; // pop mean
  real&lt;lower = 0&gt; tau; // pop std deviation
  vector[J] eta; // school-level errors
}

transformed parameters {
  vector[J] theta = mu + tau * eta; // school effects
}

model {
  eta ~ normal(0, 1);
  y ~ normal(theta, sigma);
} </code></pre>
<p>We fit the model with the <a href="http://mc-stan.org/rstan/reference/stanmodel-method-sampling.html">sampling</a> function.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit &lt;-<span class="st"> </span>model <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span>rstan<span class="op">::</span><span class="kw">sampling</span>(
    <span class="dt">data =</span> <span class="kw">list</span>(
      <span class="dt">J =</span> <span class="kw">nrow</span>(df),
      <span class="dt">y =</span> df<span class="op">$</span>y,
      <span class="dt">sigma =</span> df<span class="op">$</span>std
    ),
    <span class="dt">warmup =</span> <span class="dv">1000</span>,
    <span class="dt">iter =</span> <span class="dv">5000</span>,
    <span class="dt">chains =</span> <span class="dv">4</span>
  )</code></pre></div>
<p>The <a href="https://mjskay.github.io/tidybayes/articles/tidybayes.html">tidybayes package</a> is super useful for custom calculations from the posterior draws. We’ll also add in the original school labels.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">draws &lt;-<span class="st"> </span>fit <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span>tidybayes<span class="op">::</span><span class="kw">spread_draws</span>(mu, tau, eta[school_idx]) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(
    <span class="dt">theta =</span> mu <span class="op">+</span><span class="st"> </span>tau <span class="op">*</span><span class="st"> </span>eta,
    <span class="dt">school =</span> <span class="kw">levels</span>(df<span class="op">$</span>school)[school_idx]
  ) </code></pre></div>
<p>We have 4 chains, each with 4000 (post-warmup) iterations, with a draw for each school parameter. Each draw is one sample from the posterior.</p>
<table class="table table-striped table-hover table-responsive" style="margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:right;">
.chain
</th>
<th style="text-align:right;">
.iteration
</th>
<th style="text-align:right;">
.draw
</th>
<th style="text-align:right;">
mu
</th>
<th style="text-align:right;">
tau
</th>
<th style="text-align:right;">
school_idx
</th>
<th style="text-align:right;">
eta
</th>
<th style="text-align:right;">
theta
</th>
<th style="text-align:left;">
school
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
8.498217
</td>
<td style="text-align:right;">
2.685788
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1.6796328
</td>
<td style="text-align:right;">
13.009354
</td>
<td style="text-align:left;">
A
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
8.498217
</td>
<td style="text-align:right;">
2.685788
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
-0.9133598
</td>
<td style="text-align:right;">
6.045126
</td>
<td style="text-align:left;">
B
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
8.498217
</td>
<td style="text-align:right;">
2.685788
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
0.1107519
</td>
<td style="text-align:right;">
8.795673
</td>
<td style="text-align:left;">
C
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
8.498217
</td>
<td style="text-align:right;">
2.685788
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
1.4297585
</td>
<td style="text-align:right;">
12.338245
</td>
<td style="text-align:left;">
D
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
8.498217
</td>
<td style="text-align:right;">
2.685788
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
1.3404477
</td>
<td style="text-align:right;">
12.098375
</td>
<td style="text-align:left;">
E
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
8.498217
</td>
<td style="text-align:right;">
2.685788
</td>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
0.3785970
</td>
<td style="text-align:right;">
9.515048
</td>
<td style="text-align:left;">
F
</td>
</tr>
</tbody>
</table>
<p>Tidybayes also gives us convenient ggplot geoms for plotting the posterior distributions.</p>
<figure>
<img src="chapter_05_exercise_03_files/figure-markdown/effects_plot-1..svg" />
</figure>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">comparisons &lt;-<span class="st"> </span>draws <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(school) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span>tidybayes<span class="op">::</span><span class="kw">compare_levels</span>(theta, <span class="dt">by =</span> school) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span>tidybayes<span class="op">::</span><span class="kw">mean_qi</span>()</code></pre></div>
<figure>
<img src="chapter_05_exercise_03_files/figure-markdown/comparisons_plot-1..svg" />
</figure>
<p>We can also see how the estimated treatment effect varies as a function of the population variation. The curves are noiser than in the book because we are using our posterior draws to approximate the shape and there are relatively fewer draws for larger values of <span class="math inline">\(\tau\)</span>.</p>
<figure>
<img src="chapter_05_exercise_03_files/figure-markdown/effect_vs_tau-1..svg" />
</figure>
<p>Here’s a simple histogram of the posterior draws for school A.</p>
<figure>
<img src="chapter_05_exercise_03_files/figure-markdown/school_a_effect_plot-1..svg" />
</figure>
<p>To estimate the posterior for the maximum effect, we can simply calculate the maximum effect across all schools for each posterior draw.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">max_theta &lt;-<span class="st"> </span>draws <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(.chain, .iteration, .draw) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">slice</span>(<span class="kw">which.max</span>(theta)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">ungroup</span>()</code></pre></div>
<p>The probability that the maximum effect is larger than 28.4 can then be approximated by the fraction of draws larger than 28.4.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">p_max_theta &lt;-<span class="st"> </span>max_theta <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">larger =</span> theta <span class="op">&gt;</span><span class="st"> </span><span class="fl">28.4</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">summarise</span>(<span class="dt">p_larger =</span> <span class="kw">sum</span>(larger) <span class="op">/</span><span class="st"> </span><span class="kw">n</span>()) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">pull</span>() <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">percent</span>()

p_max_theta</code></pre></div>
<pre><code>[1] &quot;6.97%&quot;</code></pre>
<figure>
<img src="chapter_05_exercise_03_files/figure-markdown/max_plot-1..svg" />
</figure>
<p>To estimate the probability than the effect in school A is larger than the effect in school C, we first have to spread the data so that there is one draw per row.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">a_better_c &lt;-<span class="st"> </span>draws <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">ungroup</span>()  <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">select</span>(.chain, .iteration, school, theta) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">spread</span>(school, theta) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">a_minus_c =</span> A <span class="op">-</span><span class="st"> </span>C) </code></pre></div>
<p>The probability is then just the fraction of draws where A - C &gt; 0.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">prob_a_better_c &lt;-<span class="st"> </span>a_better_c <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">summarise</span>(<span class="kw">mean</span>(a_minus_c <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">pull</span>() <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">percent</span>()

prob_a_better_c</code></pre></div>
<pre><code>[1] &quot;65%&quot;</code></pre>
<figure>
<img src="chapter_05_exercise_03_files/figure-markdown/a_better_c_plot-1..svg" />
</figure>
<h2 id="infinite-population-variance">Infinite population variance</h2>
<p>With <span class="math inline">\(\tau = \infty\)</span>, we would expect there to be no shrinkage. From equation 5.17 (page 116), the posteriors of the school effects with <span class="math inline">\(\tau \to \infty\)</span> are</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
  \theta_j \mid \mu, \tau = \infty, y \sim \dnorm\left( \bar y_{\cdot j}, \sigma_j^2 \right)
\end{align}
\]</span></p>
<p>since <span class="math inline">\(\frac{1}{\tau} \to 0\)</span> as <span class="math inline">\(\tau \to \infty\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">iters &lt;-<span class="st"> </span><span class="dv">16000</span>

draws_infty &lt;-<span class="st"> </span>df <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">transmute</span>(
    school,
    <span class="dt">draws =</span> <span class="kw">map2</span>(
      y, std, 
      <span class="cf">function</span>(mu, sigma) {
        <span class="kw">tibble</span>(
          <span class="dt">iteration =</span> <span class="dv">1</span><span class="op">:</span>iters,
          <span class="dt">theta =</span> <span class="kw">rnorm</span>(iters, mu, sigma)
        )
      }
    )
  ) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">unnest</span>(draws) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">arrange</span>(iteration)</code></pre></div>
<table class="table table-striped table-hover table-responsive" style="margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
school
</th>
<th style="text-align:right;">
iteration
</th>
<th style="text-align:right;">
theta
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
A
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
17.305220
</td>
</tr>
<tr>
<td style="text-align:left;">
B
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
14.584088
</td>
</tr>
<tr>
<td style="text-align:left;">
C
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
-8.254098
</td>
</tr>
<tr>
<td style="text-align:left;">
D
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
-5.412114
</td>
</tr>
<tr>
<td style="text-align:left;">
E
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
-5.607103
</td>
</tr>
<tr>
<td style="text-align:left;">
F
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
6.429439
</td>
</tr>
</tbody>
</table>
<p>We calculate the maximum effect just as before. The histogram shows that there is a higher probability of higher treatment effects than under the hierarchical model.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">max_theta_infty &lt;-<span class="st"> </span>draws_infty <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(iteration) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">slice</span>(<span class="kw">which.max</span>(theta))</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">p_max_theta_infty &lt;-<span class="st"> </span>max_theta_infty <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">ungroup</span>() <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">larger =</span> theta <span class="op">&gt;</span><span class="st"> </span><span class="fl">28.4</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">summarise</span>(<span class="dt">p_larger =</span> <span class="kw">sum</span>(larger) <span class="op">/</span><span class="st"> </span><span class="kw">n</span>()) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">pull</span>() <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">percent</span>()

p_max_theta_infty</code></pre></div>
<pre><code>[1] &quot;63.8%&quot;</code></pre>
<p>There is now a 63.8% probability of an extreme effect under the unpooled model, which is a lot larger than 6.97% under the hierarchical model.</p>
<figure>
<img src="chapter_05_exercise_03_files/figure-markdown/max_plot_infty-1..svg" />
</figure>
<p>For the pairwise differences, both the point estimates and the credible intervals are more extreme.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">comparisons_infty &lt;-<span class="st"> </span>draws_infty <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(school) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">compare_levels</span>(theta, <span class="dt">by =</span> school, <span class="dt">draw_indices =</span> <span class="kw">c</span>(<span class="st">&#39;iteration&#39;</span>)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">select</span>(<span class="op">-</span><span class="kw">starts_with</span>(<span class="st">&#39;iter&#39;</span>)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mean_qi</span>()</code></pre></div>
<figure>
<img src="chapter_05_exercise_03_files/figure-markdown/comparisons_infty_plot-1..svg" />
</figure>
<h2 id="zero-population-variance">Zero population variance</h2>
<p>With <span class="math inline">\(\tau = 0\)</span>, we would expect the estimates of school effects to all be equal to the population effect. Letting <span class="math inline">\(\tau \to 0\)</span> in equation 5.17 (page 116), we see that <span class="math inline">\(\theta_j \mid \mu, \tau, y\)</span> gets a point mass at $. This follows from the fact that</p>
<p class="mathjaxWide"><span class="math display">\[
\frac{\frac{1}{\tau}}{c + \frac{1}{\tau}} \to 1 \to \infty
\]</span></p>
<p>for any fixed <span class="math inline">\(c\)</span> as <span class="math inline">\(\tau \to 0\)</span>. Thus,</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
  \hat \theta_j
  &amp;=
  \frac{\frac{\bar y_{\cdot j}}{\sigma_j}}{\frac{1}{\sigma_j} + \frac{1}{\tau^2}} + \frac{\frac{1}{\tau^2}}{\frac{1}{\sigma_j} + \frac{1}{\tau^2}}\mu
  \to
  0 + \mu
  \\
  V_j &amp;\to 0
  .
\end{align}
\]</span></p>
<p>It follows that <span class="math inline">\(p(\theta \mid \mu, \tau, y) \to p(\mu \mid \tau, y)\)</span> as <span class="math inline">\(\tau \to 0\)</span>. From equation 5.20 (page 117), the distribution of <span class="math inline">\(\mu \mid \tau, y\)</span> is <span class="math inline">\(\dnorm(\hat\mu, V_\mu)\)</span> with</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
\hat \mu 
&amp;= 
\frac{\sum_1^J \frac{1}{\sigma_j^2} \bar y_{\cdot j}}{\sum_1^J \frac{1}{\sigma_j^2}}
=
\bar y_{\cdot \cdot}
\\
V_\mu^{-1}
&amp;=
\sum_1^J \frac{1}{\sigma_j^2}
.
\end{align}
\]</span></p>

<div id="disqus_thread"></div>
<script>
/**
 *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
 *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
 */
/*
   var disqus_config = function () {
   this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
   this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
   };
 */
(function() {  // DON'T EDIT BELOW THIS LINE
 var d = document, s = d.createElement('script');

 s.src = '//stappit-github-io.disqus.com/embed.js';

 s.setAttribute('data-timestamp', +new Date());
 (d.head || d.body).appendChild(s);
 })();
</script>

<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

<script id="dsq-count-scr" src="//stappit-github-io.disqus.com/count.js" async></script>
]]></summary>
</entry>
<entry>
    <title>BDA3 Chapter 5 Exercise 2</title>
    <link href="http://stappit.github.io/posts/bda3/chapter_05_exercise_02.html" />
    <id>http://stappit.github.io/posts/bda3/chapter_05_exercise_02.html</id>
    <published>2018-11-05T00:00:00Z</published>
    <updated>2018-11-05T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 class="post-title">BDA3 Chapter 5 Exercise 2</h1>

<div class="card post-meta border-0">
  <div class="card-body post-meta">
    <p class="card-text text-muted text-left">
    Posted on November  5, 2018  by Brian </br>
     Tags: <a href="/tags/bda%20chapter%205.html">bda chapter 5</a>, <a href="/tags/solutions.html">solutions</a>, <a href="/tags/bayes.html">bayes</a>, <a href="/tags/exchangeability.html">exchangeability</a> </br>
     Category: <a href="/categories/bda3.html">bda3</a> 
    </p>
  </div>
</div>

<p>Here’s my solution to exercise 2, chapter 5, of <a href="https://andrewgelman.com/">Gelman’s</a> <em>Bayesian Data Analysis</em> (BDA), 3rd edition. There are <a href="http://www.stat.columbia.edu/~gelman/book/solutions.pdf">solutions</a> to some of the exercises on the <a href="http://www.stat.columbia.edu/~gelman/book/">book’s webpage</a>.</p>
<!--more-->
<div style="display:none">
<p class="mathjaxWide"><span class="math inline">\(\DeclareMathOperator{\dbinomial}{Binomial}  \DeclareMathOperator{\dbern}{Bernoulli}  \DeclareMathOperator{\dpois}{Poisson}  \DeclareMathOperator{\dnorm}{Normal}  \DeclareMathOperator{\dt}{t}  \DeclareMathOperator{\dcauchy}{Cauchy}  \DeclareMathOperator{\dexponential}{Exp}  \DeclareMathOperator{\duniform}{Uniform}  \DeclareMathOperator{\dgamma}{Gamma}  \DeclareMathOperator{\dinvgamma}{InvGamma}  \DeclareMathOperator{\invlogit}{InvLogit}  \DeclareMathOperator{\dinvchi}{InvChi2}  \DeclareMathOperator{\dsinvchi}{SInvChi2}  \DeclareMathOperator{\dchi}{Chi2}  \DeclareMathOperator{\dnorminvchi}{NormInvChi2}  \DeclareMathOperator{\logit}{Logit}  \DeclareMathOperator{\ddirichlet}{Dirichlet}  \DeclareMathOperator{\dbeta}{Beta}\)</span></p>
</div>
<p>This exercise is an extension of <a href="./chapter_05_exercise_01.html">the previous exercise</a>. Suppose we have a box with <span class="math inline">\(n\)</span> balls, with <span class="math inline">\(B\)</span> black balls and <span class="math inline">\(W\)</span> white balls (<span class="math inline">\(n = B + W\)</span>), where we don’t know how many of each. We pick a ball <span class="math inline">\(y_1\)</span> at random, put it back, then pick another ball <span class="math inline">\(y_2\)</span> at random. In this case, Then <span class="math inline">\(y_1\)</span> and <span class="math inline">\(y_2\)</span> are independent, and therefore also exchangeable.</p>
<p>The draws are no longer independent if we don’t put the first ball back before picking the second ball. They remain exchangeable since the conditional joint probability function does not depend on the order</p>
<p class="mathjaxWide"><span class="math display">\[
  p(y_1, y_2 \mid B, W)
  =
  \begin{cases}
    \frac{BW}{n(n - 1)} &amp; \text{ if } y_1 \ne y_2 \\
    \frac{W(W - 1)}{n (n - 1)} &amp; \text{ if } y_1 = y_2 = \text{white} \\
    \frac{B(B - 1)}{n (n - 1)} &amp; \text{ if } y_1 = y_2 = \text{black,} 
  \end{cases}
\]</span></p>
<p>which implies that the joint probability does not depend on the order</p>
<p class="mathjaxWide"><span class="math display">\[
  p(y_1, y_2)
  =
  \sum_{B + W \ge 2 \\ B \cdot W &gt; 0}^\infty p(y_1, y_2 \mid B, W) p(B, W)
  .
\]</span></p>
<p>Whether we can treat them as if they were independent depends on the prior <span class="math inline">\(p(B, W)\)</span>. If there is significant probability mass on low values, then we shouldn’t treat them as independent (see <a href="./chapter_05_exercise_01.html">the previous exercise</a>). If the only significant probability mass were on very large values of <span class="math inline">\(B\)</span> and <span class="math inline">\(W\)</span>, then we could treat them as if they were independent. This follows from the fact that</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
  \frac{B-1}{n - 1} 
  &amp;\approx
  \frac{B}{n}
  \approx
  \frac{B}{n-1}
  \\
  \frac{W-1}{n - 1} 
  &amp;\approx
  \frac{W}{n}
  \approx
  \frac{W}{n-1}
  ,
\end{align}
\]</span></p>
<p>when <span class="math inline">\(B, W \gg 0\)</span>.</p>

<div id="disqus_thread"></div>
<script>
/**
 *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
 *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
 */
/*
   var disqus_config = function () {
   this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
   this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
   };
 */
(function() {  // DON'T EDIT BELOW THIS LINE
 var d = document, s = d.createElement('script');

 s.src = '//stappit-github-io.disqus.com/embed.js';

 s.setAttribute('data-timestamp', +new Date());
 (d.head || d.body).appendChild(s);
 })();
</script>

<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

<script id="dsq-count-scr" src="//stappit-github-io.disqus.com/count.js" async></script>
]]></summary>
</entry>
<entry>
    <title>BDA3 Chapter 5 Exercise 1</title>
    <link href="http://stappit.github.io/posts/bda3/chapter_05_exercise_01.html" />
    <id>http://stappit.github.io/posts/bda3/chapter_05_exercise_01.html</id>
    <published>2018-11-05T00:00:00Z</published>
    <updated>2018-11-05T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 class="post-title">BDA3 Chapter 5 Exercise 1</h1>

<div class="card post-meta border-0">
  <div class="card-body post-meta">
    <p class="card-text text-muted text-left">
    Posted on November  5, 2018  by Brian </br>
     Tags: <a href="/tags/bda%20chapter%205.html">bda chapter 5</a>, <a href="/tags/solutions.html">solutions</a>, <a href="/tags/bayes.html">bayes</a>, <a href="/tags/exchangeability.html">exchangeability</a> </br>
     Category: <a href="/categories/bda3.html">bda3</a> 
    </p>
  </div>
</div>

<p>Here’s my solution to exercise 1, chapter 5, of <a href="https://andrewgelman.com/">Gelman’s</a> <em>Bayesian Data Analysis</em> (BDA), 3rd edition. There are <a href="http://www.stat.columbia.edu/~gelman/book/solutions.pdf">solutions</a> to some of the exercises on the <a href="http://www.stat.columbia.edu/~gelman/book/">book’s webpage</a>.</p>
<!--more-->
<div style="display:none">
<p class="mathjaxWide"><span class="math inline">\(\DeclareMathOperator{\dbinomial}{Binomial}  \DeclareMathOperator{\dbern}{Bernoulli}  \DeclareMathOperator{\dpois}{Poisson}  \DeclareMathOperator{\dnorm}{Normal}  \DeclareMathOperator{\dt}{t}  \DeclareMathOperator{\dcauchy}{Cauchy}  \DeclareMathOperator{\dexponential}{Exp}  \DeclareMathOperator{\duniform}{Uniform}  \DeclareMathOperator{\dgamma}{Gamma}  \DeclareMathOperator{\dinvgamma}{InvGamma}  \DeclareMathOperator{\invlogit}{InvLogit}  \DeclareMathOperator{\dinvchi}{InvChi2}  \DeclareMathOperator{\dsinvchi}{SInvChi2}  \DeclareMathOperator{\dchi}{Chi2}  \DeclareMathOperator{\dnorminvchi}{NormInvChi2}  \DeclareMathOperator{\logit}{Logit}  \DeclareMathOperator{\ddirichlet}{Dirichlet}  \DeclareMathOperator{\dbeta}{Beta}\)</span></p>
</div>
<p>Suppose we have a box with one black ball and one white ball inside. We picka ball <span class="math inline">\(y_1\)</span> at random, put it back, then pick another ball <span class="math inline">\(y_2\)</span> at random. Then <span class="math inline">\(y_1\)</span> and <span class="math inline">\(y_2\)</span> are independent, and therefore also exchangeable.</p>
<p>The draws are no longer independent if we don’t put the first ball back before picking the second ball. They remain exchangeable since the joint probability only depends on the equality of the arguments, not on their order. More specifically,</p>
<p class="mathjaxWide"><span class="math display">\[
  p(y_1, y_2)
  =
  \begin{cases}
    \frac{1}{2} &amp; \text{ if } y_1 \ne y_2 \\
    0 &amp; \text{ otherwise.}
  \end{cases}
\]</span></p>
<p>Treating the draws as if they were independent would not be a good idea.</p>
<p>If there were <span class="math inline">\(M\)</span> balls of each colour, where <span class="math inline">\(M \gg 0\)</span> such as <span class="math inline">\(M = 10^6\)</span>, then the draws would still not be independent, but they would be very close to independence. They remain exchangeable. This is clearer from the joint probability function</p>
<p class="mathjaxWide"><span class="math display">\[
  p(y_1, y_2)
  =
  \begin{cases}
    \frac{1}{2}\frac{M}{2M - 1} &amp; \text{ if } y_1 \ne y_2 \\
    \frac{1}{2}\frac{M - 1}{2M - 1} &amp; \text{ otherwise,}
  \end{cases}
\]</span></p>
<p>since <span class="math inline">\(\frac{M}{2M - 1} \approx \frac{1}{2} \approx \frac{M - 1}{2M - 1}\)</span> for <span class="math inline">\(M \gg 0\)</span>.</p>

<div id="disqus_thread"></div>
<script>
/**
 *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
 *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
 */
/*
   var disqus_config = function () {
   this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
   this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
   };
 */
(function() {  // DON'T EDIT BELOW THIS LINE
 var d = document, s = d.createElement('script');

 s.src = '//stappit-github-io.disqus.com/embed.js';

 s.setAttribute('data-timestamp', +new Date());
 (d.head || d.body).appendChild(s);
 })();
</script>

<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

<script id="dsq-count-scr" src="//stappit-github-io.disqus.com/count.js" async></script>
]]></summary>
</entry>
<entry>
    <title>BDA3 Chapter 4 Exercise 1</title>
    <link href="http://stappit.github.io/posts/bda3/chapter_04_exercise_01.html" />
    <id>http://stappit.github.io/posts/bda3/chapter_04_exercise_01.html</id>
    <published>2018-11-03T00:00:00Z</published>
    <updated>2018-11-03T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 class="post-title">BDA3 Chapter 4 Exercise 1</h1>

<div class="card post-meta border-0">
  <div class="card-body post-meta">
    <p class="card-text text-muted text-left">
    Posted on November  3, 2018  by Brian </br>
     Tags: <a href="/tags/bda%20chapter%204.html">bda chapter 4</a>, <a href="/tags/solutions.html">solutions</a>, <a href="/tags/bayes.html">bayes</a>, <a href="/tags/cauchy.html">cauchy</a>, <a href="/tags/normal%20approximation.html">normal approximation</a> </br>
     Category: <a href="/categories/bda3.html">bda3</a> 
    </p>
  </div>
</div>

<p>Here’s my solution to exercise 1, chapter 4, of <a href="https://andrewgelman.com/">Gelman’s</a> <em>Bayesian Data Analysis</em> (BDA), 3rd edition. There are <a href="http://www.stat.columbia.edu/~gelman/book/solutions.pdf">solutions</a> to some of the exercises on the <a href="http://www.stat.columbia.edu/~gelman/book/">book’s webpage</a>.</p>
<!--more-->
<div style="display:none">
<p class="mathjaxWide"><span class="math inline">\(\DeclareMathOperator{\dbinomial}{Binomial} \DeclareMathOperator{\dbern}{Bernoulli} \DeclareMathOperator{\dpois}{Poisson} \DeclareMathOperator{\dnorm}{Normal} \DeclareMathOperator{\dt}{t} \DeclareMathOperator{\dcauchy}{Cauchy} \DeclareMathOperator{\dexponential}{Exp} \DeclareMathOperator{\duniform}{Uniform} \DeclareMathOperator{\dgamma}{Gamma} \DeclareMathOperator{\dinvgamma}{InvGamma} \DeclareMathOperator{\invlogit}{InvLogit} \DeclareMathOperator{\dinvchi}{InvChi2} \DeclareMathOperator{\dnorminvchi}{NormInvChi2} \DeclareMathOperator{\logit}{Logit} \DeclareMathOperator{\ddirichlet}{Dirichlet} \DeclareMathOperator{\dbeta}{Beta}\)</span></p>
</div>
<p>Suppose the likelihood is Cauchy, <span class="math inline">\(p(y_i \mid \theta) \propto (1 + (y_i - \theta)^2)^{-1}\)</span>, with a prior uniform on <span class="math inline">\([0, 1]\)</span>. Then the posterior has the same equation as the likelihood on the support of <span class="math inline">\(\theta\)</span>. Part of the exercise is to find the posterior mode but the hint is more confusing than helpful. Solving for the mode algebraically involves solving a polynomial of degree <span class="math inline">\(2n + 1\)</span>, where <span class="math inline">\(n\)</span> is the number of observations. We’ll use some numerical approximations to find the mode.</p>
<h2 id="posterior-mode">Posterior mode</h2>
<p>The observed data are as follows.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">y &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>, <span class="dv">0</span>, <span class="fl">1.5</span>, <span class="fl">2.5</span>)</code></pre></div>
<p>We could make draws from the posterior by coding this up in stan. However, an estimation of the mode from such values is difficult. An alternative is to use numerical optimisation. For that we’ll use the posterior on the log scale.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">log_likelihood &lt;-<span class="st"> </span><span class="cf">function</span>(y, theta)
  (<span class="dv">1</span> <span class="op">+</span><span class="st"> </span>(y <span class="op">-</span><span class="st"> </span>theta)<span class="op">^</span><span class="dv">2</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">log</span>() <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">sum</span>() <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">map_dbl</span>(<span class="st">`</span><span class="dt">*</span><span class="st">`</span>, <span class="op">-</span><span class="dv">1</span>)

log_posterior_given &lt;-<span class="st"> </span><span class="cf">function</span>(y) {
  <span class="cf">function</span>(theta) {
    <span class="cf">if</span> (theta <span class="op">&lt;</span><span class="st"> </span><span class="dv">0</span> <span class="op">|</span><span class="st"> </span><span class="dv">1</span> <span class="op">&lt;</span><span class="st"> </span>theta) {
      <span class="kw">return</span>(<span class="op">-</span><span class="ot">Inf</span>)
    } <span class="cf">else</span> {
      <span class="kw">return</span>(<span class="kw">log_likelihood</span>(y, theta))
    }
  }
}

log_posterior &lt;-<span class="st"> </span><span class="kw">log_posterior_given</span>(y)</code></pre></div>
<p>Numerical maximisation gives us a value near, but not quite, zero.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mode_numerical &lt;-<span class="st"> </span><span class="kw">optimise</span>(log_posterior, <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">maximum =</span> <span class="ot">TRUE</span>)<span class="op">$</span>maximum
mode_numerical</code></pre></div>
<pre><code>[1] 6.610696e-05</code></pre>
<p>Let’s plot the posterior.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">granularity &lt;-<span class="st"> </span><span class="fl">1e5</span>
grid &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">theta =</span> <span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dt">length.out =</span> granularity)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(
    <span class="dt">id =</span> <span class="dv">1</span><span class="op">:</span><span class="kw">n</span>(),
    <span class="dt">log_unnormalised_density =</span> <span class="kw">map_dbl</span>(theta, log_posterior),
    <span class="dt">unnormalised_density =</span> <span class="kw">exp</span>(log_unnormalised_density),
    <span class="dt">density =</span> granularity <span class="op">*</span><span class="st"> </span>unnormalised_density <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(unnormalised_density),
    <span class="dt">is_mode =</span> <span class="kw">abs</span>(theta <span class="op">-</span><span class="st"> </span><span class="kw">signif</span>(mode_numerical, <span class="dt">digits =</span> <span class="dv">1</span>)) <span class="op">&lt;</span><span class="st"> </span><span class="fl">0.5</span> <span class="op">/</span><span class="st"> </span>granularity
  ) </code></pre></div>
<figure>
<img src="chapter_04_exercise_01_files/figure-markdown/grid_plot-1..svg" />
</figure>
<p>Indeed, it looks like the mode could be 0. Zooming in we see that it is very likely to be zero.</p>
<figure>
<img src="chapter_04_exercise_01_files/figure-markdown/closeup-1..svg" />
</figure>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mode &lt;-<span class="st"> </span><span class="dv">0</span></code></pre></div>
<h2 id="normal-approximation">Normal approximation</h2>
<p>Now let’s calculate the derivatives in order to find the normal approximation. The derivative of the log posterior is</p>
<p class="mathjaxWide"><span class="math display">\[
\frac{d}{d\theta} \log p(y \mid \theta)
=
2 \sum_1^5 \frac{y_i - \theta}{1 + (y_i - \theta)^2}
.
\]</span></p>
<p>The second derivative of the log posterior is</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
\frac{d^2}{d\theta^2} \log p(y \mid \theta)
&amp;=
\sum_1^5
\frac{
  \frac{-2}{1 + (y_i - \theta)^2} - \frac{2(y_i - \theta)}{(1 + (y_i - \theta)^2)^2}\cdot 2(y_i - \theta)
}{
  \left( 1 + (y_i - \theta)^2 \right)^2
}
\\
&amp;=
\sum_1^5
\frac{-2\left( 1 + (y_i - \theta)^2 \right)^2 - 4 (y_i - \theta)^2}{\left( 1 + (y_i - \theta)^2 \right)^4}
\\
&amp;=
-2
\sum_1^5
\frac{3(y_i - \theta)^2 + 2(y_i - \theta) + 1}{\left( 1 + (y_i - \theta)^2 \right)^4}
.
\end{align}
\]</span></p>
<p>Evaluating this at the mode gives</p>
<p class="mathjaxWide"><span class="math display">\[
-2 \sum_1^5 \frac{3y_i^2 + 2y_i + 1}{\left( 1 + y_i^2 \right)^4}
.
\]</span></p>
<p>The means that the observed information is</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">I &lt;-<span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span><span class="kw">sum</span>((<span class="dv">3</span> <span class="op">*</span><span class="st"> </span>y<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>y <span class="op">+</span><span class="st"> </span><span class="dv">1</span>) <span class="op">/</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span>y<span class="op">^</span><span class="dv">2</span>)<span class="op">^</span><span class="dv">4</span>)
I</code></pre></div>
<pre><code>[1] 2.489427</code></pre>
<p>This gives us the normal approximation with</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mu &lt;-<span class="st"> </span>mode
variance &lt;-<span class="st"> </span><span class="dv">1</span> <span class="op">/</span><span class="st"> </span>I
std &lt;-<span class="st"> </span><span class="kw">sqrt</span>(variance)

<span class="kw">c</span>(mu, std)</code></pre></div>
<pre><code>[1] 0.0000000 0.6337972</code></pre>
<p>which gives us the normal approximation <span class="math inline">\(p(\theta \mid y) \approx \dnorm(0, 0.634)\)</span> on <span class="math inline">\([0, 1]\)</span>.</p>
<figure>
<img src="chapter_04_exercise_01_files/figure-markdown/approx_plot-1..svg" />
</figure>
<p>The approximation isn’t very good.</p>

<div id="disqus_thread"></div>
<script>
/**
 *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
 *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
 */
/*
   var disqus_config = function () {
   this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
   this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
   };
 */
(function() {  // DON'T EDIT BELOW THIS LINE
 var d = document, s = d.createElement('script');

 s.src = '//stappit-github-io.disqus.com/embed.js';

 s.setAttribute('data-timestamp', +new Date());
 (d.head || d.body).appendChild(s);
 })();
</script>

<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

<script id="dsq-count-scr" src="//stappit-github-io.disqus.com/count.js" async></script>
]]></summary>
</entry>
<entry>
    <title>BDA3 Chapter 3 Exercise 11</title>
    <link href="http://stappit.github.io/posts/bda3/chapter_03_exercise_11.html" />
    <id>http://stappit.github.io/posts/bda3/chapter_03_exercise_11.html</id>
    <published>2018-10-21T00:00:00Z</published>
    <updated>2018-10-21T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 class="post-title">BDA3 Chapter 3 Exercise 11</h1>

<div class="card post-meta border-0">
  <div class="card-body post-meta">
    <p class="card-text text-muted text-left">
    Posted on October 21, 2018  by Brian </br>
     Tags: <a href="/tags/bda%20chapter%203.html">bda chapter 3</a>, <a href="/tags/solutions.html">solutions</a>, <a href="/tags/bayes.html">bayes</a> </br>
     Category: <a href="/categories/bda3.html">bda3</a> 
    </p>
  </div>
</div>

<p>Here’s my solution to exercise 11, chapter 3, of <a href="https://andrewgelman.com/">Gelman’s</a> <em>Bayesian Data Analysis</em> (BDA), 3rd edition. There are <a href="http://www.stat.columbia.edu/~gelman/book/solutions.pdf">solutions</a> to some of the exercises on the <a href="http://www.stat.columbia.edu/~gelman/book/">book’s webpage</a>.</p>
<!--more-->
<div style="display:none">
<p class="mathjaxWide"><span class="math inline">\(\DeclareMathOperator{\dbinomial}{Binomial} \DeclareMathOperator{\dbern}{Bernoulli} \DeclareMathOperator{\dpois}{Poisson} \DeclareMathOperator{\dnorm}{Normal} \DeclareMathOperator{\dt}{t} \DeclareMathOperator{\dcauchy}{Cauchy} \DeclareMathOperator{\dexponential}{Exp} \DeclareMathOperator{\duniform}{Uniform} \DeclareMathOperator{\dgamma}{Gamma} \DeclareMathOperator{\dinvgamma}{InvGamma} \DeclareMathOperator{\invlogit}{InvLogit} \DeclareMathOperator{\logit}{Logit} \DeclareMathOperator{\ddirichlet}{Dirichlet} \DeclareMathOperator{\dbeta}{Beta}\)</span></p>
</div>
<p>We will analyse <a href="data/chapter_03_exercise_11.csv">the data</a> given in section 3.7 using different priors.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">df &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="st">&#39;data/chapter_03_exercise_11.csv&#39;</span>) </code></pre></div>
<table class="table table-striped table-hover table-responsive" style="margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:right;">
dose_log_g_ml
</th>
<th style="text-align:right;">
animals
</th>
<th style="text-align:right;">
deaths
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
-0.86
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:right;">
-0.30
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
1
</td>
</tr>
<tr>
<td style="text-align:right;">
-0.05
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
3
</td>
</tr>
<tr>
<td style="text-align:right;">
0.73
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
5
</td>
</tr>
</tbody>
</table>
<p>Here is the model specification.</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
  y_i \mid \theta_i 
  &amp;\sim 
  \dbinomial(n_i, \theta_i)
  \\
  \logit(\theta_i)
  &amp;=
  \alpha + \beta x_i
  \\
  \alpha
  &amp;\sim
  \dnorm(0, 2^2)
  \\
  \beta
  &amp;\sim
  \dnorm(10, 10^2)
\end{align}
\]</span></p>
<p>We won’t use a grid approximation to the posterior but instead just use <a href="https://www.rdocumentation.org/packages/rstanarm/versions/2.17.4/topics/stan_glm">Stan</a> because it is a lot simpler.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m &lt;-<span class="st"> </span>rstanarm<span class="op">::</span><span class="kw">stan_glm</span>(
  <span class="kw">cbind</span>(deaths, animals <span class="op">-</span><span class="st"> </span>deaths)  <span class="op">~</span><span class="st"> </span><span class="dv">1</span> <span class="op">+</span><span class="st"> </span>dose_log_g_ml,
  <span class="dt">family =</span> <span class="kw">binomial</span>(<span class="dt">link =</span> logit),
  <span class="dt">data =</span> df,
  <span class="dt">prior_intercept =</span> <span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">2</span>),
  <span class="dt">prior =</span> <span class="kw">normal</span>(<span class="dv">10</span>, <span class="dv">10</span>),
  <span class="dt">warmup =</span> <span class="dv">500</span>,
  <span class="dt">iter =</span> <span class="dv">4000</span>
)

<span class="kw">summary</span>(m)</code></pre></div>
<pre><code>Model Info:

 function:     stan_glm
 family:       binomial [logit]
 formula:      cbind(deaths, animals - deaths) ~ 1 + dose_log_g_ml
 algorithm:    sampling
 priors:       see help(&#39;prior_summary&#39;)
 sample:       14000 (posterior sample size)
 observations: 4
 predictors:   2

Estimates:
                mean   sd   2.5%   25%   50%   75%   97.5%
(Intercept)    1.3    1.0 -0.5    0.6   1.2   1.9   3.4   
dose_log_g_ml 10.9    5.1  3.4    7.1  10.2  13.9  22.7   
mean_PPD       2.3    0.4  1.5    2.0   2.2   2.5   3.2   
log-posterior -5.6    1.1 -8.4   -6.0  -5.2  -4.8  -4.6   

Diagnostics:
              mcse Rhat n_eff
(Intercept)   0.0  1.0   6000
dose_log_g_ml 0.1  1.0   4565
mean_PPD      0.0  1.0  10222
log-posterior 0.0  1.0   3733

For each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).</code></pre>
<p>The <a href="https://mjskay.github.io/tidybayes/articles/tidybayes.html">tidybayes package</a> offers convenient functions for drawing from the posterior. We’ll also add in our <code>LD50</code> estimate.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">draws &lt;-<span class="st"> </span>m <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span>tidybayes<span class="op">::</span><span class="kw">spread_draws</span>(<span class="st">`</span><span class="dt">(Intercept)</span><span class="st">`</span>, dose_log_g_ml) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">rename</span>(
    <span class="dt">alpha =</span> <span class="st">`</span><span class="dt">(Intercept)</span><span class="st">`</span>,
    <span class="dt">beta =</span> dose_log_g_ml
  ) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">LD50 =</span> <span class="op">-</span>alpha <span class="op">/</span><span class="st"> </span>beta)</code></pre></div>
<figure>
<img src="chapter_03_exercise_11_files/figure-markdown/joint_posterior_plot-1..svg" />
</figure>
<p>The estimates look much the same with the more informative priors as with the uninformative priors. The posterior probability that <span class="math inline">\(\beta &gt; 0\)</span> is:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">draws <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">positive =</span> beta <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">summarise</span>(<span class="kw">mean</span>(positive)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">pull</span>() <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">percent</span>()</code></pre></div>
<pre><code>[1] &quot;100%&quot;</code></pre>
<p>The posterior LD50 estimate (conditional on <span class="math inline">\(\beta &gt; 0\)</span>) is as follows:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">draws <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">filter</span>(beta <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">ggplot</span>() <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">aes</span>(LD50) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">bins =</span> <span class="dv">50</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> <span class="dv">0</span>, <span class="dt">linetype =</span> <span class="st">&#39;dashed&#39;</span>, <span class="dt">colour =</span> <span class="st">&#39;chocolate&#39;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(
    <span class="dt">y =</span> <span class="st">&#39;Count&#39;</span>,
    <span class="dt">title =</span> <span class="st">&#39;Histogram of posterior LD50 estimate&#39;</span>,
    <span class="dt">subtitle =</span> <span class="st">&#39;Conditional on β &gt; 0&#39;</span>
  )</code></pre></div>
<figure>
<img src="chapter_03_exercise_11_files/figure-markdown/unnamed-chunk-2-1..svg" />
</figure>

<div id="disqus_thread"></div>
<script>
/**
 *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
 *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
 */
/*
   var disqus_config = function () {
   this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
   this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
   };
 */
(function() {  // DON'T EDIT BELOW THIS LINE
 var d = document, s = d.createElement('script');

 s.src = '//stappit-github-io.disqus.com/embed.js';

 s.setAttribute('data-timestamp', +new Date());
 (d.head || d.body).appendChild(s);
 })();
</script>

<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

<script id="dsq-count-scr" src="//stappit-github-io.disqus.com/count.js" async></script>
]]></summary>
</entry>
<entry>
    <title>BDA3 Chapter 3 Exercise 10</title>
    <link href="http://stappit.github.io/posts/bda3/chapter_03_exercise_10.html" />
    <id>http://stappit.github.io/posts/bda3/chapter_03_exercise_10.html</id>
    <published>2018-10-21T00:00:00Z</published>
    <updated>2018-10-21T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 class="post-title">BDA3 Chapter 3 Exercise 10</h1>

<div class="card post-meta border-0">
  <div class="card-body post-meta">
    <p class="card-text text-muted text-left">
    Posted on October 21, 2018  by Brian </br>
     Tags: <a href="/tags/bda%20chapter%203.html">bda chapter 3</a>, <a href="/tags/solutions.html">solutions</a>, <a href="/tags/bayes.html">bayes</a>, <a href="/tags/f.html">f</a>, <a href="/tags/scaled%20inverse%20chi2.html">scaled inverse chi2</a>, <a href="/tags/inverse%20chi2.html">inverse chi2</a>, <a href="/tags/chi2.html">chi2</a> </br>
     Category: <a href="/categories/bda3.html">bda3</a> 
    </p>
  </div>
</div>

<p>Here’s my solution to exercise 10, chapter 3, of <a href="https://andrewgelman.com/">Gelman’s</a> <em>Bayesian Data Analysis</em> (BDA), 3rd edition. There are <a href="http://www.stat.columbia.edu/~gelman/book/solutions.pdf">solutions</a> to some of the exercises on the <a href="http://www.stat.columbia.edu/~gelman/book/">book’s webpage</a>.</p>
<!--more-->
<div style="display:none">
<p class="mathjaxWide"><span class="math inline">\(\DeclareMathOperator{\dbinomial}{Binomial}  \DeclareMathOperator{\dbern}{Bernoulli}  \DeclareMathOperator{\dpois}{Poisson}  \DeclareMathOperator{\dnorm}{Normal}  \DeclareMathOperator{\dt}{t}  \DeclareMathOperator{\dcauchy}{Cauchy}  \DeclareMathOperator{\dexponential}{Exp}  \DeclareMathOperator{\duniform}{Uniform}  \DeclareMathOperator{\dgamma}{Gamma}  \DeclareMathOperator{\dinvgamma}{InvGamma}  \DeclareMathOperator{\invlogit}{InvLogit}  \DeclareMathOperator{\dinvchi}{InvChi2}  \DeclareMathOperator{\dsinvchi}{SInvChi2}  \DeclareMathOperator{\dchi}{Chi2}  \DeclareMathOperator{\dnorminvchi}{NormInvChi2}  \DeclareMathOperator{\logit}{Logit}  \DeclareMathOperator{\ddirichlet}{Dirichlet}  \DeclareMathOperator{\dbeta}{Beta}\)</span></p>
</div>
<p>For <span class="math inline">\(j = 1, 2\)</span>, let</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
y_j \mid \mu_j \sigma_j^2 
&amp;\sim
\dnorm(\mu_j, \sigma_j^2)
\\
p(\mu_j, \log \sigma_j^2)
&amp;\propto
1.
\end{align}
\]</span></p>
<p>We show that</p>
<p class="mathjaxWide"><span class="math display">\[
\frac{s_1^2 \sigma_2^2}{s_2^2 \sigma_1^2}
\sim
F(n_1 - 1, n_2 - 1)
.
\]</span></p>
<p>Equation 3.5 in the book shows that <span class="math inline">\(\sigma_j^2 \mid y \sim \dinvchi(n_j - 1, s_j^2)\)</span>. It follows that <span class="math inline">\(\frac{\sigma_j^2}{(n_j - 1) s_j^2} \sim \dinvChi(n_j - 1)\)</span>. Thus, <span class="math inline">\(\frac{(n_j - 1) s_j^2}{\sigma_j^2} \sim \dchi(n_j - 1)\)</span>. The result follows from <a href="https://en.wikipedia.org/wiki/F-distribution#Characterization">the fact</a> that the ratio of two <span class="math inline">\(\chi^2\)</span> random variables (divided by the ratio of their degrees of freedom) has an <span class="math inline">\(F\)</span>-distribution.</p>

<div id="disqus_thread"></div>
<script>
/**
 *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
 *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
 */
/*
   var disqus_config = function () {
   this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
   this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
   };
 */
(function() {  // DON'T EDIT BELOW THIS LINE
 var d = document, s = d.createElement('script');

 s.src = '//stappit-github-io.disqus.com/embed.js';

 s.setAttribute('data-timestamp', +new Date());
 (d.head || d.body).appendChild(s);
 })();
</script>

<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

<script id="dsq-count-scr" src="//stappit-github-io.disqus.com/count.js" async></script>
]]></summary>
</entry>
<entry>
    <title>BDA3 Chapter 3 Exercise 9</title>
    <link href="http://stappit.github.io/posts/bda3/chapter_03_exercise_09.html" />
    <id>http://stappit.github.io/posts/bda3/chapter_03_exercise_09.html</id>
    <published>2018-10-21T00:00:00Z</published>
    <updated>2018-10-21T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 class="post-title">BDA3 Chapter 3 Exercise 9</h1>

<div class="card post-meta border-0">
  <div class="card-body post-meta">
    <p class="card-text text-muted text-left">
    Posted on October 21, 2018  by Brian </br>
     Tags: <a href="/tags/bda%20chapter%203.html">bda chapter 3</a>, <a href="/tags/solutions.html">solutions</a>, <a href="/tags/bayes.html">bayes</a>, <a href="/tags/conjugate%20prior.html">conjugate prior</a>, <a href="/tags/normal.html">normal</a>, <a href="/tags/inverse%20chi2.html">inverse chi2</a>, <a href="/tags/normal%20inverse%20chi2.html">normal inverse chi2</a> </br>
     Category: <a href="/categories/bda3.html">bda3</a> 
    </p>
  </div>
</div>

<p>Here’s my solution to exercise 9, chapter 3, of <a href="https://andrewgelman.com/">Gelman’s</a> <em>Bayesian Data Analysis</em> (BDA), 3rd edition. There are <a href="http://www.stat.columbia.edu/~gelman/book/solutions.pdf">solutions</a> to some of the exercises on the <a href="http://www.stat.columbia.edu/~gelman/book/">book’s webpage</a>.</p>
<!--more-->
<div style="display:none">
<p class="mathjaxWide"><span class="math inline">\(\DeclareMathOperator{\dbinomial}{Binomial}  \DeclareMathOperator{\dbern}{Bernoulli}  \DeclareMathOperator{\dpois}{Poisson}  \DeclareMathOperator{\dnorm}{Normal}  \DeclareMathOperator{\dt}{t}  \DeclareMathOperator{\dcauchy}{Cauchy}  \DeclareMathOperator{\dexponential}{Exp}  \DeclareMathOperator{\duniform}{Uniform}  \DeclareMathOperator{\dgamma}{Gamma}  \DeclareMathOperator{\dinvgamma}{InvGamma}  \DeclareMathOperator{\invlogit}{InvLogit}  \DeclareMathOperator{\dinvchi}{InvChi2}  \DeclareMathOperator{\dnorminvchi}{NormInvChi2}  \DeclareMathOperator{\logit}{Logit}  \DeclareMathOperator{\ddirichlet}{Dirichlet}  \DeclareMathOperator{\dbeta}{Beta}\)</span></p>
</div>
<p>Suppose we have a normal likelihood <span class="math inline">\(y \mid \mu, \sigma \sim \dnorm(\mu, \sigma)\)</span> with conjugate priors</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
  \sigma^2 &amp;\sim \dinvchi(\nu_0, \sigma_0^2)
  \\
  \mu \mid \sigma^2 &amp;\sim \dnorm\left(\mu_0, \frac{\sigma^2}{\kappa_0}\right)
  .
\end{align}
\]</span></p>
<p>We need to show that the posterior is</p>
<p class="mathjaxWide"><span class="math display">\[
\mu, \sigma^2 \mid y \sim \dnorminvchi\left(\mu_n, \frac{\sigma_n^2}{\kappa_n}, \nu_n, \sigma_n^2\right)
\]</span></p>
<p>where</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
  \mu_n &amp;= \frac{\kappa_0}{\kappa_0 + n}\mu_0 + \frac{n}{\kappa_0 + n} \bar y
  \\
  \kappa_n &amp;= \kappa_0 + n
  \\
  \nu_n &amp;= \nu_0 + n
  \\
  \nu_n \sigma_n^2 &amp;= \nu_0 \sigma_0^2 + (n - 1) s^2 + \frac{\kappa_0 n}{\kappa_0 + n}(\bar y - \mu_0)^2
  .
\end{align}
\]</span></p>
<p>Using the calculations on pages 67/68, we can compare the factors in front of the exponentials and the exponents of the exponentials, to see that it is sufficient to show that</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
  \frac{1}{\sigma(\sigma^2)^{-(\nu_n / 2 - 1)}}
  &amp;=
  \frac{1}{\sigma (\sigma^2)^{-(\nu_0 / 2 + 1)} (\sigma^2)^{-n / 2}}
  \\
  \nu_n \sigma_n^2 + \kappa_n (\mu_n - \mu)^2
  &amp;=
  \nu_0 \sigma_0^2 + \kappa_0 (\mu - \mu_0)^2 + (n - 1)s^2 + n(\bar y - \mu)^2
.
\end{align}
\]</span></p>
<p>The first identity is straight forward so we focus on the second. We will expand the left hand side and drop any terms we find that match those on the right. Expanding the LHS in terms on the hyperpriors, we get</p>
<p class="mathjaxWide"><span class="math display">\[
\nu_0 \sigma_0^2 + (n - 1) s^2 + \frac{\kappa_0 n}{\kappa_0 + n}(\bar y - \mu_0)^2
+
(\kappa_0 + n) \left(\frac{\kappa_0}{\kappa_0 + n}\mu_0 + \frac{n}{\kappa_0 + n} \bar y- \mu\right)^2
-
\text{RHS}
\\
=
\frac{\kappa_0 n}{\kappa_0 + n}(\bar y - \mu_0)^2
+
(\kappa_0 + n) \left(\frac{\kappa_0}{\kappa_0 + n}\mu_0 + \frac{n}{\kappa_0 + n} \bar y- \mu\right)^2
-
\kappa_0 (\mu - \mu_0)^2 - n(\bar y - \mu)^2
.
\]</span></p>
<p>Moving the <span class="math inline">\(\kappa_0 + n\)</span> denominator of the second term out of the brackets we obtain</p>
<p class="mathjaxWide"><span class="math display">\[
\frac{\kappa_0 n}{\kappa_0 + n}(\bar y - \mu_0)^2
+
\frac{1}{(\kappa_0 + n)} \left(\kappa_0\mu_0 + n \bar y- (\kappa_0 + n)\mu\right)^2
-
\kappa_0 (\mu - \mu_0)^2 - n(\bar y - \mu)^2
.
\]</span></p>
<p>Simplifying and multiplying out the brackets of the second term gives</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
  \left(\kappa_0\mu_0 + n \bar y- (\kappa_0 + n)\mu\right)^2
  &amp;=
  \left( \kappa_0 (\mu_0 - \bar y) + (\kappa_0 + n)(\bar y - \mu) \right)^2
  \\
  &amp;=
  \kappa_0^2 (\bar y - \mu_0)^2 + (\kappa_0 + n)^2 (\bar y - \mu)^2 + 2\kappa_0(\kappa_0 + n)(\bar y - \mu)(\mu_0 - \bar y)
. 
\end{align}
\]</span></p>
<p>Substituting this back in, we can combine the first terms of each and multiply out all the brackets to get</p>
<span class="math display">\[\begin{align}
  \frac{\kappa_0 n}{\kappa_0 + n}(\bar y - \mu_0)^2
  +
  \frac{1}{\kappa_0 + n} 
  \left( 
    \kappa_0^2 (\bar y - \mu_0)^2 
    + 
    (\kappa_0 + n)^2 (\bar y - \mu)^2 
    + 
    2\kappa_0 (\kappa_0 + n) (\bar y - \mu) (\mu_0 - \bar y) 
  \right)
  \\
  -
  \kappa_0 (\mu - \mu_0)^2 
  - 
  n(\bar y - \mu)^2
  
  \\
  =
  
  \kappa_0 (\bar y - \mu_0)^2 
  + 
  (\kappa_0 + n) (\bar y - \mu)^2 
  + 
  2\kappa_0 (\bar y - \mu) (\mu_0 - \bar y) 
  \\
  -
  \kappa_0 (\mu - \mu_0)^2 
  - 
  n(\bar y - \mu)^2
  
  \\
  =
  
  \color{red}{ \kappa_0 \bar y^2  }
  + 
  \color{blue}{\kappa_0 \mu_0^2}
  - 
  \color{green}{2\kappa_0 \mu_0 \bar y}
  +
  \color{red}{\kappa_0 \bar y^2}
  +
  \color{orange}{\kappa_0 \mu^2}
  -
  \color{black}{2 \kappa_0 \mu \bar y}
  +
  \color{green}{2 \kappa_0 \mu_0 \bar y}
  \\
  +
  \color{black}{2 \kappa_0 \mu \bar y}
  -
  \color{purple}{2 \kappa_0 \mu_0 \mu}
  -
  \color{red}{2 \kappa_0 \bar y^2}
  -
  \color{orange}{\kappa_0 \mu^2}
  -
  \color{blue}{\kappa_0 \mu_0^2}
  +
  \color{purple}{2 \kappa_0 \mu_0 \mu}
  ,
\end{align}\]</span>
<p>which cancel to 0.</p>

<div id="disqus_thread"></div>
<script>
/**
 *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
 *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
 */
/*
   var disqus_config = function () {
   this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
   this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
   };
 */
(function() {  // DON'T EDIT BELOW THIS LINE
 var d = document, s = d.createElement('script');

 s.src = '//stappit-github-io.disqus.com/embed.js';

 s.setAttribute('data-timestamp', +new Date());
 (d.head || d.body).appendChild(s);
 })();
</script>

<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

<script id="dsq-count-scr" src="//stappit-github-io.disqus.com/count.js" async></script>
]]></summary>
</entry>
<entry>
    <title>BDA3 Chapter 3 Exercise 8</title>
    <link href="http://stappit.github.io/posts/bda3/chapter_03_exercise_08.html" />
    <id>http://stappit.github.io/posts/bda3/chapter_03_exercise_08.html</id>
    <published>2018-10-21T00:00:00Z</published>
    <updated>2018-10-21T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 class="post-title">BDA3 Chapter 3 Exercise 8</h1>

<div class="card post-meta border-0">
  <div class="card-body post-meta">
    <p class="card-text text-muted text-left">
    Posted on October 21, 2018  by Brian </br>
     Tags: <a href="/tags/bda%20chapter%203.html">bda chapter 3</a>, <a href="/tags/solutions.html">solutions</a>, <a href="/tags/bayes.html">bayes</a>, <a href="/tags/poisson.html">poisson</a> </br>
     Category: <a href="/categories/bda3.html">bda3</a> 
    </p>
  </div>
</div>

<p>Here’s my solution to exercise 8, chapter 3, of <a href="https://andrewgelman.com/">Gelman’s</a> <em>Bayesian Data Analysis</em> (BDA), 3rd edition. There are <a href="http://www.stat.columbia.edu/~gelman/book/solutions.pdf">solutions</a> to some of the exercises on the <a href="http://www.stat.columbia.edu/~gelman/book/">book’s webpage</a>.</p>
<!--more-->
<div style="display:none">
<p class="mathjaxWide"><span class="math inline">\(\DeclareMathOperator{\dbinomial}{Binomial} \DeclareMathOperator{\dbern}{Bernoulli} \DeclareMathOperator{\dpois}{Poisson} \DeclareMathOperator{\dnorm}{Normal} \DeclareMathOperator{\dt}{t} \DeclareMathOperator{\dcauchy}{Cauchy} \DeclareMathOperator{\dexponential}{Exp} \DeclareMathOperator{\duniform}{Uniform} \DeclareMathOperator{\dgamma}{Gamma} \DeclareMathOperator{\dinvgamma}{InvGamma} \DeclareMathOperator{\invlogit}{InvLogit} \DeclareMathOperator{\logit}{Logit} \DeclareMathOperator{\ddirichlet}{Dirichlet} \DeclareMathOperator{\dbeta}{Beta}\)</span></p>
</div>
<p>You can download the <a href="data/chapter_03_exercise_08.csv">full dataset shown in table 3.3</a>. Let’s load it into a dataframe and select just the residential data, as suggested.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">df0 &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="st">&#39;data/chapter_03_exercise_08.csv&#39;</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(
    <span class="dt">type =</span> <span class="kw">as_factor</span>(
      type, 
      <span class="dt">levels =</span> <span class="kw">c</span>(<span class="st">&#39;residential&#39;</span>, <span class="st">&#39;fairly_busy&#39;</span>, <span class="st">&#39;busy&#39;</span>), 
      <span class="dt">ordered =</span> <span class="ot">TRUE</span>
    ),
    <span class="dt">bikes =</span> <span class="kw">as.integer</span>(bikes),
    <span class="dt">other =</span> <span class="kw">as.integer</span>(other)
  )

df &lt;-<span class="st"> </span>df0 <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">filter</span>(type <span class="op">==</span><span class="st"> &#39;residential&#39;</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(
    <span class="dt">total =</span> bikes <span class="op">+</span><span class="st"> </span>other,
    <span class="dt">bike_fraction =</span> bikes <span class="op">/</span><span class="st"> </span>total,
    <span class="dt">other_fraction =</span> other <span class="op">/</span><span class="st"> </span>total
  )</code></pre></div>
<p>Here are the first few rows with each value of <code>bike_route</code>.</p>
<table class="table table-hover table-striped table-responsive" style="margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
type
</th>
<th style="text-align:left;">
bike_route
</th>
<th style="text-align:right;">
bikes
</th>
<th style="text-align:right;">
other
</th>
<th style="text-align:right;">
total
</th>
<th style="text-align:right;">
bike_fraction
</th>
<th style="text-align:right;">
other_fraction
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
residential
</td>
<td style="text-align:left;">
TRUE
</td>
<td style="text-align:right;">
16
</td>
<td style="text-align:right;">
58
</td>
<td style="text-align:right;">
74
</td>
<td style="text-align:right;">
0.2162162
</td>
<td style="text-align:right;">
0.7837838
</td>
</tr>
<tr>
<td style="text-align:left;">
residential
</td>
<td style="text-align:left;">
TRUE
</td>
<td style="text-align:right;">
9
</td>
<td style="text-align:right;">
90
</td>
<td style="text-align:right;">
99
</td>
<td style="text-align:right;">
0.0909091
</td>
<td style="text-align:right;">
0.9090909
</td>
</tr>
<tr>
<td style="text-align:left;">
residential
</td>
<td style="text-align:left;">
TRUE
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
48
</td>
<td style="text-align:right;">
58
</td>
<td style="text-align:right;">
0.1724138
</td>
<td style="text-align:right;">
0.8275862
</td>
</tr>
<tr>
<td style="text-align:left;">
residential
</td>
<td style="text-align:left;">
FALSE
</td>
<td style="text-align:right;">
12
</td>
<td style="text-align:right;">
113
</td>
<td style="text-align:right;">
125
</td>
<td style="text-align:right;">
0.0960000
</td>
<td style="text-align:right;">
0.9040000
</td>
</tr>
<tr>
<td style="text-align:left;">
residential
</td>
<td style="text-align:left;">
FALSE
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
18
</td>
<td style="text-align:right;">
19
</td>
<td style="text-align:right;">
0.0526316
</td>
<td style="text-align:right;">
0.9473684
</td>
</tr>
<tr>
<td style="text-align:left;">
residential
</td>
<td style="text-align:left;">
FALSE
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
14
</td>
<td style="text-align:right;">
16
</td>
<td style="text-align:right;">
0.1250000
</td>
<td style="text-align:right;">
0.8750000
</td>
</tr>
</tbody>
</table>
<p>We’ll use an uninformative gamma prior with a Poisson likelihood for the counts. The posterior can then be calculated as follows.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">draws &lt;-<span class="st"> </span><span class="dv">10000</span>

shape_prior &lt;-<span class="st"> </span><span class="dv">2</span>
rate_prior &lt;-<span class="st"> </span><span class="dv">0</span>

posterior &lt;-<span class="st"> </span><span class="cf">function</span>(data, <span class="dt">draws =</span> <span class="dv">10000</span>) {
  
  bikes &lt;-<span class="st"> </span>data <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">pull</span>(bikes)
  other &lt;-<span class="st"> </span>data <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">pull</span>(other)
  n &lt;-<span class="st"> </span>data <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">pull</span>(n)
  
  <span class="kw">tibble</span>(<span class="dt">draw =</span> <span class="dv">1</span><span class="op">:</span>draws) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">mutate</span>(
      <span class="dt">theta_bike =</span> <span class="kw">rgamma</span>(draws, bikes, n),
      <span class="dt">theta_other =</span> <span class="kw">rgamma</span>(draws, other, n),
      <span class="dt">mu =</span> <span class="kw">rpois</span>(draws, theta_bike),
      <span class="dt">p =</span> theta_bike <span class="op">/</span><span class="st"> </span>(theta_bike <span class="op">+</span><span class="st"> </span>theta_other)
    )
  
}

posterior_draws &lt;-<span class="st"> </span>df <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(bike_route) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">summarise</span>(
    <span class="dt">bikes =</span> <span class="kw">sum</span>(bikes) <span class="op">+</span><span class="st"> </span>shape_prior,
    <span class="dt">other =</span> <span class="kw">sum</span>(other) <span class="op">+</span><span class="st"> </span>shape_prior,
    <span class="dt">n =</span> <span class="kw">n</span>() <span class="op">+</span><span class="st"> </span>rate_prior
  ) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">nest</span>(<span class="op">-</span>bike_route) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">draws =</span> <span class="kw">map</span>(data, posterior, draws)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">unnest</span>(draws)</code></pre></div>
<p>Plotting posterior predictive draws of <span class="math inline">\(\theta_y\)</span> and <span class="math inline">\(\theta_z\)</span>, we can see that there seems to be quite a difference.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">posterior_draws <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">ggplot</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">aes</span>(mu, <span class="dt">fill =</span> bike_route) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_bar</span>(<span class="dt">position =</span> <span class="st">&#39;identity&#39;</span>, <span class="dt">alpha =</span> <span class="fl">0.75</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(
    <span class="dt">x =</span> <span class="st">&#39;Bike count&#39;</span>,
    <span class="dt">y =</span> <span class="st">&#39;Count&#39;</span>,
    <span class="dt">fill =</span> <span class="st">&#39;Has bike route?&#39;</span>,
    <span class="dt">title =</span> <span class="st">&#39;Posterior expectation of bike count&#39;</span>
  )</code></pre></div>
<figure>
<img src="chapter_03_exercise_08_files/figure-markdown/posterior_predictive_plot-1..svg" />
</figure>
<p>To quantify this difference, we’ll have to match up our posterior draws for <span class="math inline">\(\theta_y\)</span> and <span class="math inline">\(\theta_z\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">difference &lt;-<span class="st"> </span>posterior_draws <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">select</span>(draw, bike_route, mu) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">spread</span>(bike_route, mu) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">difference =</span> <span class="st">`</span><span class="dt">TRUE</span><span class="st">`</span> <span class="op">-</span><span class="st"> `</span><span class="dt">FALSE</span><span class="st">`</span>) </code></pre></div>
<figure>
<img src="chapter_03_exercise_08_files/figure-markdown/difference_plot-1..svg" />
</figure>
<p>The difference <span class="math inline">\(\mu_y - \mu_z\)</span> has the following 95% credible interval:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">difference <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">pull</span>(difference) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">quantile</span>(<span class="dt">probs =</span> <span class="kw">c</span>(<span class="fl">0.05</span>, <span class="fl">0.5</span>, <span class="fl">0.95</span>))</code></pre></div>
<pre><code> 5% 50% 95% 
  6  15  24</code></pre>

<div id="disqus_thread"></div>
<script>
/**
 *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
 *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
 */
/*
   var disqus_config = function () {
   this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
   this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
   };
 */
(function() {  // DON'T EDIT BELOW THIS LINE
 var d = document, s = d.createElement('script');

 s.src = '//stappit-github-io.disqus.com/embed.js';

 s.setAttribute('data-timestamp', +new Date());
 (d.head || d.body).appendChild(s);
 })();
</script>

<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

<script id="dsq-count-scr" src="//stappit-github-io.disqus.com/count.js" async></script>
]]></summary>
</entry>
<entry>
    <title>BDA3 Chapter 3 Exercise 7</title>
    <link href="http://stappit.github.io/posts/bda3/chapter_03_exercise_07.html" />
    <id>http://stappit.github.io/posts/bda3/chapter_03_exercise_07.html</id>
    <published>2018-10-21T00:00:00Z</published>
    <updated>2018-10-21T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 class="post-title">BDA3 Chapter 3 Exercise 7</h1>

<div class="card post-meta border-0">
  <div class="card-body post-meta">
    <p class="card-text text-muted text-left">
    Posted on October 21, 2018  by Brian </br>
     Tags: <a href="/tags/bda%20chapter%203.html">bda chapter 3</a>, <a href="/tags/solutions.html">solutions</a>, <a href="/tags/bayes.html">bayes</a>, <a href="/tags/poisson.html">poisson</a>, <a href="/tags/binomial.html">binomial</a>, <a href="/tags/unsolved.html">unsolved</a> </br>
     Category: <a href="/categories/bda3.html">bda3</a> 
    </p>
  </div>
</div>

<p>Here’s my solution to exercise 7, chapter 3, of <a href="https://andrewgelman.com/">Gelman’s</a> <em>Bayesian Data Analysis</em> (BDA), 3rd edition. There are <a href="http://www.stat.columbia.edu/~gelman/book/solutions.pdf">solutions</a> to some of the exercises on the <a href="http://www.stat.columbia.edu/~gelman/book/">book’s webpage</a>.</p>
<!--more-->
<div style="display:none">
<p class="mathjaxWide"><span class="math inline">\(\DeclareMathOperator{\dbinomial}{Binomial} \DeclareMathOperator{\dbern}{Bernoulli} \DeclareMathOperator{\dpois}{Poisson} \DeclareMathOperator{\dnorm}{Normal} \DeclareMathOperator{\dt}{t} \DeclareMathOperator{\dcauchy}{Cauchy} \DeclareMathOperator{\dexponential}{Exp} \DeclareMathOperator{\duniform}{Uniform} \DeclareMathOperator{\dgamma}{Gamma} \DeclareMathOperator{\dinvgamma}{InvGamma} \DeclareMathOperator{\invlogit}{InvLogit} \DeclareMathOperator{\logit}{Logit} \DeclareMathOperator{\ddirichlet}{Dirichlet} \DeclareMathOperator{\dbeta}{Beta}\)</span></p>
</div>
<p>Suppose we observe <span class="math inline">\(b\)</span> bikes and <span class="math inline">\(v\)</span> other vehicles passing a section of road within an hour. We can model the counts as Poisson distributed</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
  b \mid \theta_b &amp;\sim \dpois(\theta_b)
  \\
  v \mid \theta_v &amp;\sim \dpois(\theta_v)
\end{align}
\]</span></p>
<p>or as binomial distributed</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
  b \mid n, p &amp;\sim \dbinomial(n, p)
\end{align}
\]</span></p>
<p>where <span class="math inline">\(n\)</span> is the number of trials and <span class="math inline">\(p\)</span> is the probability of observing a bike. Let</p>
<p class="mathjaxWide"><span class="math display">\[
p := \frac{\theta_b}{\theta_b + \theta_v}
.
\]</span></p>
<p>We are supposed to show that this definition of <span class="math inline">\(p\)</span> gives the two models the same likelihood, but I’m stuck. At best I can show that the expectations are different</p>
<p class="mathjaxWide"><span class="math display">\[
\mathbb E (b \mid \theta_b) = \theta_b
\\
\mathbb E (b \mid n, p) = np = n\frac{\theta_b}{\theta_b + \theta_v}
\]</span></p>
<p>which suggests the conditioning should be done differently.</p>

<div id="disqus_thread"></div>
<script>
/**
 *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
 *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
 */
/*
   var disqus_config = function () {
   this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
   this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
   };
 */
(function() {  // DON'T EDIT BELOW THIS LINE
 var d = document, s = d.createElement('script');

 s.src = '//stappit-github-io.disqus.com/embed.js';

 s.setAttribute('data-timestamp', +new Date());
 (d.head || d.body).appendChild(s);
 })();
</script>

<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

<script id="dsq-count-scr" src="//stappit-github-io.disqus.com/count.js" async></script>
]]></summary>
</entry>

</feed>
