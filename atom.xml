<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>Thoughts from the Café</title>
    <link href="http://stappit.github.io/atom.xml" rel="self" />
    <link href="http://stappit.github.io" />
    <id>http://stappit.github.io/atom.xml</id>
    <author>
        <name>Brian</name>
        <email>ha@hahaha.com</email>
    </author>
    <updated>2018-09-01T00:00:00Z</updated>
    <entry>
    <title>BDA3 Chapter 2 Exercise 11</title>
    <link href="http://stappit.github.io/posts/bda3/chapter_02_exercise_11.html" />
    <id>http://stappit.github.io/posts/bda3/chapter_02_exercise_11.html</id>
    <published>2018-09-01T00:00:00Z</published>
    <updated>2018-09-01T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 class="post-title">BDA3 Chapter 2 Exercise 11</h1>

<div class="card post-meta border-0">
  <div class="card-body post-meta">
    <p class="card-text text-muted text-left">
    Posted on September  1, 2018  by Brian </br>
     Tags: <a href="/tags/bda%20chapter%202.html">bda chapter 2</a>, <a href="/tags/solutions.html">solutions</a>, <a href="/tags/bayes.html">bayes</a>, <a href="/tags/cauchy.html">cauchy</a>, <a href="/tags/posterior%20predictive.html">posterior predictive</a>, <a href="/tags/grid%0Aapproximation.html">grid
approximation</a> </br>
     Category: <a href="/categories/bda3.html">bda3</a> 
    </p>
  </div>
</div>

<p>Here’s my solution to exercise 11, chapter 2, of <a href="https://andrewgelman.com/">Gelman’s</a> <em>Bayesian Data Analysis</em> (BDA), 3rd edition. There are <a href="http://www.stat.columbia.edu/~gelman/book/solutions.pdf">solutions</a> to some of the exercises on the <a href="http://www.stat.columbia.edu/~gelman/book/">book’s webpage</a>.</p>
<!--more-->
<div style="display:none">
<p class="mathjaxWide"><span class="math inline">\(\DeclareMathOperator{\dbinomial}{binomial} \DeclareMathOperator{\dbern}{Bernoulli} \DeclareMathOperator{\dnorm}{normal} \DeclareMathOperator{\dcauchy}{Cauchy} \DeclareMathOperator{\dgamma}{gamma} \DeclareMathOperator{\invlogit}{invlogit} \DeclareMathOperator{\logit}{logit} \DeclareMathOperator{\dbeta}{beta}\)</span></p>
</div>
<p>Assume the sampling distribution is <span class="math inline">\(\dcauchy(y \mid \theta, 1)\)</span> with uniform prior <span class="math inline">\(p(\theta) \propto 1\)</span> on <span class="math inline">\([0, 100]\)</span>. Given observations <span class="math inline">\(y\)</span>, we can approximate the posterior for <span class="math inline">\(\theta\)</span> by dividing the interval <span class="math inline">\([0, 100]\)</span> into partitions of length <span class="math inline">\(\frac{1}{m}\)</span>. The unnormalised posterior for <span class="math inline">\(\theta\)</span> on this grid is then computed as follows.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># observations</span>
y &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">43</span>, <span class="dv">44</span>, <span class="dv">45</span>, <span class="fl">46.5</span>, <span class="fl">47.5</span>) 

<span class="co"># grid granularity</span>
m &lt;-<span class="st"> </span><span class="dv">100</span>

<span class="co"># L(θ) := p(y | θ)</span>
likelihood &lt;-<span class="st"> </span><span class="cf">function</span>(theta) 
  y <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">map</span>(dcauchy, theta, <span class="dv">1</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">reduce</span>(prod)

<span class="co"># unnormalised posterior grid</span>
posterior_unnorm &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">theta =</span> <span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">100</span>, <span class="dv">1</span> <span class="op">/</span><span class="st"> </span>m)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">density =</span> <span class="kw">map</span>(theta, likelihood) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">unlist</span>())</code></pre></div>
<p>We can approximate the normalising constant by summing the approximate area on each partition. Each partition has width <span class="math inline">\(\frac{1}{m}\)</span> and approximate height given by the density, so the approximate area is the multiple of the two.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># grid approx to area under curve</span>
normalising_constant &lt;-<span class="st"> </span>posterior_unnorm <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">summarise</span>(<span class="kw">sum</span>(density) <span class="op">/</span><span class="st"> </span>m) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">pull</span>()

<span class="co"># normalised posterior grid</span>
posterior &lt;-<span class="st"> </span>posterior_unnorm <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">density =</span> density <span class="op">/</span><span class="st"> </span>normalising_constant)

normalising_constant</code></pre></div>
<pre><code>[1] 3.418359e-05</code></pre>
<figure>
<img src="chapter_02_exercise_11_files/figure-markdown/posterior_plot-1.png" />
</figure>
<p>Let’s zoom in on the region <span class="math inline">\([40, 50]\)</span> where most of the density lies.</p>
<figure>
<img src="chapter_02_exercise_11_files/figure-markdown/posterior_plot_zoomed-1.png" />
</figure>
<p>Sampling from this posterior yields a histogram with a similar shape.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">posterior_draws &lt;-<span class="st"> </span>posterior <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">sample_n</span>(<span class="dv">1000</span>, <span class="dt">replace =</span> <span class="ot">TRUE</span>, <span class="dt">weight =</span> density) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">select</span>(theta)</code></pre></div>
<figure>
<img src="chapter_02_exercise_11_files/figure-markdown/posterior_sample_plot-1.png" />
</figure>
<p>We can draw from the posterior predictive distribution by first drawing <span class="math inline">\(\tilde\theta\)</span> from the posterior of <span class="math inline">\(\theta\)</span>, then drawing <span class="math inline">\(\tilde y\)</span> from <span class="math inline">\(\dcauchy(\tilde\theta, 1)\)</span>. The tails of the posterior predictive distribution are much wider than for <span class="math inline">\(\theta\)</span> so we plot this histogram on the interval <span class="math inline">\([10, 90]\)</span> (although there are a few observations outside this interval).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">posterior_predictive &lt;-<span class="st"> </span>posterior_draws <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">pp =</span> <span class="kw">rcauchy</span>(<span class="kw">n</span>(), theta, <span class="dv">1</span>)) </code></pre></div>
<figure>
<img src="chapter_02_exercise_11_files/figure-markdown/posterior_predictive_plot-1.png" />
</figure>

<div id="disqus_thread"></div>
<script>
/**
 *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
 *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
 */
/*
   var disqus_config = function () {
   this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
   this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
   };
 */
(function() {  // DON'T EDIT BELOW THIS LINE
 var d = document, s = d.createElement('script');

 s.src = '//stappit-github-io.disqus.com/embed.js';

 s.setAttribute('data-timestamp', +new Date());
 (d.head || d.body).appendChild(s);
 })();
</script>

<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

<script id="dsq-count-scr" src="//stappit-github-io.disqus.com/count.js" async></script>
]]></summary>
</entry>
<entry>
    <title>BDA3 Chapter 2 Exercise 10</title>
    <link href="http://stappit.github.io/posts/bda3/chapter_02_exercise_10.html" />
    <id>http://stappit.github.io/posts/bda3/chapter_02_exercise_10.html</id>
    <published>2018-09-01T00:00:00Z</published>
    <updated>2018-09-01T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 class="post-title">BDA3 Chapter 2 Exercise 10</h1>

<div class="card post-meta border-0">
  <div class="card-body post-meta">
    <p class="card-text text-muted text-left">
    Posted on September  1, 2018  by Brian </br>
     Tags: <a href="/tags/bda%20chapter%202.html">bda chapter 2</a>, <a href="/tags/solutions.html">solutions</a>, <a href="/tags/bayes.html">bayes</a>, <a href="/tags/geometric.html">geometric</a>, <a href="/tags/noninformative%20prior.html">noninformative prior</a>, <a href="/tags/jeffrey%20prior.html">jeffrey prior</a> </br>
     Category: <a href="/categories/bda3.html">bda3</a> 
    </p>
  </div>
</div>

<p>Here’s my solution to exercise 10, chapter 2, of <a href="https://andrewgelman.com/">Gelman’s</a> <em>Bayesian Data Analysis</em> (BDA), 3rd edition. There are <a href="http://www.stat.columbia.edu/~gelman/book/solutions.pdf">solutions</a> to some of the exercises on the <a href="http://www.stat.columbia.edu/~gelman/book/">book’s webpage</a>.</p>
<!--more-->
<div style="display:none">
<p class="mathjaxWide"><span class="math inline">\(\DeclareMathOperator{\dbinomial}{binomial} \DeclareMathOperator{\dbern}{Bernoulli} \DeclareMathOperator{\dnorm}{normal} \DeclareMathOperator{\dgamma}{gamma} \DeclareMathOperator{\invlogit}{invlogit} \DeclareMathOperator{\logit}{logit} \DeclareMathOperator{\dbeta}{beta}\)</span></p>
</div>
<h2 id="the-posterior-density">The posterior density</h2>
<p>There an N cars labelled 1 to N and we observe a random car labelled 203. With a geometric prior with mean 100, the unnormalised posterior is</p>
<p class="mathjaxWide"><span class="math display">\[
p(N \mid y = 203)
\propto
p(y = 203 \mid N) \cdot p (N)
= 
\left.
  \begin{cases}
    \frac{1}{N} \cdot \frac{1}{100} \cdot \left( \frac{99}{100} \right)^{N - 1} 
    &amp; 
    \text{for } 203 \le N 
    \\
    0
    &amp;
    \text{otherwise}
  \end{cases}
\right\}
\]</span></p>
<p>We find the normalising constant in two steps. First set <span class="math inline">\(x := \frac{99}{100}\)</span> and use the <a href="https://en.wikipedia.org/wiki/Taylor_series#Natural_logarithm">Taylor series</a> of <span class="math inline">\(\log(1 - x)\)</span> to show</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
  \sum_1^\infty \frac{1}{N} \cdot \frac{1}{100}\cdot \left( \frac{99}{100} \right)^{N - 1}
  &amp;=
  \frac{1}{100} \left( 1 + \frac{x}{2} + \frac{x^2}{3} + \dotsc + \frac{x^k}{k + 1} + \dotsc \right)
  \\
  &amp;=
  \frac{1}{100} \frac{1}{x} \left(  x + \frac{x^2}{2} + \frac{x^3}{3} + \dotsc + \frac{x^k}{k} + \dotsc   \right)
  \\
  &amp;=
  -\frac{1}{100} \frac{1}{x} \log (1 - x)
  \\
  &amp;=
  \frac{\log 100}{99} 
  .
\end{align}
\]</span></p>
<p>The normalising constant <span class="math inline">\(c\)</span> is then</p>
<p class="mathjaxWide"><span class="math display">\[
  c
  =
  \frac{\log 100}{99} 
  -
  \sum_1^{202} \frac{1}{N} \cdot \frac{1}{100}\cdot \left( \frac{99}{100} \right)^{N - 1}
\]</span></p>
<p>which we approximate with the following computation.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># value of the Nth term in the sum</span>
term &lt;-<span class="st"> </span><span class="cf">function</span>(N) 
  (<span class="dv">1</span> <span class="op">/</span><span class="st"> </span>N) <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">/</span><span class="st"> </span><span class="dv">100</span>) <span class="op">*</span><span class="st"> </span>(<span class="dv">99</span> <span class="op">/</span><span class="st"> </span><span class="dv">100</span>)<span class="op">^</span>(N <span class="op">-</span><span class="st"> </span><span class="dv">1</span>) 

<span class="co"># left hand side</span>
c0 &lt;-<span class="st"> </span><span class="kw">log</span>(<span class="dv">100</span>) <span class="op">/</span><span class="st"> </span><span class="dv">99</span>

<span class="co"># right hand side (the sum)</span>
c1 &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="dv">202</span> <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">map</span>(term) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">reduce</span>(sum)
  
c &lt;-<span class="st"> </span>c0 <span class="op">-</span><span class="st"> </span>c1

c</code></pre></div>
<pre><code>[1] 0.0004705084</code></pre>
<h2 id="the-posterior-moments">The posterior moments</h2>
<p>The posterior mean is</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
  \mathbb E(N \mid y = 203)
  &amp;=
  \frac{1}{c}\sum_{203}^\infty \frac{N}{N} \cdot \frac{1}{100} \cdot \left( \frac{99}{100} \right)^{N - 1}
  \\
  &amp;=
  \frac{1}{100c} \left( x^{202} + x^{203} + \dotsc \right), \qquad x := \frac{99}{100}
  \\
  &amp;=
  \frac{1}{100c} \left( \frac{1}{1 - x} - (1 + x + x^2 + \dotsc + x^{201}) \right)
  \\
  &amp;=
  \frac{1}{100c} \left( \frac{1}{1 - x} - \frac{1 - x^{202}}{1 - x} \right)
  \\
  &amp;=
  \frac{1}{100c} \frac{x^{202}}{1 - x}
  \\
  &amp;=
  \frac{1}{c}\left( \frac{99}{100} \right)^{202}
\end{align}
,
\]</span></p>
<p>which is approximately</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mu &lt;-<span class="st"> </span>(<span class="dv">1</span> <span class="op">/</span><span class="st"> </span>c) <span class="op">*</span><span class="st"> </span>(<span class="dv">99</span> <span class="op">/</span><span class="st"> </span><span class="dv">100</span>)<span class="op">^</span><span class="dv">202</span>
mu</code></pre></div>
<pre><code>[1] 279.0885</code></pre>
<p>This is larger than the prior mean of 100.</p>
<p>The second moment is</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
  \mathbb E(N^2 \mid y = 203)
  &amp;=
  \frac{1}{c}\sum_{203}^\infty \frac{N^2}{N} \cdot \frac{1}{100} \cdot \left( \frac{99}{100} \right)^{N - 1}
  \\
  &amp;=
  \frac{1}{100c} \left( 203x^{202} + 204x^{203} + \dotsc \right), \qquad x := \frac{99}{100}
  \\
  &amp;=
  \frac{1}{100c} \left( \frac{1}{(1 - x)^2} - \left(1 + 2x + 3x^2 + \dotsc + 202 x^{201} \right) \right)
  \\
  &amp;=
  \frac{1}{100c} \left( 100^2 - \left(1 + 2x + 3x^2 + \dotsc + 202 x^{201} \right) \right)
  \\
  &amp;=
  \frac{100}{c}  - \frac{1 + 2x + 3x^2 + \dotsc + 202 x^{201} }{100c}
\end{align}
,
\]</span></p>
<p>which we approximate with the following code.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">EN2_left &lt;-<span class="st"> </span><span class="dv">100</span> <span class="op">/</span><span class="st"> </span>c

EN2_right &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="dv">201</span> <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">map</span>(<span class="cf">function</span>(N) N <span class="op">*</span><span class="st"> </span>(<span class="dv">99</span> <span class="op">/</span><span class="st"> </span><span class="dv">100</span>)<span class="op">^</span>(N <span class="op">-</span><span class="st"> </span><span class="dv">1</span>)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">reduce</span>(sum) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  `</span><span class="dt">/</span><span class="st">`</span>(<span class="dv">100</span> <span class="op">*</span><span class="st"> </span>c)

EN2 &lt;-<span class="st"> </span>EN2_left <span class="op">-</span><span class="st"> </span>EN2_right

EN2</code></pre></div>
<pre><code>[1] 84854.18</code></pre>
<p>It follows that the posterior variance and standard deviation is approximately</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">v &lt;-<span class="st"> </span>EN2 <span class="op">-</span><span class="st"> </span>mu<span class="op">^</span><span class="dv">2</span>
sigma &lt;-<span class="st"> </span><span class="kw">sqrt</span>(v)
<span class="kw">c</span>(v, sigma)</code></pre></div>
<pre><code>[1] 6963.78665   83.44931</code></pre>
<p>These are smaller than the prior variance and standard deviation, respectively:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">v_prior &lt;-<span class="st"> </span>(<span class="dv">99</span> <span class="op">/</span><span class="st"> </span><span class="dv">100</span>) <span class="op">/</span><span class="st"> </span>(<span class="fl">0.01</span><span class="op">^</span><span class="dv">2</span>)
sigma_prior &lt;-<span class="st"> </span><span class="kw">sqrt</span>(v_prior)
<span class="kw">c</span>(v_prior, sigma_prior)</code></pre></div>
<pre><code>[1] 9900.00000   99.49874</code></pre>
<h2 id="a-non-informative-prior">A non-informative prior</h2>
<p>There is no proper uniform density over the positive integers. A uniform prior also leaves us with an improper posterior.</p>
<p>Jeffrey’s prior is <span class="math inline">\(p(N) \propto \frac{1}{N}\)</span>, which is also improper. However, it yields the following (unnormalised) posterior</p>
<p class="mathjaxWide"><span class="math display">\[
p(N \mid y = 203)
\propto
\left.
  \begin{cases}
    \frac{1}{N^2} 
    &amp; 
    \text{for } 203 \le N 
    \\
    0
    &amp;
    \text{otherwise}
  \end{cases}
\right\}
\]</span></p>
<p>which is proper.</p>
<p>Using the <a href="https://en.wikipedia.org/wiki/Basel_problem">Basel problem</a> we can calculate the normalising constant</p>
<p class="mathjaxWide"><span class="math display">\[
c
=
\sum_1^\infty \frac{1}{N^2}
-
\sum_1^{202} \frac{1}{N^2}
=
\frac{\pi^2}{6}
-
\sum_1^{202} \frac{1}{N^2}
\]</span></p>
<p>which is approximately</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">c_left &lt;-<span class="st"> </span>pi<span class="op">^</span><span class="dv">2</span> <span class="op">/</span><span class="st"> </span><span class="dv">6</span>

c_right &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="dv">202</span> <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">map</span>(<span class="cf">function</span>(N) <span class="dv">1</span> <span class="op">/</span><span class="st"> </span>N<span class="op">^</span><span class="dv">2</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">reduce</span>(sum)

c &lt;-<span class="st"> </span>c_left <span class="op">-</span><span class="st"> </span>c_right

c</code></pre></div>
<pre><code>[1] 0.004938262</code></pre>
<p>The posterior mean is not well-defined since</p>
<p class="mathjaxWide"><span class="math display">\[
\mathbb E(N \mid y = 203)
=
\frac{1}{c}\sum_{203}^\infty \frac{1}{N} 
=
\infty
.
\]</span></p>
<p>The posterior variance and standard deviation are also not well-defined.</p>

<div id="disqus_thread"></div>
<script>
/**
 *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
 *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
 */
/*
   var disqus_config = function () {
   this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
   this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
   };
 */
(function() {  // DON'T EDIT BELOW THIS LINE
 var d = document, s = d.createElement('script');

 s.src = '//stappit-github-io.disqus.com/embed.js';

 s.setAttribute('data-timestamp', +new Date());
 (d.head || d.body).appendChild(s);
 })();
</script>

<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

<script id="dsq-count-scr" src="//stappit-github-io.disqus.com/count.js" async></script>
]]></summary>
</entry>
<entry>
    <title>BDA3 Chapter 2 Exercise 9</title>
    <link href="http://stappit.github.io/posts/bda3/chapter_02_exercise_09.html" />
    <id>http://stappit.github.io/posts/bda3/chapter_02_exercise_09.html</id>
    <published>2018-08-29T00:00:00Z</published>
    <updated>2018-08-29T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 class="post-title">BDA3 Chapter 2 Exercise 9</h1>

<div class="card post-meta border-0">
  <div class="card-body post-meta">
    <p class="card-text text-muted text-left">
    Posted on August 29, 2018  by Brian </br>
     Tags: <a href="/tags/bda%20chapter%202.html">bda chapter 2</a>, <a href="/tags/solutions.html">solutions</a>, <a href="/tags/bayes.html">bayes</a>, <a href="/tags/beta.html">beta</a>, <a href="/tags/prior%20sensitivity.html">prior sensitivity</a> </br>
     Category: <a href="/categories/bda3.html">bda3</a> 
    </p>
  </div>
</div>

<p>Here’s my solution to exercise 9, chapter 2, of <a href="https://andrewgelman.com/">Gelman’s</a> <em>Bayesian Data Analysis</em> (BDA), 3rd edition. There are <a href="http://www.stat.columbia.edu/~gelman/book/solutions.pdf">solutions</a> to some of the exercises on the <a href="http://www.stat.columbia.edu/~gelman/book/">book’s webpage</a>.</p>
<!--more-->
<div style="display:none">
<p class="mathjaxWide"><span class="math inline">\(\DeclareMathOperator{\dbinomial}{binomial} \DeclareMathOperator{\dbern}{Bernoulli} \DeclareMathOperator{\dnorm}{normal} \DeclareMathOperator{\dgamma}{gamma} \DeclareMathOperator{\invlogit}{invlogit} \DeclareMathOperator{\logit}{logit} \DeclareMathOperator{\dbeta}{beta}\)</span></p>
</div>
<p>The data show 650 people in support of the death penalty and 350 against. We explore the effect of different priors on the posterior.</p>
<p>First let’s find the prior with a mean of 0.6 and standard deviation 0.3. The mean of the <span class="math inline">\(\dbeta(\alpha, \beta)\)</span> distribution is</p>
<p class="mathjaxWide"><span class="math display">\[
\frac{3}{5}
=
\frac{\alpha}{\alpha + \beta}
\]</span></p>
<p>which implies that <span class="math inline">\(\alpha = 1.5 \beta\)</span>. The variance is</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
  \frac{9}{100}
  &amp;=
  \frac{\alpha \beta}{(\alpha + \beta)^2 (\alpha + \beta  + 1)}
  \\
  &amp;=
  \frac{3}{2} \frac{\beta^2}{\frac{25}{4}\beta^2 \frac{5\beta + 2}{2}}
  \\
  &amp;=
  \frac{3}{2}\frac{4}{25}\frac{2}{5\beta + 2}
  \\
  &amp;=
  \frac{12}{25(5\beta + 2)}
  \\
  &amp;\Leftrightarrow
  \\
  5\beta + 2
  &amp;=
  \frac{12}{25}\frac{100}{9}
  \\
  &amp;=
  4,
\end{align}
\]</span></p>
<p>which implies that <span class="math inline">\(\beta = \frac{2}{5}\)</span>. Thus <span class="math inline">\(\alpha = \frac{3}{5}\)</span>. Since both parameters are below 1, we see maxima near 0 and 1.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">α &lt;-<span class="st"> </span><span class="dv">3</span> <span class="op">/</span><span class="st"> </span><span class="dv">5</span>
β &lt;-<span class="st"> </span><span class="dv">2</span> <span class="op">/</span><span class="st"> </span><span class="dv">5</span>

<span class="kw">tibble</span>(<span class="dt">x =</span> <span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="fl">0.001</span>), <span class="dt">y =</span> <span class="kw">dbeta</span>(x, α, β)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">ggplot</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">aes</span>(x, y) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_area</span>(<span class="dt">fill =</span> <span class="st">&#39;skyblue&#39;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(
    <span class="dt">x =</span> <span class="st">&#39;x&#39;</span>,
    <span class="dt">y =</span> <span class="st">&#39;beta(x | α, β)&#39;</span>,
    <span class="dt">title =</span> <span class="st">&#39;Beta prior with mean 0.3 and standard deviation 0.6&#39;</span>,
    <span class="dt">subtitle =</span> <span class="kw">str_glue</span>(<span class="st">&#39;α = {α}, β = {β}&#39;</span>)
  )</code></pre></div>
<figure>
<img src="chapter_02_exercise_09_files/figure-markdown/prior-1.png" />
</figure>
<p>The beta distribution is self-conjugate so the posterior is <span class="math inline">\(\dbeta(0.6 + 650, 0.4 + 350)\)</span>.</p>
<p>Let’s plot the posterior with priors of different strength. We can increase the strength of the prior whilst keeping the mean constant by multiplying <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> by the same constant c. We will use <span class="math inline">\(c \in \{ 1, 10, 100, 1000\}\)</span>. In the plot below, we have restricted the x-axis to focus on the differences in the shape of the posteriors.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">support &lt;-<span class="st"> </span><span class="dv">650</span>
against &lt;-<span class="st"> </span><span class="dv">350</span>

<span class="kw">expand.grid</span>(<span class="dt">magnitude =</span> <span class="dv">0</span><span class="op">:</span><span class="dv">3</span>, <span class="dt">x =</span> <span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="fl">0.001</span>)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">as_tibble</span>() <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(
    <span class="dt">c =</span> <span class="dv">10</span><span class="op">^</span>magnitude,
    <span class="dt">a_prior =</span> α <span class="op">*</span><span class="st"> </span>c,
    <span class="dt">b_prior =</span> β <span class="op">*</span><span class="st"> </span>c,
    <span class="dt">y =</span> <span class="kw">dbeta</span>(x, support <span class="op">+</span><span class="st"> </span>a_prior, against <span class="op">+</span><span class="st"> </span>b_prior),
    <span class="dt">prior_magnitude =</span> <span class="kw">factor</span>(<span class="kw">as.character</span>(<span class="dv">10</span><span class="op">^</span>magnitude))
  ) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">aes</span>(x, y, <span class="dt">colour =</span> prior_magnitude) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_x_continuous</span>(<span class="dt">limits =</span> <span class="kw">c</span>(<span class="fl">0.55</span>, <span class="fl">0.75</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(
    <span class="dt">x =</span> <span class="st">&#39;x&#39;</span>,
    <span class="dt">y =</span> <span class="st">&#39;beta(x | support + a, against + b)&#39;</span>,
    <span class="dt">title =</span> <span class="st">&#39;Beta posterior with different priors&#39;</span>,
    <span class="dt">subtitle =</span> <span class="kw">str_glue</span>(<span class="kw">paste</span>(
      <span class="st">&#39;a = {α} * 10^magnitude, b = {β} * 10^magnitude&#39;</span>,
      <span class="st">&#39;support = 650, against = 350&#39;</span>,
      <span class="dt">sep =</span> <span class="st">&#39;</span><span class="ch">\n</span><span class="st">&#39;</span>
    )),
    <span class="dt">colour =</span> <span class="st">&#39;Magnitude of the prior&#39;</span>
  )</code></pre></div>
<figure>
<img src="chapter_02_exercise_09_files/figure-markdown/posterior-1.png" />
</figure>
<p>Magnitudes 1 and 10 give very similar results close to the maximum likelihood estimate of 65%. The higher magnitudes pull the mean towards the prior mean of 60%.</p>

<div id="disqus_thread"></div>
<script>
/**
 *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
 *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
 */
/*
   var disqus_config = function () {
   this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
   this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
   };
 */
(function() {  // DON'T EDIT BELOW THIS LINE
 var d = document, s = d.createElement('script');

 s.src = '//stappit-github-io.disqus.com/embed.js';

 s.setAttribute('data-timestamp', +new Date());
 (d.head || d.body).appendChild(s);
 })();
</script>

<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

<script id="dsq-count-scr" src="//stappit-github-io.disqus.com/count.js" async></script>
]]></summary>
</entry>
<entry>
    <title>BDA3 Chapter 2 Exercise 8</title>
    <link href="http://stappit.github.io/posts/bda3/chapter_02_exercise_08.html" />
    <id>http://stappit.github.io/posts/bda3/chapter_02_exercise_08.html</id>
    <published>2018-08-27T00:00:00Z</published>
    <updated>2018-08-27T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 class="post-title">BDA3 Chapter 2 Exercise 8</h1>

<div class="card post-meta border-0">
  <div class="card-body post-meta">
    <p class="card-text text-muted text-left">
    Posted on August 27, 2018  by Brian </br>
     Tags: <a href="/tags/bda%20chapter%202.html">bda chapter 2</a>, <a href="/tags/bda.html">bda</a>, <a href="/tags/solutions.html">solutions</a>, <a href="/tags/bayes.html">bayes</a>, <a href="/tags/normal.html">normal</a>, <a href="/tags/posterior%20predictive.html">posterior predictive</a> </br>
     Category: <a href="/categories/bda3.html">bda3</a> 
    </p>
  </div>
</div>

<p>Here’s my solution to exercise 8, chapter 2, of <a href="https://andrewgelman.com/">Gelman’s</a> <em>Bayesian Data Analysis</em> (BDA), 3rd edition. There are <a href="http://www.stat.columbia.edu/~gelman/book/solutions.pdf">solutions</a> to some of the exercises on the <a href="http://www.stat.columbia.edu/~gelman/book/">book’s webpage</a>.</p>
<!--more-->
<div style="display:none">
<p class="mathjaxWide"><span class="math inline">\(\DeclareMathOperator{\dbinomial}{binomial} \DeclareMathOperator{\dbern}{Bernoulli} \DeclareMathOperator{\dnorm}{normal} \DeclareMathOperator{\dgamma}{gamma} \DeclareMathOperator{\invlogit}{invlogit} \DeclareMathOperator{\logit}{logit} \DeclareMathOperator{\dbeta}{beta}\)</span></p>
</div>
<p>With prior <span class="math inline">\(\theta \sim \dnorm(180, 40)\)</span>, sampling distribution <span class="math inline">\(y \mid \theta \sim \dnorm(\theta, 20)\)</span>, and <span class="math inline">\(n\)</span> sampled students with average weight <span class="math inline">\(\bar y = 150\)</span>, it follows from 2.11 that the posterior mean is</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
  \mu
  :=
  \mathbb E(\theta \mid \bar y) 
  &amp;=
  \frac{\frac{180}{1600} + \frac{150n}{400}}{\frac{1}{1600} + \frac{n}{400}} 
  \\
  &amp;=
  \frac{60(3 + 10n)}{1600} \cdot \frac{1600}{1 + 4n}
  \\
  &amp;=
  \frac{60(3 + 10n)}{1 + 4n}
  \\
  1 / \sigma^2 
  :=
  1 / \mathbb V (\theta \mid \bar y)
  &amp;=
  \frac{1}{1600} + \frac{n}{400}
  \\
  &amp;=
  \frac{1 + 4n}{1600}
  .
\end{align}
\]</span></p>
<p>So <span class="math inline">\(\theta \mid \bar y ~ \dnorm \left( \frac{60(3 + 10n)}{1 + 4n}, \frac{40}{\sqrt{1 + 4n}} \right)\)</span>.</p>
<p>It follows from the calculations shown in the book that the posterior predictive distribution is <span class="math inline">\(\tilde y \mid y \sim \dnorm(\mu, \sigma + 20)\)</span>.</p>
<p>We can obtain 95% posterior intervals as follows.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mu &lt;-<span class="st"> </span><span class="cf">function</span>(n) <span class="dv">60</span> <span class="op">*</span><span class="st"> </span>(<span class="dv">3</span> <span class="op">+</span><span class="st"> </span><span class="dv">10</span> <span class="op">*</span><span class="st"> </span>n) <span class="op">/</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="dv">4</span> <span class="op">*</span><span class="st"> </span>n)
sigma &lt;-<span class="st"> </span><span class="cf">function</span>(n) <span class="dv">40</span> <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="dv">4</span> <span class="op">*</span><span class="st"> </span>n)

percentiles &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.05</span>, <span class="fl">0.95</span>)

theta_posterior_interval &lt;-<span class="st"> </span><span class="kw">qnorm</span>(percentiles, <span class="kw">mu</span>(<span class="dv">10</span>), <span class="kw">sigma</span>(<span class="dv">10</span>))
y_posterior_interval &lt;-<span class="st"> </span><span class="kw">qnorm</span>(percentiles, <span class="kw">mu</span>(<span class="dv">10</span>), <span class="kw">sigma</span>(<span class="dv">10</span>) <span class="op">+</span><span class="st"> </span><span class="dv">20</span>)</code></pre></div>
<p>With a sample of size of 10, we get θ ϵ [140.5, 161] and <span class="math inline">\(\tilde y\)</span> ϵ [107.6, 193.9].</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">theta_posterior_interval &lt;-<span class="st"> </span><span class="kw">qnorm</span>(percentiles, <span class="kw">mu</span>(<span class="dv">100</span>), <span class="kw">sigma</span>(<span class="dv">100</span>))
y_posterior_interval &lt;-<span class="st"> </span><span class="kw">qnorm</span>(percentiles, <span class="kw">mu</span>(<span class="dv">100</span>), <span class="kw">sigma</span>(<span class="dv">100</span>) <span class="op">+</span><span class="st"> </span><span class="dv">20</span>)</code></pre></div>
<p>With a sample of size of 100, we get θ ϵ [146.8, 153.4] and <span class="math inline">\(\tilde y\)</span> ϵ [113.9, 186.3].</p>

<div id="disqus_thread"></div>
<script>
/**
 *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
 *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
 */
/*
   var disqus_config = function () {
   this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
   this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
   };
 */
(function() {  // DON'T EDIT BELOW THIS LINE
 var d = document, s = d.createElement('script');

 s.src = '//stappit-github-io.disqus.com/embed.js';

 s.setAttribute('data-timestamp', +new Date());
 (d.head || d.body).appendChild(s);
 })();
</script>

<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

<script id="dsq-count-scr" src="//stappit-github-io.disqus.com/count.js" async></script>
]]></summary>
</entry>
<entry>
    <title>BDA3 Chapter 2 Exercise 7</title>
    <link href="http://stappit.github.io/posts/bda3/chapter_02_exercise_07.html" />
    <id>http://stappit.github.io/posts/bda3/chapter_02_exercise_07.html</id>
    <published>2018-08-26T00:00:00Z</published>
    <updated>2018-08-26T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 class="post-title">BDA3 Chapter 2 Exercise 7</h1>

<div class="card post-meta border-0">
  <div class="card-body post-meta">
    <p class="card-text text-muted text-left">
    Posted on August 26, 2018  by Brian </br>
     Tags: <a href="/tags/bda%20chapter%202.html">bda chapter 2</a>, <a href="/tags/bda.html">bda</a>, <a href="/tags/solutions.html">solutions</a>, <a href="/tags/bayes.html">bayes</a>, <a href="/tags/binomial.html">binomial</a>, <a href="/tags/natural%20parameter.html">natural parameter</a>, <a href="/tags/exponential%20family.html">exponential family</a>, <a href="/tags/improper%20prior.html">improper prior</a> </br>
     Category: <a href="/categories/bda3.html">bda3</a> 
    </p>
  </div>
</div>

<p>Here’s my solution to exercise 7, chapter 2, of <a href="https://andrewgelman.com/">Gelman’s</a> <em>Bayesian Data Analysis</em> (BDA), 3rd edition. There are <a href="http://www.stat.columbia.edu/~gelman/book/solutions.pdf">solutions</a> to some of the exercises on the <a href="http://www.stat.columbia.edu/~gelman/book/">book’s webpage</a>.</p>
<!--more-->
<div style="display:none">
<p class="mathjaxWide"><span class="math inline">\(\DeclareMathOperator{\dbinomial}{binomial} \DeclareMathOperator{\dbern}{Bernoulli} \DeclareMathOperator{\dgamma}{gamma} \DeclareMathOperator{\invlogit}{invlogit} \DeclareMathOperator{\logit}{logit} \DeclareMathOperator{\dbeta}{beta}\)</span></p>
</div>
<p>We show that a uniform prior on the natural parameter of a binomial model implies an improper prior under a different parameterisation.</p>
<p>The binomial likelihood can be written as a member of the exponential family as</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
  \dbinomial(y \mid \theta)
  &amp;=
  \binom{n}{y} \theta^y (1 - \theta)^{n - y}
  \\
  &amp;=
  \binom{n}{y} \cdot (1 - \theta)^n \cdot \exp \left(y \log \left(\frac{\theta}{1 - \theta}\right)\right)
  \\
  &amp;=
  f(y) \cdot g(\theta) \cdot \exp (\phi(\theta) \cdot u(y))
  ,
\end{align}
\]</span></p>
<p>where <span class="math inline">\(\phi(\theta) := \log \frac{\theta}{1 - \theta}\)</span>, <span class="math inline">\(u(y) := y\)</span>, <span class="math inline">\(g(\theta) := (1 - \theta)^n\)</span>. Suppose the natural parameter <span class="math inline">\(\phi \sim \dbeta(1, 1)\)</span> is uniformly distributed. Then the distribution of <span class="math inline">\(\theta\)</span> is</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
  p(\theta) 
  &amp;\propto
  p(\phi) \cdot \vert \invlogit^\prime (\phi) \vert^{-1}
  \\
  &amp;=
  \left \vert \frac{1}{1 + \exp(-\phi)}^\prime \right\vert^{-1}
  \\
  &amp;=
  \left \vert \frac{1}{\left(1 + \exp(-\phi)\right)^2} \cdot \exp(-\phi) \right\vert^{-1}
  \\
  &amp;=
  \frac{1 + 2 \exp(-\phi) + \exp(-2\phi)}{\exp(-\phi)}
  \\
  &amp;=
  \exp(\phi) + 2 + \exp(-\phi)
  \\
  &amp;=
  \frac{\theta}{1 - \theta} + 2 + \frac{1 - \theta}{\theta}
  \\
  &amp;=
  \frac{\theta^2 + 2\theta(1 - \theta) + (1 - \theta)^2}{\theta(1 - \theta)}
  \\
  &amp;=
  \frac{1}{\theta(1 - \theta)}
  \\
  &amp;=
  \theta^{-1}(1 - \theta)^{-1}
  \qquad \square
\end{align}
\]</span></p>
<p>This is an improper distribution on <span class="math inline">\(\theta\)</span> because</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
  \int_0^1 \frac{1}{\theta(1 - \theta)}
  &amp;\ge
  \int_0^1 \frac{1}{\theta}
  \\
  &amp;=
  \log\theta \vert_0^1
  \\
  &amp;=
  \infty.
\end{align}
\]</span></p>
<p>When <span class="math inline">\(y = 0\)</span>, then the posterior distribution is <span class="math inline">\(p(\theta \mid y = 0) \propto (1 - \theta)^{n - 1}\theta^{-1}\)</span>. When <span class="math inline">\(y = n\)</span>, then the posterior distribution is <span class="math inline">\(p(\theta \mid y = n) \propto \theta^{n-1}(1 - \theta)^{-1}\)</span>. These two cases are equivalent by the change of variable <span class="math inline">\(\theta \mapsto 1 - \theta\)</span>.</p>
<p>We show that the distribution is improper for <span class="math inline">\(y = 0\)</span> by induction. The case <span class="math inline">\(n = 0\)</span> is shown above (for the prior). Assume the distribution is improper for any integer <span class="math inline">\(k &lt; n\)</span>. Then using integration by parts yields</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
\int_0^1 \theta^{- 1}(1 - \theta)^{n - 1} d\theta
&amp;=
\int_0^1 \theta^{- 1}(1 - \theta)^{n - 2} \cdot (1 - \theta)d\theta
\\
&amp;=
\left[(1 - \theta)^{n-2}(1 - \frac{\theta}{2}) \right]_0^1
+
\int_0^1 \frac{(1 - \theta)^{n - 2}}{\theta}
+
(n-2)(1 - \theta)^{n-3}
-
\frac{(1 - \theta)^{n-2}}{2}
-
(n - 2)\theta\frac{(1 - \theta)^{n-3}}{2}
d\theta
\\
&amp;=
c
+
\int_0^1 \frac{(1 - \theta)^{n - 2}}{\theta} d\theta,
\end{align}
\]</span></p>
<p>where <span class="math inline">\(c &lt; \infty\)</span>. By the induction hypothesis, the integral on the last line is <span class="math inline">\(\infty\)</span>. Therefore, the distribution is also improper for <span class="math inline">\(n\)</span>.</p>

<div id="disqus_thread"></div>
<script>
/**
 *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
 *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
 */
/*
   var disqus_config = function () {
   this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
   this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
   };
 */
(function() {  // DON'T EDIT BELOW THIS LINE
 var d = document, s = d.createElement('script');

 s.src = '//stappit-github-io.disqus.com/embed.js';

 s.setAttribute('data-timestamp', +new Date());
 (d.head || d.body).appendChild(s);
 })();
</script>

<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

<script id="dsq-count-scr" src="//stappit-github-io.disqus.com/count.js" async></script>
]]></summary>
</entry>
<entry>
    <title>BDA3 Chapter 2 Exercise 6</title>
    <link href="http://stappit.github.io/posts/bda3/chapter_02_exercise_06.html" />
    <id>http://stappit.github.io/posts/bda3/chapter_02_exercise_06.html</id>
    <published>2018-08-25T00:00:00Z</published>
    <updated>2018-08-25T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 class="post-title">BDA3 Chapter 2 Exercise 6</h1>

<div class="card post-meta border-0">
  <div class="card-body post-meta">
    <p class="card-text text-muted text-left">
    Posted on August 25, 2018  by Brian </br>
     Tags: <a href="/tags/bda%20chapter%202.html">bda chapter 2</a>, <a href="/tags/bda.html">bda</a>, <a href="/tags/solutions.html">solutions</a>, <a href="/tags/bayes.html">bayes</a>, <a href="/tags/poisson.html">poisson</a>, <a href="/tags/gamma.html">gamma</a>, <a href="/tags/negative%20binomial.html">negative binomial</a> </br>
     Category: <a href="/categories/bda3.html">bda3</a> 
    </p>
  </div>
</div>

<p>Here’s my solution to exercise 6, chapter 2, of <a href="https://andrewgelman.com/">Gelman’s</a> <em>Bayesian Data Analysis</em> (BDA), 3rd edition. There are <a href="http://www.stat.columbia.edu/~gelman/book/solutions.pdf">solutions</a> to some of the exercises on the <a href="http://www.stat.columbia.edu/~gelman/book/">book’s webpage</a>.</p>
<!--more-->
<div style="display:none">
<p class="mathjaxWide"><span class="math inline">\(\DeclareMathOperator{\dbinomial}{binomial} \DeclareMathOperator{\dbern}{Bernoulli} \DeclareMathOperator{\dgamma}{gamma} \DeclareMathOperator{\dpois}{Poisson} \DeclareMathOperator{\dbeta}{beta}\)</span></p>
</div>
<p>Considering the negative binomial variable <span class="math inline">\(y\)</span> as a gamma-Poisson variable, we derive expressions for the mean and variance.</p>
<p>From equation 1.6, <span class="math inline">\(\mathbb E (y) = \mathbb E (\mathbb E(y \mid \theta))\)</span>. Since <span class="math inline">\(y \mid \theta \sim \dpois(10n\theta)\)</span>, it follows that <span class="math inline">\(\mathbb E (y \mid \theta) = 10n\theta\)</span>. The rate <span class="math inline">\(\theta \sim \dgamma(\alpha, \beta)\)</span> so <span class="math inline">\(\mathbb E(\theta) = \frac{\alpha}{\beta}\)</span>. Thus, <span class="math inline">\(\mathbb E(y) = 10n\mathbb E(\theta) = 10n \frac{\alpha}{\beta}\)</span>.</p>
<p>We also have <span class="math inline">\(\mathbb V(\theta) = \frac{\alpha}{\beta^2}\)</span> since <span class="math inline">\(\theta \sim \dgamma(\alpha, \beta)\)</span>, and <span class="math inline">\(\mathbb V(y \mid \theta) = 10n\theta\)</span> since <span class="math inline">\(y \mid \theta \sim \dpois(10n\theta)\)</span>. Thus,</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
  \mathbb V (y) 
  &amp;= 
  \mathbb E(\mathbb V(y \mid \theta)) + \mathbb V (\mathbb E (y \mid \theta)) 
  \\
  &amp;=
  \mathbb E(10n\theta) + \mathbb V (10n\theta)
  \\
  &amp;=
  10n\frac{\alpha}{\beta} + (10n)^2\frac{\alpha}{\beta^2}
  \qquad \square
\end{align}
\]</span></p>

<div id="disqus_thread"></div>
<script>
/**
 *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
 *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
 */
/*
   var disqus_config = function () {
   this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
   this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
   };
 */
(function() {  // DON'T EDIT BELOW THIS LINE
 var d = document, s = d.createElement('script');

 s.src = '//stappit-github-io.disqus.com/embed.js';

 s.setAttribute('data-timestamp', +new Date());
 (d.head || d.body).appendChild(s);
 })();
</script>

<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

<script id="dsq-count-scr" src="//stappit-github-io.disqus.com/count.js" async></script>
]]></summary>
</entry>
<entry>
    <title>BDA3 Chapter 2 Exercise 5</title>
    <link href="http://stappit.github.io/posts/bda3/chapter_02_exercise_05.html" />
    <id>http://stappit.github.io/posts/bda3/chapter_02_exercise_05.html</id>
    <published>2018-08-24T00:00:00Z</published>
    <updated>2018-08-24T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 class="post-title">BDA3 Chapter 2 Exercise 5</h1>

<div class="card post-meta border-0">
  <div class="card-body post-meta">
    <p class="card-text text-muted text-left">
    Posted on August 24, 2018  by Brian </br>
     Tags: <a href="/tags/bayes.html">bayes</a>, <a href="/tags/solutions.html">solutions</a>, <a href="/tags/bda%20chapter%202.html">bda chapter 2</a>, <a href="/tags/bda.html">bda</a>, <a href="/tags/beta.html">beta</a>, <a href="/tags/binomial.html">binomial</a>, <a href="/tags/beta-binomial.html">beta-binomial</a>, <a href="/tags/variance.html">variance</a> </br>
     Category: <a href="/categories/bda3.html">bda3</a> 
    </p>
  </div>
</div>

<p>Here’s my solution to exercise 5, chapter 2, of <a href="https://andrewgelman.com/">Gelman’s</a> <em>Bayesian Data Analysis</em> (BDA), 3rd edition. There are <a href="http://www.stat.columbia.edu/~gelman/book/solutions.pdf">solutions</a> to some of the exercises on the <a href="http://www.stat.columbia.edu/~gelman/book/">book’s webpage</a>.</p>
<!--more-->
<div style="display:none">
<p class="mathjaxWide"><span class="math inline">\(\DeclareMathOperator{\dbinomial}{binomial} \DeclareMathOperator{\dbern}{Bernoulli} \DeclareMathOperator{\dbeta}{beta}\)</span></p>
</div>
<p>Let’s derive the prior predictive distribution of a beta-binomial model with a uniform prior. See <a href="https://math.stackexchange.com/questions/122296/how-to-evaluate-this-integral-relating-to-binomial">stackexchange</a> and <a href="https://en.wikipedia.org/wiki/Beta_function">wikipedia</a> for useful results for solving the integral below.</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
  p(y = k)
  &amp;=
  \int_0^1 p(y = k \mid \theta) p(\theta) d\theta
  \\
  &amp;=
  \binom{n}{k} \cdot \int_0^1 \theta^k (1 - \theta)^{n - k} d\theta
  \\
  &amp;=
  \binom{n}{k} \cdot \frac{1}{\binom{n}{k} \cdot (n + 1)}
  \\
  &amp;=
  \frac{1}{n + 1}
\end{align}
\]</span></p>
<p>Now let’s show that the posterior mean of <span class="math inline">\(\theta\)</span> lies between the prior mean and observed frequency. The posterior is</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
  p(\theta \mid y)
  &amp;\propto
  p(y \mid \theta) \cdot p(\theta)
  \\
  &amp;\propto
  \theta^y (1 - \theta)^{n - y}\cdot \theta^{\alpha - 1} (1 - \theta)^{\beta - 1}
  \\
  &amp;=
  \theta^{y + \alpha - 1} (1 - \theta)^{n + \beta - y - 1}.
\end{align}
\]</span></p>
<p>So <span class="math inline">\(p(\theta \mid y) \sim \dbeta(y + \alpha, n - y + \beta)\)</span>, which has mean <span class="math inline">\(\frac{y + \alpha}{n + \alpha + \beta}\)</span>. Suppose <span class="math inline">\(\frac{y}{n} \le \frac{\alpha}{\alpha + \beta}\)</span>. Then</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
  \frac{y}{n}
  &amp;\le
  \frac{y + \alpha}{n + \alpha + \beta}
  \\
  \Leftrightarrow
  y(n + \alpha + \beta)
  &amp;\le
  n(y + \alpha)
  \\
  \Leftrightarrow
  y(\alpha + \beta)
  &amp;\le
  n\alpha
  \\
  \Leftrightarrow
  \frac{y}{n} 
  &amp;\le 
  \frac{\alpha}{\alpha + \beta}
\end{align}
\]</span></p>
<p>A similar argument shows that <span class="math inline">\(\frac{y + \alpha}{n + \alpha + \beta} \le \frac{\alpha}{\alpha + \beta}\)</span>.</p>
<p>If <span class="math inline">\(\frac{y}{n} \ge \frac{\alpha}{\alpha + \beta}\)</span>, then the analogous argument shows that <span class="math inline">\(\frac{\alpha}{\alpha + \beta} \le \frac{y + \alpha}{n + \alpha + \beta} \le \frac{y}{n}. \square\)</span></p>
<p>The prior variance is <span class="math inline">\(\mathbb V (\theta) = \frac{\alpha \beta}{(\alpha + \beta)^2(\alpha + \beta + 1)}\)</span>. For a uniform prior this is <span class="math inline">\(\frac{1}{4 \cdot 3} = \frac{1}{12}\)</span>. The posterior variance with a uniform prior is <span class="math inline">\(\frac{y + 1}{n + 2} \cdot \frac{n - y + 1}{n + 2} \cdot \frac{1}{n + 3}\)</span>. For <span class="math inline">\(p \in [0, 1]\)</span>, the function <span class="math inline">\(p \mapsto p(1 - p)\)</span> is maximised when <span class="math inline">\(p = 0.5\)</span>. Thus for fixed <span class="math inline">\(n\)</span>, the posterior variance is maximised when <span class="math inline">\(y = \frac{n}{2}\)</span>. This means that the posterior variance is at most <span class="math inline">\(\frac{1}{4} \cdot \frac{1}{n + 3} \le \frac{1}{4n + 12} \le \frac{1}{12}. \square\)</span></p>
<p>Intuitively, the posterior variance should be larger than the prior variance when the observed data is different from what would be expected from the prior distribution. (This can’t happen with a uniform prior because every value is equally likely). Indeed, with prior <span class="math inline">\(\theta \sim \dbeta(1, 9)\)</span> and observed data <span class="math inline">\(y = 9, n = 10\)</span>, we have <span class="math inline">\(\mathbb V(\theta) = \frac{9}{1100}\)</span> and <span class="math inline">\(\mathbb V(\theta \mid y) = \frac{1}{2} \cdot \frac{1}{2} \cdot \frac{1}{21} = \frac{1}{84}\)</span>.</p>

<div id="disqus_thread"></div>
<script>
/**
 *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
 *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
 */
/*
   var disqus_config = function () {
   this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
   this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
   };
 */
(function() {  // DON'T EDIT BELOW THIS LINE
 var d = document, s = d.createElement('script');

 s.src = '//stappit-github-io.disqus.com/embed.js';

 s.setAttribute('data-timestamp', +new Date());
 (d.head || d.body).appendChild(s);
 })();
</script>

<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

<script id="dsq-count-scr" src="//stappit-github-io.disqus.com/count.js" async></script>
]]></summary>
</entry>
<entry>
    <title>BDA3 Chapter 2 Exercise 4</title>
    <link href="http://stappit.github.io/posts/bda3/chapter_02_exercise_04.html" />
    <id>http://stappit.github.io/posts/bda3/chapter_02_exercise_04.html</id>
    <published>2018-08-23T00:00:00Z</published>
    <updated>2018-08-23T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 class="post-title">BDA3 Chapter 2 Exercise 4</h1>

<div class="card post-meta border-0">
  <div class="card-body post-meta">
    <p class="card-text text-muted text-left">
    Posted on August 23, 2018  by Brian </br>
     Tags: <a href="/tags/binomial.html">binomial</a>, <a href="/tags/bayes.html">bayes</a>, <a href="/tags/solutions.html">solutions</a>, <a href="/tags/bda%20chapter%202.html">bda chapter 2</a>, <a href="/tags/bda.html">bda</a>, <a href="/tags/normal%20approximation.html">normal approximation</a>, <a href="/tags/multi-modal.html">multi-modal</a> </br>
     Category: <a href="/categories/bda3.html">bda3</a> 
    </p>
  </div>
</div>

<p>Here’s my solution to exercise 4, chapter 2, of <a href="https://andrewgelman.com/">Gelman’s</a> <em>Bayesian Data Analysis</em> (BDA), 3rd edition. There are <a href="http://www.stat.columbia.edu/~gelman/book/solutions.pdf">solutions</a> to some of the exercises on the <a href="http://www.stat.columbia.edu/~gelman/book/">book’s webpage</a>.</p>
<!--more-->
<div style="display:none">
<p class="mathjaxWide"><span class="math inline">\(\DeclareMathOperator{\dbinomial}{binomial} \DeclareMathOperator{\dbern}{Bernoulli} \DeclareMathOperator{\dbeta}{beta}\)</span></p>
</div>
<p>Consider 1000 rolls of an unfair die, where the probability of a 6 is either 1/4, 1/6, or 1/12. Let’s draw the distribution and the normal approximation.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">N &lt;-<span class="st"> </span><span class="dv">1000</span>
p6 &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span> <span class="op">/</span><span class="st"> </span><span class="dv">4</span>, <span class="dv">1</span> <span class="op">/</span><span class="st"> </span><span class="dv">6</span>, <span class="dv">1</span> <span class="op">/</span><span class="st"> </span><span class="dv">12</span>)

ex4 &lt;-<span class="st"> </span><span class="kw">expand.grid</span>(
    <span class="dt">y =</span> <span class="kw">seq</span>(<span class="dv">0</span>, N),
    <span class="dt">theta =</span> p6
  ) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(
    <span class="dt">mu =</span> N <span class="op">*</span><span class="st"> </span>theta,
    <span class="dt">sigma =</span> <span class="kw">sqrt</span>(N <span class="op">*</span><span class="st"> </span>theta <span class="op">*</span><span class="st"> </span>(<span class="dv">10</span> <span class="op">-</span><span class="st"> </span>theta)),
    <span class="dt">binomial =</span> <span class="kw">dbinom</span>(y, N, theta),
    <span class="dt">normal_approx =</span> <span class="kw">dnorm</span>(y, mu, sigma),
    <span class="dt">theta =</span> scales<span class="op">::</span><span class="kw">percent</span>(<span class="kw">signif</span>(theta))
  ) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">select</span>(<span class="op">-</span>mu, <span class="op">-</span>sigma) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">gather</span>(distribution, probability, binomial, normal_approx) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">spread</span>(theta, probability) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">prior_probability =</span> <span class="fl">0.25</span> <span class="op">*</span><span class="st"> `</span><span class="dt">8.3%</span><span class="st">`</span> <span class="op">+</span><span class="st"> </span><span class="fl">0.5</span> <span class="op">*</span><span class="st"> `</span><span class="dt">16.7%</span><span class="st">`</span> <span class="op">+</span><span class="st"> </span><span class="fl">0.25</span> <span class="op">*</span><span class="st"> `</span><span class="dt">25.0%</span><span class="st">`</span>)</code></pre></div>
<table class="table" style="margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:right;">
y
</th>
<th style="text-align:left;">
distribution
</th>
<th style="text-align:right;">
16.7%
</th>
<th style="text-align:right;">
25.0%
</th>
<th style="text-align:right;">
8.3%
</th>
<th style="text-align:right;">
prior_probability
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
0
</td>
<td style="text-align:left;">
binomial
</td>
<td style="text-align:right;">
0.0e+00
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0.0000000
</td>
<td style="text-align:right;">
0.00e+00
</td>
</tr>
<tr>
<td style="text-align:right;">
0
</td>
<td style="text-align:left;">
normal_approx
</td>
<td style="text-align:right;">
2.1e-06
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0.0002078
</td>
<td style="text-align:right;">
5.30e-05
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
binomial
</td>
<td style="text-align:right;">
0.0e+00
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0.0000000
</td>
<td style="text-align:right;">
0.00e+00
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
normal_approx
</td>
<td style="text-align:right;">
2.3e-06
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0.0002297
</td>
<td style="text-align:right;">
5.86e-05
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:left;">
binomial
</td>
<td style="text-align:right;">
0.0e+00
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0.0000000
</td>
<td style="text-align:right;">
0.00e+00
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:left;">
normal_approx
</td>
<td style="text-align:right;">
2.5e-06
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0.0002536
</td>
<td style="text-align:right;">
6.47e-05
</td>
</tr>
</tbody>
</table>
<figure>
<img src="chapter_02_exercise_04_files/figure-markdown/ex4_plot-1.png" />
</figure>
<p>The normal approximation underestimates the maxima and overestimates the values between the maxima. From the percentiles in the table below, we see that the normal approximation is best near the median but becomes gradually worse towards towards both extremes.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">percentiles &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.05</span>, <span class="fl">0.25</span>, <span class="fl">0.5</span>, <span class="fl">0.75</span>, <span class="fl">0.95</span>)

ex4 <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(distribution) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">arrange</span>(y) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(
    <span class="dt">cdf =</span> <span class="kw">cumsum</span>(prior_probability),
    <span class="dt">percentile =</span> <span class="kw">case_when</span>(
      cdf <span class="op">&lt;=</span><span class="st"> </span><span class="fl">0.05</span> <span class="op">~</span><span class="st"> &#39;05%&#39;</span>,
      cdf <span class="op">&lt;=</span><span class="st"> </span><span class="fl">0.25</span> <span class="op">~</span><span class="st"> &#39;25%&#39;</span>,
      cdf <span class="op">&lt;=</span><span class="st"> </span><span class="fl">0.50</span> <span class="op">~</span><span class="st"> &#39;50%&#39;</span>,
      cdf <span class="op">&lt;=</span><span class="st"> </span><span class="fl">0.75</span> <span class="op">~</span><span class="st"> &#39;75%&#39;</span>,
      cdf <span class="op">&lt;=</span><span class="st"> </span><span class="fl">0.95</span> <span class="op">~</span><span class="st"> &#39;95%&#39;</span>
    )
  ) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">filter</span>(cdf <span class="op">&lt;=</span><span class="st"> </span><span class="fl">0.95</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(distribution, percentile) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">slice</span>(<span class="kw">which.max</span>(cdf)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">select</span>(distribution, percentile, y) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">spread</span>(distribution, y) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">arrange</span>(percentile) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">kable</span>() <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">kable_styling</span>()</code></pre></div>
<table class="table" style="margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
percentile
</th>
<th style="text-align:right;">
binomial
</th>
<th style="text-align:right;">
normal_approx
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
05%
</td>
<td style="text-align:right;">
75
</td>
<td style="text-align:right;">
58
</td>
</tr>
<tr>
<td style="text-align:left;">
25%
</td>
<td style="text-align:right;">
119
</td>
<td style="text-align:right;">
110
</td>
</tr>
<tr>
<td style="text-align:left;">
50%
</td>
<td style="text-align:right;">
166
</td>
<td style="text-align:right;">
164
</td>
</tr>
<tr>
<td style="text-align:left;">
75%
</td>
<td style="text-align:right;">
206
</td>
<td style="text-align:right;">
214
</td>
</tr>
<tr>
<td style="text-align:left;">
95%
</td>
<td style="text-align:right;">
260
</td>
<td style="text-align:right;">
291
</td>
</tr>
</tbody>
</table>

<div id="disqus_thread"></div>
<script>
/**
 *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
 *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
 */
/*
   var disqus_config = function () {
   this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
   this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
   };
 */
(function() {  // DON'T EDIT BELOW THIS LINE
 var d = document, s = d.createElement('script');

 s.src = '//stappit-github-io.disqus.com/embed.js';

 s.setAttribute('data-timestamp', +new Date());
 (d.head || d.body).appendChild(s);
 })();
</script>

<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

<script id="dsq-count-scr" src="//stappit-github-io.disqus.com/count.js" async></script>
]]></summary>
</entry>
<entry>
    <title>BDA3 Chapter 2 Exercise 3</title>
    <link href="http://stappit.github.io/posts/bda3/chapter_02_exercise_03.html" />
    <id>http://stappit.github.io/posts/bda3/chapter_02_exercise_03.html</id>
    <published>2018-08-22T00:00:00Z</published>
    <updated>2018-08-22T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 class="post-title">BDA3 Chapter 2 Exercise 3</h1>

<div class="card post-meta border-0">
  <div class="card-body post-meta">
    <p class="card-text text-muted text-left">
    Posted on August 22, 2018  by Brian </br>
     Tags: <a href="/tags/binomial.html">binomial</a>, <a href="/tags/bayes.html">bayes</a>, <a href="/tags/solutions.html">solutions</a>, <a href="/tags/bda%20chapter%202.html">bda chapter 2</a>, <a href="/tags/bda.html">bda</a>, <a href="/tags/normal%20approximation.html">normal approximation</a> </br>
     Category: <a href="/categories/bda3.html">bda3</a> 
    </p>
  </div>
</div>

<p>Here’s my solution to exercise 3, chapter 2, of <a href="https://andrewgelman.com/">Gelman’s</a> <em>Bayesian Data Analysis</em> (BDA), 3rd edition. There are <a href="http://www.stat.columbia.edu/~gelman/book/solutions.pdf">solutions</a> to some of the exercises on the <a href="http://www.stat.columbia.edu/~gelman/book/">book’s webpage</a>.</p>
<!--more-->
<div style="display:none">
<p class="mathjaxWide"><span class="math inline">\(\DeclareMathOperator{\dbinomial}{binomial} \DeclareMathOperator{\dbern}{Bernoulli} \DeclareMathOperator{\dbeta}{beta}\)</span></p>
</div>
<p>For 1000 rolls of a fair die, The mean number of sixs is 1000/6 = 166.667, the variance is 138.889, and the standard deviation is 11.7851.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">N &lt;-<span class="st"> </span><span class="dv">1000</span>
p &lt;-<span class="st"> </span><span class="dv">1</span> <span class="op">/</span><span class="st"> </span><span class="dv">6</span>
mu &lt;-<span class="st"> </span>N <span class="op">*</span><span class="st"> </span>p
sigma &lt;-<span class="st"> </span><span class="kw">sqrt</span>(N <span class="op">*</span><span class="st"> </span>p <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>p))

ex3 &lt;-<span class="st"> </span><span class="kw">tibble</span>(
    <span class="dt">y =</span> <span class="kw">seq</span>(<span class="dv">0</span>, N),
    <span class="dt">binomial =</span> <span class="kw">dbinom</span>(y, N, p),
    <span class="dt">normal_approx =</span> <span class="kw">dnorm</span>(y, mu, sigma)
  ) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">gather</span>(metric, probability, <span class="op">-</span>y) </code></pre></div>
<table class="table" style="margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:right;">
y
</th>
<th style="text-align:left;">
metric
</th>
<th style="text-align:right;">
probability
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
0
</td>
<td style="text-align:left;">
binomial
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
binomial
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:left;">
binomial
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:right;">
3
</td>
<td style="text-align:left;">
binomial
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:right;">
4
</td>
<td style="text-align:left;">
binomial
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:right;">
5
</td>
<td style="text-align:left;">
binomial
</td>
<td style="text-align:right;">
0
</td>
</tr>
</tbody>
</table>
<figure>
<img src="chapter_02_exercise_03_files/figure-markdown/ex3_plot-1.png" />
</figure>
<p>The two curves are visually indistinguishable. The percentiles are listed in the table below.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">percentiles &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.05</span>, <span class="fl">0.25</span>, <span class="fl">0.5</span>, <span class="fl">0.75</span>, <span class="fl">0.95</span>)

<span class="kw">tibble</span>(
    <span class="dt">percentile =</span> scales<span class="op">::</span><span class="kw">percent</span>(percentiles),
    <span class="dt">binom =</span> <span class="kw">qbinom</span>(percentiles, N, p),
    <span class="dt">norm =</span> <span class="kw">qnorm</span>(percentiles, mu, sigma)
) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">kable</span>() <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">kable_styling</span>()</code></pre></div>
<table class="table" style="margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
percentile
</th>
<th style="text-align:right;">
binom
</th>
<th style="text-align:right;">
norm
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
5%
</td>
<td style="text-align:right;">
147
</td>
<td style="text-align:right;">
147.2819
</td>
</tr>
<tr>
<td style="text-align:left;">
25%
</td>
<td style="text-align:right;">
159
</td>
<td style="text-align:right;">
158.7177
</td>
</tr>
<tr>
<td style="text-align:left;">
50%
</td>
<td style="text-align:right;">
167
</td>
<td style="text-align:right;">
166.6667
</td>
</tr>
<tr>
<td style="text-align:left;">
75%
</td>
<td style="text-align:right;">
175
</td>
<td style="text-align:right;">
174.6156
</td>
</tr>
<tr>
<td style="text-align:left;">
95%
</td>
<td style="text-align:right;">
186
</td>
<td style="text-align:right;">
186.0515
</td>
</tr>
</tbody>
</table>

<div id="disqus_thread"></div>
<script>
/**
 *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
 *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
 */
/*
   var disqus_config = function () {
   this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
   this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
   };
 */
(function() {  // DON'T EDIT BELOW THIS LINE
 var d = document, s = d.createElement('script');

 s.src = '//stappit-github-io.disqus.com/embed.js';

 s.setAttribute('data-timestamp', +new Date());
 (d.head || d.body).appendChild(s);
 })();
</script>

<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

<script id="dsq-count-scr" src="//stappit-github-io.disqus.com/count.js" async></script>
]]></summary>
</entry>
<entry>
    <title>BDA3 Chapter 2 Exercise 2</title>
    <link href="http://stappit.github.io/posts/bda3/chapter_02_exercise_02.html" />
    <id>http://stappit.github.io/posts/bda3/chapter_02_exercise_02.html</id>
    <published>2018-08-21T00:00:00Z</published>
    <updated>2018-08-21T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 class="post-title">BDA3 Chapter 2 Exercise 2</h1>

<div class="card post-meta border-0">
  <div class="card-body post-meta">
    <p class="card-text text-muted text-left">
    Posted on August 21, 2018  by Brian </br>
     Tags: <a href="/tags/stan.html">stan</a>, <a href="/tags/binomial.html">binomial</a>, <a href="/tags/bayes.html">bayes</a>, <a href="/tags/solutions.html">solutions</a>, <a href="/tags/bda%20chapter%202.html">bda chapter 2</a>, <a href="/tags/bda.html">bda</a> </br>
     Category: <a href="/categories/bda3.html">bda3</a> 
    </p>
  </div>
</div>

<p>Here’s my solution to exercise 2, chapter 2, of <a href="https://andrewgelman.com/">Gelman’s</a> <em>Bayesian Data Analysis</em> (BDA), 3rd edition. There are <a href="http://www.stat.columbia.edu/~gelman/book/solutions.pdf">solutions</a> to some of the exercises on the <a href="http://www.stat.columbia.edu/~gelman/book/">book’s webpage</a>.</p>
<!--more-->
<div style="display:none">
<p class="mathjaxWide"><span class="math inline">\(\DeclareMathOperator{\dbinomial}{binomial} \DeclareMathOperator{\dbern}{Bernoulli} \DeclareMathOperator{\dbeta}{beta}\)</span></p>
</div>
<p>We are given the following information about the two coins.</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
  p(C_1) &amp;= 0.5 &amp; p(H \mid C_1) &amp;= \dbern(H \mid 0.6) 
  \\
  p(C_2) &amp;= 0.5 &amp; p(H \mid C_2) &amp;= \dbern(H \mid 0.4)
\end{align}
\]</span></p>
<p>The posterior probability of each coin given two tails is:</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
  p(C_1 \mid TT )
  &amp;\propto
  p(TT \mid C_1) \cdot p(C_1)
  \\
  &amp;=
  \left(\frac{2}{5}\right)^2 \frac{1}{2}
  \\
  &amp;=
  \frac{2}{25}
\end{align}
\]</span> <span class="math display">\[
\begin{align}
  p(C_2 \mid TT )
  &amp;\propto
  p(TT \mid C_2) \cdot p(C_2)
  \\
  &amp;=
  \left(\frac{3}{5}\right)^2 \frac{1}{2}
  \\
  &amp;=
  \frac{9}{50}
\end{align}
\]</span></p>
<p>Both of the previous probabilities are normalised by the same constant. Since <span class="math inline">\(p(C_1 \mid TT) + p(C_2 \mid TT) = 1\)</span>, the normalising constant is <span class="math inline">\(\frac{2}{25} + \frac{9}{50} = \frac{13}{50}\)</span>. Thus</p>
<p class="mathjaxWide"><span class="math display">\[
p(C_1 \mid TT) = \frac{4}{13}
\qquad
\text{and}
\qquad
p(C_2 \mid TT) = \frac{9}{13}.
\]</span></p>
<p>Let <span class="math inline">\(y\)</span> be the number of additional spins until the next head. Conditional on a coin, <span class="math inline">\(y\)</span> is <a href="https://en.wikipedia.org/wiki/Geometric_distribution">geometrically</a> distributed. So the expected number of spins before the next head is:</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
  \mathbb E(y \mid TT)
  &amp;=
  \frac{4}{13}\mathbb E(y \mid C_1)
  +
  \frac{9}{13}\mathbb E(y \mid C_1)
  \\
  &amp;=
  \frac{4}{13}\frac{5}{3}
  +
  \frac{9}{13}\frac{5}{2}
  \\
  &amp;=
  \frac{20}{39}
  +
  \frac{45}{26}
  \\
  &amp;=
  \frac{175}{78},
\end{align}
\]</span></p>
<p>which is 2.24359.</p>

<div id="disqus_thread"></div>
<script>
/**
 *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
 *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
 */
/*
   var disqus_config = function () {
   this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
   this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
   };
 */
(function() {  // DON'T EDIT BELOW THIS LINE
 var d = document, s = d.createElement('script');

 s.src = '//stappit-github-io.disqus.com/embed.js';

 s.setAttribute('data-timestamp', +new Date());
 (d.head || d.body).appendChild(s);
 })();
</script>

<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

<script id="dsq-count-scr" src="//stappit-github-io.disqus.com/count.js" async></script>
]]></summary>
</entry>

</feed>
