<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>Thoughts from the Café</title>
    <link href="http://stappit.github.io/atom.xml" rel="self" />
    <link href="http://stappit.github.io" />
    <id>http://stappit.github.io/atom.xml</id>
    <author>
        <name>Brian</name>
        <email>ha@hahaha.com</email>
    </author>
    <updated>2018-08-25T00:00:00Z</updated>
    <entry>
    <title>BDA3 Chapter 2 Exercise 7</title>
    <link href="http://stappit.github.io/posts/bda3/chapter_02_exercise_07.html" />
    <id>http://stappit.github.io/posts/bda3/chapter_02_exercise_07.html</id>
    <published>2018-08-25T00:00:00Z</published>
    <updated>2018-08-25T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 class="post-title">BDA3 Chapter 2 Exercise 7</h1>

<div class="card post-meta border-0">
  <div class="card-body post-meta">
    <p class="card-text text-muted text-left">
    Posted on August 25, 2018  by Brian </br>
     Tags: <a href="/tags/bda%20chapter%202.html">bda chapter 2</a>, <a href="/tags/bda.html">bda</a>, <a href="/tags/solutions.html">solutions</a>, <a href="/tags/bayes.html">bayes</a>, <a href="/tags/binomial.html">binomial</a>, <a href="/tags/natural%20parameter.html">natural parameter</a>, <a href="/tags/exponential%20family.html">exponential family</a>, <a href="/tags/improper%20prior.html">improper prior</a> </br>
     Category: <a href="/categories/bda3.html">bda3</a> 
    </p>
  </div>
</div>

<p>Here’s my solution to exercise 7, chapter 2, of <a href="https://andrewgelman.com/">Gelman’s</a> <em>Bayesian Data Analysis</em> (BDA), 3rd edition. There are <a href="http://www.stat.columbia.edu/~gelman/book/solutions.pdf">solutions</a> to some of the exercises on the <a href="http://www.stat.columbia.edu/~gelman/book/">book’s webpage</a>.</p>
<!--more-->
<div style="display:none">
<p class="mathjaxWide"><span class="math inline">\(\DeclareMathOperator{\dbinomial}{binomial} \DeclareMathOperator{\dbern}{Bernoulli} \DeclareMathOperator{\dgamma}{gamma} \DeclareMathOperator{\invlogit}{invlogit} \DeclareMathOperator{\logit}{logit} \DeclareMathOperator{\dbeta}{beta}\)</span></p>
</div>
<p>We show that a uniform prior on the natural parameter of a binomial model implies an improper prior under a different parameterisation.</p>
<p>The binomial likelihood can be written as a member of the exponential family as</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
  \dbinomial(y \mid \theta)
  &amp;=
  \binom{n}{y} \theta^y (1 - \theta)^{n - y}
  \\
  &amp;=
  \binom{n}{y} \cdot (1 - \theta)^n \cdot \exp \left(y \log \left(\frac{\theta}{1 - \theta}\right)\right)
  \\
  &amp;=
  f(y) \cdot g(\theta) \cdot \exp (\phi(\theta) \cdot u(y))
  ,
\end{align}
\]</span></p>
<p>where <span class="math inline">\(\phi(\theta) := \log \frac{\theta}{1 - \theta}\)</span>, <span class="math inline">\(u(y) := y\)</span>, <span class="math inline">\(g(\theta) := (1 - \theta)^n\)</span>. Suppose the natural parameter <span class="math inline">\(\phi \sim \dbeta(1, 1)\)</span> is uniformly distributed. Then the distribution of <span class="math inline">\(\theta\)</span> is</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
  p(\theta) 
  &amp;\propto
  p(\phi) \cdot \vert \invlogit^\prime (\phi) \vert^{-1}
  \\
  &amp;=
  \left \vert \frac{1}{1 + \exp(-\phi)}^\prime \right\vert^{-1}
  \\
  &amp;=
  \left \vert \frac{1}{\left(1 + \exp(-\phi)\right)^2} \cdot \exp(-\phi) \right\vert^{-1}
  \\
  &amp;=
  \frac{1 + 2 \exp(-\phi) + \exp(-2\phi)}{\exp(-\phi)}
  \\
  &amp;=
  \exp(\phi) + 2 + \exp(-\phi)
  \\
  &amp;=
  \frac{\theta}{1 - \theta} + 2 + \frac{1 - \theta}{\theta}
  \\
  &amp;=
  \frac{\theta^2 + 2\theta(1 - \theta) + (1 - \theta)^2}{\theta(1 - \theta)}
  \\
  &amp;=
  \frac{1}{\theta(1 - \theta)}
  \\
  &amp;=
  \theta^{-1}(1 - \theta)^{-1}
  \qquad \square
\end{align}
\]</span></p>
<p>This is an improper distribution on <span class="math inline">\(\theta\)</span> because</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
  \int_0^1 \frac{1}{\theta(1 - \theta)}
  &amp;\ge
  \int_0^1 \frac{1}{\theta}
  \\
  &amp;=
  \log\theta \vert_0^1
  \\
  &amp;=
  \infty.
\end{align}
\]</span></p>
<p>When <span class="math inline">\(y = 0\)</span>, then the posterior distribution is <span class="math inline">\(p(\theta \mid y = 0) \propto (1 - \theta)^{n - 1}\theta^{-1}\)</span>. When <span class="math inline">\(y = n\)</span>, then the posterior distribution is <span class="math inline">\(p(\theta \mid y = n) \propto \theta^{n-1}(1 - \theta)^{-1}\)</span>. These two cases are equivalent by the change of variable <span class="math inline">\(\theta \mapsto 1 - \theta\)</span>.</p>
<p>We show that the distribution is improper for <span class="math inline">\(y = 0\)</span> by induction. The case <span class="math inline">\(n = 0\)</span> is shown above (for the prior). Assume the distribution is improper for any integer <span class="math inline">\(k &lt; n\)</span>. Then</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
\int_0^1 \theta^{- 1}(1 - \theta)^{n - 1} d\theta
&amp;=
\int_0^1 \theta^{- 1}(1 - \theta)^{n - 2} \cdot (1 - \theta)d\theta
\\
&amp;=
\left[(1 - \theta)^{n-2}(1 - \frac{\theta}{2}) \right]_0^1
+
\int_0^1 \frac{(1 - \theta)^{n - 2}}{\theta}
+
(n-2)(1 - \theta)^{n-3}
-
\frac{(1 - \theta)^{n-2}}{2}
-
(n - 2)\theta\frac{(1 - \theta)^{n-3}}{2}
d\theta
\\
&amp;=
c
+
\int_0^1 \frac{(1 - \theta)^{n - 2}}{\theta} d\theta,
\end{align}
\]</span></p>
<p>where <span class="math inline">\(c &lt; \infty\)</span>. By the induction hypothesis, the integral on the last line is <span class="math inline">\(\infty\)</span>. Therefore, the distribution is also improper for <span class="math inline">\(n\)</span>.</p>

<div id="disqus_thread"></div>
<script>
/**
 *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
 *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
 */
/*
   var disqus_config = function () {
   this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
   this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
   };
 */
(function() {  // DON'T EDIT BELOW THIS LINE
 var d = document, s = d.createElement('script');

 s.src = '//stappit-github-io.disqus.com/embed.js';

 s.setAttribute('data-timestamp', +new Date());
 (d.head || d.body).appendChild(s);
 })();
</script>

<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

<script id="dsq-count-scr" src="//stappit-github-io.disqus.com/count.js" async></script>
]]></summary>
</entry>
<entry>
    <title>BDA3 Chapter 2 Exercise 6</title>
    <link href="http://stappit.github.io/posts/bda3/chapter_02_exercise_06.html" />
    <id>http://stappit.github.io/posts/bda3/chapter_02_exercise_06.html</id>
    <published>2018-08-25T00:00:00Z</published>
    <updated>2018-08-25T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 class="post-title">BDA3 Chapter 2 Exercise 6</h1>

<div class="card post-meta border-0">
  <div class="card-body post-meta">
    <p class="card-text text-muted text-left">
    Posted on August 25, 2018  by Brian </br>
     Tags: <a href="/tags/bda%20chapter%202.html">bda chapter 2</a>, <a href="/tags/bda.html">bda</a>, <a href="/tags/solutions.html">solutions</a>, <a href="/tags/bayes.html">bayes</a>, <a href="/tags/poisson.html">poisson</a>, <a href="/tags/gamma.html">gamma</a>, <a href="/tags/negative%20binomial.html">negative binomial</a> </br>
     Category: <a href="/categories/bda3.html">bda3</a> 
    </p>
  </div>
</div>

<p>Here’s my solution to exercise 6, chapter 2, of <a href="https://andrewgelman.com/">Gelman’s</a> <em>Bayesian Data Analysis</em> (BDA), 3rd edition. There are <a href="http://www.stat.columbia.edu/~gelman/book/solutions.pdf">solutions</a> to some of the exercises on the <a href="http://www.stat.columbia.edu/~gelman/book/">book’s webpage</a>.</p>
<!--more-->
<div style="display:none">
<p class="mathjaxWide"><span class="math inline">\(\DeclareMathOperator{\dbinomial}{binomial} \DeclareMathOperator{\dbern}{Bernoulli} \DeclareMathOperator{\dgamma}{gamma} \DeclareMathOperator{\dpois}{Poisson} \DeclareMathOperator{\dbeta}{beta}\)</span></p>
</div>
<p>Considering the negative binomial variable <span class="math inline">\(y\)</span> as a gamma-Poisson variable, we derive expressions for the mean and variance.</p>
<p>From equation 1.6, <span class="math inline">\(\mathbb E (y) = \mathbb E (\mathbb E(y \mid \theta))\)</span>. Since <span class="math inline">\(y \mid \theta \sim \dpois(10n\theta)\)</span>, it follows that <span class="math inline">\(\mathbb E (y \mid \theta) = 10n\theta\)</span>. The rate <span class="math inline">\(\theta \sim \dgamma(\alpha, \beta)\)</span> so <span class="math inline">\(\mathbb E(\theta) = \frac{\alpha}{\beta}\)</span>. Thus, <span class="math inline">\(\mathbb E(y) = 10n\mathbb E(\theta) = 10n \frac{\alpha}{\beta}\)</span>.</p>
<p>We also have <span class="math inline">\(\mathbb V(\theta) = \frac{\alpha}{\beta^2}\)</span> since <span class="math inline">\(\theta \sim \dgamma(\alpha, \beta)\)</span>, and <span class="math inline">\(\mathbb V(y \mid \theta) = 10n\theta\)</span> since <span class="math inline">\(y \mid \theta \sim \dpois(10n\theta)\)</span>. Thus,</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
  \mathbb V (y) 
  &amp;= 
  \mathbb E(\mathbb V(y \mid \theta)) + \mathbb V (\mathbb E (y \mid \theta)) 
  \\
  &amp;=
  \mathbb E(10n\theta) + \mathbb V (10n\theta)
  \\
  &amp;=
  10n\frac{\alpha}{\beta} + (10n)^2\frac{\alpha}{\beta^2}
  \qquad \square
\end{align}
\]</span></p>

<div id="disqus_thread"></div>
<script>
/**
 *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
 *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
 */
/*
   var disqus_config = function () {
   this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
   this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
   };
 */
(function() {  // DON'T EDIT BELOW THIS LINE
 var d = document, s = d.createElement('script');

 s.src = '//stappit-github-io.disqus.com/embed.js';

 s.setAttribute('data-timestamp', +new Date());
 (d.head || d.body).appendChild(s);
 })();
</script>

<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

<script id="dsq-count-scr" src="//stappit-github-io.disqus.com/count.js" async></script>
]]></summary>
</entry>
<entry>
    <title>BDA3 Chapter 2 Exercise 5</title>
    <link href="http://stappit.github.io/posts/bda3/chapter_02_exercise_05.html" />
    <id>http://stappit.github.io/posts/bda3/chapter_02_exercise_05.html</id>
    <published>2018-08-24T00:00:00Z</published>
    <updated>2018-08-24T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 class="post-title">BDA3 Chapter 2 Exercise 5</h1>

<div class="card post-meta border-0">
  <div class="card-body post-meta">
    <p class="card-text text-muted text-left">
    Posted on August 24, 2018  by Brian </br>
     Tags: <a href="/tags/bayes.html">bayes</a>, <a href="/tags/solutions.html">solutions</a>, <a href="/tags/bda%20chapter%202.html">bda chapter 2</a>, <a href="/tags/bda.html">bda</a>, <a href="/tags/beta.html">beta</a>, <a href="/tags/binomial.html">binomial</a>, <a href="/tags/beta-binomial.html">beta-binomial</a>, <a href="/tags/variance.html">variance</a> </br>
     Category: <a href="/categories/bda3.html">bda3</a> 
    </p>
  </div>
</div>

<p>Here’s my solution to exercise 5, chapter 2, of <a href="https://andrewgelman.com/">Gelman’s</a> <em>Bayesian Data Analysis</em> (BDA), 3rd edition. There are <a href="http://www.stat.columbia.edu/~gelman/book/solutions.pdf">solutions</a> to some of the exercises on the <a href="http://www.stat.columbia.edu/~gelman/book/">book’s webpage</a>.</p>
<!--more-->
<div style="display:none">
<p class="mathjaxWide"><span class="math inline">\(\DeclareMathOperator{\dbinomial}{binomial} \DeclareMathOperator{\dbern}{Bernoulli} \DeclareMathOperator{\dbeta}{beta}\)</span></p>
</div>
<p>Let’s derive the prior predictive distribution of a beta-binomial model with a uniform prior. See <a href="https://math.stackexchange.com/questions/122296/how-to-evaluate-this-integral-relating-to-binomial">stackexchange</a> and <a href="https://en.wikipedia.org/wiki/Beta_function">wikipedia</a> for useful results for solving the integral below.</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
  p(y = k)
  &amp;=
  \int_0^1 p(y = k \mid \theta) p(\theta) d\theta
  \\
  &amp;=
  \binom{n}{k} \cdot \int_0^1 \theta^k (1 - \theta)^{n - k} d\theta
  \\
  &amp;=
  \binom{n}{k} \cdot \frac{1}{\binom{n}{k} \cdot (n + 1)}
  \\
  &amp;=
  \frac{1}{n + 1}
\end{align}
\]</span></p>
<p>Now let’s show that the posterior mean of <span class="math inline">\(\theta\)</span> lies between the prior mean and observed frequency. The posterior is</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
  p(\theta \mid y)
  &amp;\propto
  p(y \mid \theta) \cdot p(\theta)
  \\
  &amp;\propto
  \theta^y (1 - \theta)^{n - y}\cdot \theta^{\alpha - 1} (1 - \theta)^{\beta - 1}
  \\
  &amp;=
  \theta^{y + \alpha - 1} (1 - \theta)^{n + \beta - y - 1}.
\end{align}
\]</span></p>
<p>So <span class="math inline">\(p(\theta \mid y) \sim \dbeta(y + \alpha, n - y + \beta)\)</span>, which has mean <span class="math inline">\(\frac{y + \alpha}{n + \alpha + \beta}\)</span>. Suppose <span class="math inline">\(\frac{y}{n} \le \frac{\alpha}{\alpha + \beta}\)</span>. Then</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
  \frac{y}{n}
  &amp;\le
  \frac{y + \alpha}{n + \alpha + \beta}
  \\
  \Leftrightarrow
  y(n + \alpha + \beta)
  &amp;\le
  n(y + \alpha)
  \\
  \Leftrightarrow
  y(\alpha + \beta)
  &amp;\le
  n\alpha
  \\
  \Leftrightarrow
  \frac{y}{n} 
  &amp;\le 
  \frac{\alpha}{\alpha + \beta}
\end{align}
\]</span></p>
<p>A similar argument shows that <span class="math inline">\(\frac{y + \alpha}{n + \alpha + \beta} \le \frac{\alpha}{\alpha + \beta}\)</span>.</p>
<p>If <span class="math inline">\(\frac{y}{n} \ge \frac{\alpha}{\alpha + \beta}\)</span>, then the analogous argument shows that <span class="math inline">\(\frac{\alpha}{\alpha + \beta} \le \frac{y + \alpha}{n + \alpha + \beta} \le \frac{y}{n}. \square\)</span></p>
<p>The prior variance is <span class="math inline">\(\mathbb V (\theta) = \frac{\alpha \beta}{(\alpha + \beta)^2(\alpha + \beta + 1)}\)</span>. For a uniform prior this is <span class="math inline">\(\frac{1}{4 \cdot 3} = \frac{1}{12}\)</span>. The posterior variance with a uniform prior is <span class="math inline">\(\frac{y + 1}{n + 2} \cdot \frac{n - y + 1}{n + 2} \cdot \frac{1}{n + 3}\)</span>. For <span class="math inline">\(p \in [0, 1]\)</span>, the function <span class="math inline">\(p \mapsto p(1 - p)\)</span> is maximised when <span class="math inline">\(p = 0.5\)</span>. Thus for fixed <span class="math inline">\(n\)</span>, the posterior variance is maximised when <span class="math inline">\(y = \frac{n}{2}\)</span>. This means that the posterior variance is at most <span class="math inline">\(\frac{1}{4} \cdot \frac{1}{n + 3} \le \frac{1}{4n + 12} \le \frac{1}{12}. \square\)</span></p>
<p>Intuitively, the posterior variance should be larger than the prior variance when the observed data is different from what would be expected from the prior distribution. (This can’t happen with a uniform prior because every value is equally likely). Indeed, with prior <span class="math inline">\(\theta \sim \dbeta(1, 9)\)</span> and observed data <span class="math inline">\(y = 9, n = 10\)</span>, we have <span class="math inline">\(\mathbb V(\theta) = \frac{9}{1100}\)</span> and <span class="math inline">\(\mathbb V(\theta \mid y) = \frac{1}{2} \cdot \frac{1}{2} \cdot \frac{1}{21} = \frac{1}{84}\)</span>.</p>

<div id="disqus_thread"></div>
<script>
/**
 *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
 *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
 */
/*
   var disqus_config = function () {
   this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
   this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
   };
 */
(function() {  // DON'T EDIT BELOW THIS LINE
 var d = document, s = d.createElement('script');

 s.src = '//stappit-github-io.disqus.com/embed.js';

 s.setAttribute('data-timestamp', +new Date());
 (d.head || d.body).appendChild(s);
 })();
</script>

<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

<script id="dsq-count-scr" src="//stappit-github-io.disqus.com/count.js" async></script>
]]></summary>
</entry>
<entry>
    <title>BDA3 Chapter 2 Exercise 4</title>
    <link href="http://stappit.github.io/posts/bda3/chapter_02_exercise_04.html" />
    <id>http://stappit.github.io/posts/bda3/chapter_02_exercise_04.html</id>
    <published>2018-08-23T00:00:00Z</published>
    <updated>2018-08-23T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 class="post-title">BDA3 Chapter 2 Exercise 4</h1>

<div class="card post-meta border-0">
  <div class="card-body post-meta">
    <p class="card-text text-muted text-left">
    Posted on August 23, 2018  by Brian </br>
     Tags: <a href="/tags/binomial.html">binomial</a>, <a href="/tags/bayes.html">bayes</a>, <a href="/tags/solutions.html">solutions</a>, <a href="/tags/bda%20chapter%202.html">bda chapter 2</a>, <a href="/tags/bda.html">bda</a>, <a href="/tags/normal%20approximation.html">normal approximation</a>, <a href="/tags/multi-modal.html">multi-modal</a> </br>
     Category: <a href="/categories/bda3.html">bda3</a> 
    </p>
  </div>
</div>

<p>Here’s my solution to exercise 4, chapter 2, of <a href="https://andrewgelman.com/">Gelman’s</a> <em>Bayesian Data Analysis</em> (BDA), 3rd edition. There are <a href="http://www.stat.columbia.edu/~gelman/book/solutions.pdf">solutions</a> to some of the exercises on the <a href="http://www.stat.columbia.edu/~gelman/book/">book’s webpage</a>.</p>
<!--more-->
<div style="display:none">
<p class="mathjaxWide"><span class="math inline">\(\DeclareMathOperator{\dbinomial}{binomial} \DeclareMathOperator{\dbern}{Bernoulli} \DeclareMathOperator{\dbeta}{beta}\)</span></p>
</div>
<p>Consider 1000 rolls of an unfair die, where the probability of a 6 is either 1/4, 1/6, or 1/12. Let’s draw the distribution and the normal approximation.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">N &lt;-<span class="st"> </span><span class="dv">1000</span>
p6 &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span> <span class="op">/</span><span class="st"> </span><span class="dv">4</span>, <span class="dv">1</span> <span class="op">/</span><span class="st"> </span><span class="dv">6</span>, <span class="dv">1</span> <span class="op">/</span><span class="st"> </span><span class="dv">12</span>)

ex4 &lt;-<span class="st"> </span><span class="kw">expand.grid</span>(
    <span class="dt">y =</span> <span class="kw">seq</span>(<span class="dv">0</span>, N),
    <span class="dt">theta =</span> p6
  ) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(
    <span class="dt">mu =</span> N <span class="op">*</span><span class="st"> </span>theta,
    <span class="dt">sigma =</span> <span class="kw">sqrt</span>(N <span class="op">*</span><span class="st"> </span>theta <span class="op">*</span><span class="st"> </span>(<span class="dv">10</span> <span class="op">-</span><span class="st"> </span>theta)),
    <span class="dt">binomial =</span> <span class="kw">dbinom</span>(y, N, theta),
    <span class="dt">normal_approx =</span> <span class="kw">dnorm</span>(y, mu, sigma),
    <span class="dt">theta =</span> scales<span class="op">::</span><span class="kw">percent</span>(<span class="kw">signif</span>(theta))
  ) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">select</span>(<span class="op">-</span>mu, <span class="op">-</span>sigma) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">gather</span>(distribution, probability, binomial, normal_approx) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">spread</span>(theta, probability) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">prior_probability =</span> <span class="fl">0.25</span> <span class="op">*</span><span class="st"> `</span><span class="dt">8.3%</span><span class="st">`</span> <span class="op">+</span><span class="st"> </span><span class="fl">0.5</span> <span class="op">*</span><span class="st"> `</span><span class="dt">16.7%</span><span class="st">`</span> <span class="op">+</span><span class="st"> </span><span class="fl">0.25</span> <span class="op">*</span><span class="st"> `</span><span class="dt">25.0%</span><span class="st">`</span>)</code></pre></div>
<table class="table" style="margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:right;">
y
</th>
<th style="text-align:left;">
distribution
</th>
<th style="text-align:right;">
16.7%
</th>
<th style="text-align:right;">
25.0%
</th>
<th style="text-align:right;">
8.3%
</th>
<th style="text-align:right;">
prior_probability
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
0
</td>
<td style="text-align:left;">
binomial
</td>
<td style="text-align:right;">
0.0e+00
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0.0000000
</td>
<td style="text-align:right;">
0.00e+00
</td>
</tr>
<tr>
<td style="text-align:right;">
0
</td>
<td style="text-align:left;">
normal_approx
</td>
<td style="text-align:right;">
2.1e-06
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0.0002078
</td>
<td style="text-align:right;">
5.30e-05
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
binomial
</td>
<td style="text-align:right;">
0.0e+00
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0.0000000
</td>
<td style="text-align:right;">
0.00e+00
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
normal_approx
</td>
<td style="text-align:right;">
2.3e-06
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0.0002297
</td>
<td style="text-align:right;">
5.86e-05
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:left;">
binomial
</td>
<td style="text-align:right;">
0.0e+00
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0.0000000
</td>
<td style="text-align:right;">
0.00e+00
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:left;">
normal_approx
</td>
<td style="text-align:right;">
2.5e-06
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0.0002536
</td>
<td style="text-align:right;">
6.47e-05
</td>
</tr>
</tbody>
</table>
<figure>
<img src="chapter_02_exercise_04_files/figure-markdown/ex4_plot-1.png" />
</figure>
<p>The normal approximation underestimates the maxima and overestimates the values between the maxima. From the percentiles in the table below, we see that the normal approximation is best near the median but becomes gradually worse towards towards both extremes.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">percentiles &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.05</span>, <span class="fl">0.25</span>, <span class="fl">0.5</span>, <span class="fl">0.75</span>, <span class="fl">0.95</span>)

ex4 <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(distribution) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">arrange</span>(y) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(
    <span class="dt">cdf =</span> <span class="kw">cumsum</span>(prior_probability),
    <span class="dt">percentile =</span> <span class="kw">case_when</span>(
      cdf <span class="op">&lt;=</span><span class="st"> </span><span class="fl">0.05</span> <span class="op">~</span><span class="st"> &#39;05%&#39;</span>,
      cdf <span class="op">&lt;=</span><span class="st"> </span><span class="fl">0.25</span> <span class="op">~</span><span class="st"> &#39;25%&#39;</span>,
      cdf <span class="op">&lt;=</span><span class="st"> </span><span class="fl">0.50</span> <span class="op">~</span><span class="st"> &#39;50%&#39;</span>,
      cdf <span class="op">&lt;=</span><span class="st"> </span><span class="fl">0.75</span> <span class="op">~</span><span class="st"> &#39;75%&#39;</span>,
      cdf <span class="op">&lt;=</span><span class="st"> </span><span class="fl">0.95</span> <span class="op">~</span><span class="st"> &#39;95%&#39;</span>
    )
  ) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">filter</span>(cdf <span class="op">&lt;=</span><span class="st"> </span><span class="fl">0.95</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(distribution, percentile) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">slice</span>(<span class="kw">which.max</span>(cdf)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">select</span>(distribution, percentile, y) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">spread</span>(distribution, y) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">arrange</span>(percentile) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">kable</span>() <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">kable_styling</span>()</code></pre></div>
<table class="table" style="margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
percentile
</th>
<th style="text-align:right;">
binomial
</th>
<th style="text-align:right;">
normal_approx
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
05%
</td>
<td style="text-align:right;">
75
</td>
<td style="text-align:right;">
58
</td>
</tr>
<tr>
<td style="text-align:left;">
25%
</td>
<td style="text-align:right;">
119
</td>
<td style="text-align:right;">
110
</td>
</tr>
<tr>
<td style="text-align:left;">
50%
</td>
<td style="text-align:right;">
166
</td>
<td style="text-align:right;">
164
</td>
</tr>
<tr>
<td style="text-align:left;">
75%
</td>
<td style="text-align:right;">
206
</td>
<td style="text-align:right;">
214
</td>
</tr>
<tr>
<td style="text-align:left;">
95%
</td>
<td style="text-align:right;">
260
</td>
<td style="text-align:right;">
291
</td>
</tr>
</tbody>
</table>

<div id="disqus_thread"></div>
<script>
/**
 *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
 *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
 */
/*
   var disqus_config = function () {
   this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
   this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
   };
 */
(function() {  // DON'T EDIT BELOW THIS LINE
 var d = document, s = d.createElement('script');

 s.src = '//stappit-github-io.disqus.com/embed.js';

 s.setAttribute('data-timestamp', +new Date());
 (d.head || d.body).appendChild(s);
 })();
</script>

<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

<script id="dsq-count-scr" src="//stappit-github-io.disqus.com/count.js" async></script>
]]></summary>
</entry>
<entry>
    <title>BDA3 Chapter 2 Exercise 3</title>
    <link href="http://stappit.github.io/posts/bda3/chapter_02_exercise_03.html" />
    <id>http://stappit.github.io/posts/bda3/chapter_02_exercise_03.html</id>
    <published>2018-08-22T00:00:00Z</published>
    <updated>2018-08-22T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 class="post-title">BDA3 Chapter 2 Exercise 3</h1>

<div class="card post-meta border-0">
  <div class="card-body post-meta">
    <p class="card-text text-muted text-left">
    Posted on August 22, 2018  by Brian </br>
     Tags: <a href="/tags/binomial.html">binomial</a>, <a href="/tags/bayes.html">bayes</a>, <a href="/tags/solutions.html">solutions</a>, <a href="/tags/bda%20chapter%202.html">bda chapter 2</a>, <a href="/tags/bda.html">bda</a>, <a href="/tags/normal%20approximation.html">normal approximation</a> </br>
     Category: <a href="/categories/bda3.html">bda3</a> 
    </p>
  </div>
</div>

<p>Here’s my solution to exercise 3, chapter 2, of <a href="https://andrewgelman.com/">Gelman’s</a> <em>Bayesian Data Analysis</em> (BDA), 3rd edition. There are <a href="http://www.stat.columbia.edu/~gelman/book/solutions.pdf">solutions</a> to some of the exercises on the <a href="http://www.stat.columbia.edu/~gelman/book/">book’s webpage</a>.</p>
<!--more-->
<div style="display:none">
<p class="mathjaxWide"><span class="math inline">\(\DeclareMathOperator{\dbinomial}{binomial} \DeclareMathOperator{\dbern}{Bernoulli} \DeclareMathOperator{\dbeta}{beta}\)</span></p>
</div>
<p>For 1000 rolls of a fair die, The mean number of sixs is 1000/6 = 166.667, the variance is 138.889, and the standard deviation is 11.7851.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">N &lt;-<span class="st"> </span><span class="dv">1000</span>
p &lt;-<span class="st"> </span><span class="dv">1</span> <span class="op">/</span><span class="st"> </span><span class="dv">6</span>
mu &lt;-<span class="st"> </span>N <span class="op">*</span><span class="st"> </span>p
sigma &lt;-<span class="st"> </span><span class="kw">sqrt</span>(N <span class="op">*</span><span class="st"> </span>p <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>p))

ex3 &lt;-<span class="st"> </span><span class="kw">tibble</span>(
    <span class="dt">y =</span> <span class="kw">seq</span>(<span class="dv">0</span>, N),
    <span class="dt">binomial =</span> <span class="kw">dbinom</span>(y, N, p),
    <span class="dt">normal_approx =</span> <span class="kw">dnorm</span>(y, mu, sigma)
  ) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">gather</span>(metric, probability, <span class="op">-</span>y) </code></pre></div>
<table class="table" style="margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:right;">
y
</th>
<th style="text-align:left;">
metric
</th>
<th style="text-align:right;">
probability
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
0
</td>
<td style="text-align:left;">
binomial
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
binomial
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:left;">
binomial
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:right;">
3
</td>
<td style="text-align:left;">
binomial
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:right;">
4
</td>
<td style="text-align:left;">
binomial
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:right;">
5
</td>
<td style="text-align:left;">
binomial
</td>
<td style="text-align:right;">
0
</td>
</tr>
</tbody>
</table>
<figure>
<img src="chapter_02_exercise_03_files/figure-markdown/ex3_plot-1.png" />
</figure>
<p>The two curves are visually indistinguishable. The percentiles are listed in the table below.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">percentiles &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.05</span>, <span class="fl">0.25</span>, <span class="fl">0.5</span>, <span class="fl">0.75</span>, <span class="fl">0.95</span>)

<span class="kw">tibble</span>(
    <span class="dt">percentile =</span> scales<span class="op">::</span><span class="kw">percent</span>(percentiles),
    <span class="dt">binom =</span> <span class="kw">qbinom</span>(percentiles, N, p),
    <span class="dt">norm =</span> <span class="kw">qnorm</span>(percentiles, mu, sigma)
) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">kable</span>() <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">kable_styling</span>()</code></pre></div>
<table class="table" style="margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
percentile
</th>
<th style="text-align:right;">
binom
</th>
<th style="text-align:right;">
norm
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
5%
</td>
<td style="text-align:right;">
147
</td>
<td style="text-align:right;">
147.2819
</td>
</tr>
<tr>
<td style="text-align:left;">
25%
</td>
<td style="text-align:right;">
159
</td>
<td style="text-align:right;">
158.7177
</td>
</tr>
<tr>
<td style="text-align:left;">
50%
</td>
<td style="text-align:right;">
167
</td>
<td style="text-align:right;">
166.6667
</td>
</tr>
<tr>
<td style="text-align:left;">
75%
</td>
<td style="text-align:right;">
175
</td>
<td style="text-align:right;">
174.6156
</td>
</tr>
<tr>
<td style="text-align:left;">
95%
</td>
<td style="text-align:right;">
186
</td>
<td style="text-align:right;">
186.0515
</td>
</tr>
</tbody>
</table>

<div id="disqus_thread"></div>
<script>
/**
 *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
 *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
 */
/*
   var disqus_config = function () {
   this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
   this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
   };
 */
(function() {  // DON'T EDIT BELOW THIS LINE
 var d = document, s = d.createElement('script');

 s.src = '//stappit-github-io.disqus.com/embed.js';

 s.setAttribute('data-timestamp', +new Date());
 (d.head || d.body).appendChild(s);
 })();
</script>

<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

<script id="dsq-count-scr" src="//stappit-github-io.disqus.com/count.js" async></script>
]]></summary>
</entry>
<entry>
    <title>BDA3 Chapter 2 Exercise 2</title>
    <link href="http://stappit.github.io/posts/bda3/chapter_02_exercise_02.html" />
    <id>http://stappit.github.io/posts/bda3/chapter_02_exercise_02.html</id>
    <published>2018-08-21T00:00:00Z</published>
    <updated>2018-08-21T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 class="post-title">BDA3 Chapter 2 Exercise 2</h1>

<div class="card post-meta border-0">
  <div class="card-body post-meta">
    <p class="card-text text-muted text-left">
    Posted on August 21, 2018  by Brian </br>
     Tags: <a href="/tags/stan.html">stan</a>, <a href="/tags/binomial.html">binomial</a>, <a href="/tags/bayes.html">bayes</a>, <a href="/tags/solutions.html">solutions</a>, <a href="/tags/bda%20chapter%202.html">bda chapter 2</a>, <a href="/tags/bda.html">bda</a> </br>
     Category: <a href="/categories/bda3.html">bda3</a> 
    </p>
  </div>
</div>

<p>Here’s my solution to exercise 2, chapter 2, of <a href="https://andrewgelman.com/">Gelman’s</a> <em>Bayesian Data Analysis</em> (BDA), 3rd edition. There are <a href="http://www.stat.columbia.edu/~gelman/book/solutions.pdf">solutions</a> to some of the exercises on the <a href="http://www.stat.columbia.edu/~gelman/book/">book’s webpage</a>.</p>
<!--more-->
<div style="display:none">
<p class="mathjaxWide"><span class="math inline">\(\DeclareMathOperator{\dbinomial}{binomial} \DeclareMathOperator{\dbern}{Bernoulli} \DeclareMathOperator{\dbeta}{beta}\)</span></p>
</div>
<p>We are given the following information about the two coins.</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
  p(C_1) &amp;= 0.5 &amp; p(H \mid C_1) &amp;= \dbern(H \mid 0.6) 
  \\
  p(C_2) &amp;= 0.5 &amp; p(H \mid C_2) &amp;= \dbern(H \mid 0.4)
\end{align}
\]</span></p>
<p>The posterior probability of each coin given two tails is:</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
  p(C_1 \mid TT )
  &amp;\propto
  p(TT \mid C_1) \cdot p(C_1)
  \\
  &amp;=
  \left(\frac{2}{5}\right)^2 \frac{1}{2}
  \\
  &amp;=
  \frac{2}{25}
\end{align}
\]</span> <span class="math display">\[
\begin{align}
  p(C_2 \mid TT )
  &amp;\propto
  p(TT \mid C_2) \cdot p(C_2)
  \\
  &amp;=
  \left(\frac{3}{5}\right)^2 \frac{1}{2}
  \\
  &amp;=
  \frac{9}{50}
\end{align}
\]</span></p>
<p>Both of the previous probabilities are normalised by the same constant. Since <span class="math inline">\(p(C_1 \mid TT) + p(C_2 \mid TT) = 1\)</span>, the normalising constant is <span class="math inline">\(\frac{2}{25} + \frac{9}{50} = \frac{13}{50}\)</span>. Thus</p>
<p class="mathjaxWide"><span class="math display">\[
p(C_1 \mid TT) = \frac{4}{13}
\qquad
\text{and}
\qquad
p(C_2 \mid TT) = \frac{9}{13}.
\]</span></p>
<p>Let <span class="math inline">\(y\)</span> be the number of additional spins until the next head. Conditional on a coin, <span class="math inline">\(y\)</span> is <a href="https://en.wikipedia.org/wiki/Geometric_distribution">geometrically</a> distributed. So the expected number of spins before the next head is:</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
  \mathbb E(y \mid TT)
  &amp;=
  \frac{4}{13}\mathbb E(y \mid C_1)
  +
  \frac{9}{13}\mathbb E(y \mid C_1)
  \\
  &amp;=
  \frac{4}{13}\frac{5}{3}
  +
  \frac{9}{13}\frac{5}{2}
  \\
  &amp;=
  \frac{20}{39}
  +
  \frac{45}{26}
  \\
  &amp;=
  \frac{175}{78},
\end{align}
\]</span></p>
<p>which is 2.24359.</p>

<div id="disqus_thread"></div>
<script>
/**
 *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
 *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
 */
/*
   var disqus_config = function () {
   this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
   this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
   };
 */
(function() {  // DON'T EDIT BELOW THIS LINE
 var d = document, s = d.createElement('script');

 s.src = '//stappit-github-io.disqus.com/embed.js';

 s.setAttribute('data-timestamp', +new Date());
 (d.head || d.body).appendChild(s);
 })();
</script>

<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

<script id="dsq-count-scr" src="//stappit-github-io.disqus.com/count.js" async></script>
]]></summary>
</entry>
<entry>
    <title>BDA3 Chapter 2 Exercise 1</title>
    <link href="http://stappit.github.io/posts/bda3/chapter_02_exercise_01.html" />
    <id>http://stappit.github.io/posts/bda3/chapter_02_exercise_01.html</id>
    <published>2018-08-20T00:00:00Z</published>
    <updated>2018-08-20T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 class="post-title">BDA3 Chapter 2 Exercise 1</h1>

<div class="card post-meta border-0">
  <div class="card-body post-meta">
    <p class="card-text text-muted text-left">
    Posted on August 20, 2018  by Brian </br>
     Tags: <a href="/tags/stan.html">stan</a>, <a href="/tags/beta.html">beta</a>, <a href="/tags/binomial.html">binomial</a>, <a href="/tags/bayes.html">bayes</a>, <a href="/tags/solutions.html">solutions</a>, <a href="/tags/bda%20chapter%202.html">bda chapter 2</a>, <a href="/tags/bda.html">bda</a> </br>
     Category: <a href="/categories/bda3.html">bda3</a> 
    </p>
  </div>
</div>

<p>Here’s my solution to exercise 1, chapter 2, of <a href="https://andrewgelman.com/">Gelman’s</a> <em>Bayesian Data Analysis</em> (BDA), 3rd edition. There are <a href="http://www.stat.columbia.edu/~gelman/book/solutions.pdf">solutions</a> to some of the exercises on the <a href="http://www.stat.columbia.edu/~gelman/book/">book’s webpage</a>.</p>
<!--more-->
<div style="display:none">
<p class="mathjaxWide"><span class="math inline">\(\DeclareMathOperator{\dbinomial}{binomial} \DeclareMathOperator{\dbern}{Bernoulli} \DeclareMathOperator{\dbeta}{beta}\)</span></p>
</div>
<p>Let <span class="math inline">\(H\)</span> be the number of heads in 10 tosses of the coin. With a <span class="math inline">\(\dbeta(4, 4)\)</span> prior on the probability <span class="math inline">\(\theta\)</span> of a head, the posterior after finding out <span class="math inline">\(H \le 2\)</span> is</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
  p(\theta \mid H \le 2)
  &amp;\propto
  p(H \le 2 \mid \theta) \cdot p(\theta)
  \\
  &amp;=
  \dbeta(\theta \mid 4, 4) \sum_{h = 0}^2 \dbinomial(h \mid \theta, 10)
  \\
  &amp;=
  \theta^3 (1 - \theta)^3 \sum_{h = 0}^2 \binom{10}{h} \theta^h (1 - \theta)^{10 - h}.
\end{align}
\]</span></p>
<p>We can plot this unnormalised posterior density from the following dataset.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ex1 &lt;-<span class="st"> </span><span class="kw">tibble</span>(
         <span class="dt">theta =</span> <span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="fl">0.01</span>), 
         <span class="dt">prior =</span> theta<span class="op">^</span><span class="dv">3</span> <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>theta)<span class="op">^</span><span class="dv">3</span>,
         <span class="dt">posterior =</span> prior <span class="op">*</span><span class="st"> </span>(
           <span class="kw">choose</span>(<span class="dv">10</span>, <span class="dv">0</span>) <span class="op">*</span><span class="st"> </span>theta<span class="op">^</span><span class="dv">0</span> <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>theta)<span class="op">^</span><span class="dv">10</span> <span class="op">+</span>
<span class="st">           </span><span class="kw">choose</span>(<span class="dv">10</span>, <span class="dv">1</span>) <span class="op">*</span><span class="st"> </span>theta<span class="op">^</span><span class="dv">1</span> <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>theta)<span class="op">^</span><span class="dv">9</span> <span class="op">+</span>
<span class="st">           </span><span class="kw">choose</span>(<span class="dv">10</span>, <span class="dv">2</span>) <span class="op">*</span><span class="st"> </span>theta<span class="op">^</span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>theta)<span class="op">^</span><span class="dv">8</span> 
         )
       )</code></pre></div>
<figure>
<img src="chapter_02_exercise_01_files/figure-markdown/ex1_plot-1.png" />
</figure>
<p>With the help of <a href="http://mc-stan.org/">Stan</a>, we can obtain the normalised posterior density. We include the information that there are at most 2 heads observed by using the (log) cumulative density function.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m1 &lt;-<span class="st"> </span>rstan<span class="op">::</span><span class="kw">stan_model</span>(<span class="st">&#39;src/ex_02_01.stan&#39;</span>)</code></pre></div>
<pre><code>S4 class stanmodel &#39;ex_02_01&#39; coded as follows:
transformed data {
  int tosses = 10;
  int max_heads = 2;
}

parameters {
  real&lt;lower = 0, upper = 1&gt; theta;
}

model {
  theta ~ beta(4, 4); // prior 
  target += binomial_lcdf(max_heads | tosses, theta); // likelihood
} </code></pre>
<p>The following posterior has the same shape as our exact unnormalised density above. The difference is that we now have a normalised probability distribution without having to work out the maths ourselves.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">f1 &lt;-<span class="st"> </span><span class="kw">sampling</span>(m1, <span class="dt">iter =</span> <span class="dv">40000</span>, <span class="dt">warmup =</span> <span class="dv">500</span>, <span class="dt">chains =</span> <span class="dv">1</span>)</code></pre></div>
<figure>
<img src="chapter_02_exercise_01_files/figure-markdown/ex1_stan_plot-1.png" />
</figure>

<div id="disqus_thread"></div>
<script>
/**
 *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
 *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
 */
/*
   var disqus_config = function () {
   this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
   this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
   };
 */
(function() {  // DON'T EDIT BELOW THIS LINE
 var d = document, s = d.createElement('script');

 s.src = '//stappit-github-io.disqus.com/embed.js';

 s.setAttribute('data-timestamp', +new Date());
 (d.head || d.body).appendChild(s);
 })();
</script>

<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

<script id="dsq-count-scr" src="//stappit-github-io.disqus.com/count.js" async></script>
]]></summary>
</entry>
<entry>
    <title>Understanding the hazard function</title>
    <link href="http://stappit.github.io/posts/survival_models/hazard.html" />
    <id>http://stappit.github.io/posts/survival_models/hazard.html</id>
    <published>2018-08-05T00:00:00Z</published>
    <updated>2018-08-05T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 class="post-title">Understanding the hazard function</h1>

<div class="card post-meta border-0">
  <div class="card-body post-meta">
    <p class="card-text text-muted text-left">
    Posted on August  5, 2018  by Brian </br>
     Tags: <a href="/tags/hazard.html">hazard</a>, <a href="/tags/censoring.html">censoring</a>, <a href="/tags/survival.html">survival</a>, <a href="/tags/exponential.html">exponential</a>, <a href="/tags/poisson.html">poisson</a> </br>
     Category: <a href="/categories/survival_models.html">survival_models</a> 
    </p>
  </div>
</div>

<p>Suppose you have fit a distribution to your censored survival times as in the <a href="./censoring.html">previous post</a> and now want to quantify the intuition of an event being imminent. For example, being able to characterise precisely when your customers are about to churn can help identify problem areas to improve on. This notion is called the <em>hazard</em>. We’ll take a look at some of its main properties and how it related to survival analysis.</p>
<!--more-->
<h2 id="definition">Definition</h2>
<p>Start with a small <span class="math inline">\(\delta\)</span>-interval around t and and consider the average probability density of an event ocurring in that interval given that it occurs after t: <span class="math inline">\(\frac{\mathbb P(t \le T &lt; t + \delta \mid t \le T)}{\delta}\)</span>, where <span class="math inline">\(\mathbb P\)</span> is the probability function. To get rid of the arbitrary choice of <span class="math inline">\(\delta\)</span>, we take the limit as <span class="math inline">\(\delta\)</span> goes to zero.</p>
<p class="mathjaxWide"><span class="math display">\[
h(t)
:=
\lim_{\delta \rightarrow 0} \frac{\mathbb P(t \le T &lt; t + \delta \mid t \le T)}{\delta}
.
\]</span></p>
<p>This is well-defined whenever the CDF is differentiable. Although it is defined in terms of probabilities, we will show below that the hazard is not itself a probability density function.</p>
<h2 id="identities">Identities</h2>
<p>There are a number of useful properties of the hazard function that make it convenient to work with in survival analysis.</p>
<h3 id="equivalent-definition">Equivalent definition</h3>
<p>The above definition helps us understand the intuition behind the hazard function but there’s an equivalent formulation that can be easier to work with. Using</p>
<ul>
<li>the definition of conditional probabilities,</li>
<li>the definition of a derivative, and</li>
<li>that <span class="math inline">\(F&#39;(t) = f(t)\)</span> where <span class="math inline">\(F\)</span> is the CDF and <span class="math inline">\(f\)</span> the probability function,</li>
</ul>
<p>we can show that</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
  h(t)
  &amp;=
  \lim_{\delta \rightarrow 0} \frac{\mathbb P(t \le T &lt; t + \delta \mid t \le T)}{\delta}
  \\
  &amp;=\lim_{\delta \rightarrow 0} \frac{\mathbb P(t \le T &lt; t + \delta)}{\mathbb P(t \le T)\delta}
  \\
  &amp;=\lim_{\delta \rightarrow 0} \frac{F(t + \delta) - F(t)}{S(t)\delta}
  \\
  &amp;=\lim_{\delta \rightarrow 0} \frac{F(t + \delta) - F(t)}{\delta} \frac{1}{S(t)}
  \\
  &amp;= F&#39;(t) \frac{1}{S(t)}
  \\
  &amp;= \frac{f(t)}{S(t)}.
\end{align}
\]</span></p>
<p>We will show below how to use this to simplify the likelihood in the case of censored observations for measuring time to an event of interest.</p>
<h3 id="relation-with-the-survival-function">Relation with the survival function</h3>
<p>Using the identity above, we can rewrite the hazard as a derivative of the survival function:</p>
<p class="mathjaxWide"><span class="math display">\[
h(t)
=
\frac{f(t)}{S(t)}
=
-\frac{d}{dt} \log S(t)
.
\]</span></p>
<p>It then follows from the <a href="https://en.wikipedia.org/wiki/Fundamental_theorem_of_calculus">first fundamental theorem of calculus</a> that</p>
<p class="mathjaxWide"><span class="math display">\[
S(t) = e^{-\int_0^t h(s) ds}.
\]</span></p>
<p>In other words, the hazard function completely determines the survival function (and therefore also the mass/density function).</p>
<p>Since the integral of the hazard appears in the above equation, we can give it a definition for easier reference. We define the <em>cumulative hazard</em> as</p>
<p class="mathjaxWide"><span class="math display">\[
H(t) 
:=
\int_0^t h(s) ds
.
\]</span></p>
<p>Since <span class="math inline">\(\lim_{t \rightarrow \infty} S(t) = 0\)</span>, it follows that <span class="math inline">\(\lim_{t \rightarrow \infty} H(t) = -\lim \log S(t) = \infty\)</span>. In particular, this means that the hazard function is NOT a probability density function!</p>
<h3 id="example">Example</h3>
<p>The exponential distribution has constant hazard. To see this, suppose <span class="math inline">\(h(t) = \lambda\)</span>. Then <span class="math inline">\(S(t) = \exp(-\int_0^t \lambda ds) = \exp(-\lambda t)\)</span> so that <span class="math inline">\(f(t) = -S&#39;(t) = \lambda \exp(-\lambda t)\)</span>, which is the probability function for the <a href="https://en.wikipedia.org/wiki/Exponential_distribution">exponential distribution</a>.</p>
<h2 id="hazard-in-censored-survival-analysis">Hazard in censored survival analysis</h2>
<p>In <a href="./censoring.html">the previous post</a>, we motivated the following likelihood in the case of censored survival times:</p>
<p class="mathjaxWide"><span class="math display">\[
L(\theta) 
:= 
\prod_{i = 1}^N \delta_i f(t_i \mid \theta)
\times
\prod_{i = 1}^N (1 - \delta_i) S(t_i \mid \theta)
\]</span></p>
<p>where <span class="math inline">\(\delta_i\)</span> is 1 if the event is observed and 0 if it is censored, and <span class="math inline">\(\theta\)</span> is the vector of parameters of the distribution of survival times. Since <span class="math inline">\(f(t) = h(t) S(t)\)</span>, we can rewrite this as</p>
<p class="mathjaxWide"><span class="math display">\[
L(\theta) 
:= 
\prod_{i = 1}^N h(t_i \mid \theta)^{\delta_i}S(t_i \mid \theta).
\]</span></p>
<h3 id="example-1">Example</h3>
<p>Assuming that survival times follow an exponential distribution, the hazard <span class="math inline">\(h(t) = \lambda\)</span> is constant, and the likelihood is</p>
<p class="mathjaxWide"><span class="math display">\[
  L(\lambda)
  = 
  \prod_{i = 1}^N \lambda^{\delta_i} e^{-\lambda t_i}
  =
  \lambda^D e^{-\lambda T}
\]</span></p>
<p>where <span class="math inline">\(D := \sum_1^N \delta_i\)</span> is the total number of events observed and <span class="math inline">\(T := \sum_1^N t_i\)</span> is the total observation time. This expression has an interesting interpretation as a <a href="https://en.wikipedia.org/wiki/Poisson_distribution">Poisson</a> likelihood. To see this, first note that <span class="math inline">\(T\)</span> and <span class="math inline">\(D\)</span> can be considered constant in our likelihood because they don’t depend on our only parameter <span class="math inline">\(\lambda\)</span>. We can consider <span class="math inline">\(D\)</span> as a Poisson variable with rate <span class="math inline">\(\lambda\)</span> and exposure <span class="math inline">\(T\)</span>:</p>
<p class="mathjaxWide"><span class="math display">\[
D \mid \lambda \sim \text{Poisson}(\lambda T),
\]</span></p>
<p>which gives probability of observing <span class="math inline">\(D\)</span> events as</p>
<p class="mathjaxWide"><span class="math display">\[
\text{Poisson}(D \mid \lambda T)
=
(\lambda T)^D e^{-\lambda T}
=
T^D L(\lambda)
.
\]</span></p>
<p>However, likelihoods are equivalent up to a multiplicative constant. Since <span class="math inline">\(T^D\)</span> is constant we can treat our likelihood, <span class="math inline">\(L\)</span>, as Poisson.</p>

<div id="disqus_thread"></div>
<script>
/**
 *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
 *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
 */
/*
   var disqus_config = function () {
   this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
   this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
   };
 */
(function() {  // DON'T EDIT BELOW THIS LINE
 var d = document, s = d.createElement('script');

 s.src = '//stappit-github-io.disqus.com/embed.js';

 s.setAttribute('data-timestamp', +new Date());
 (d.head || d.body).appendChild(s);
 })();
</script>

<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

<script id="dsq-count-scr" src="//stappit-github-io.disqus.com/count.js" async></script>
]]></summary>
</entry>
<entry>
    <title>How to deal with right-censored observations</title>
    <link href="http://stappit.github.io/posts/survival_models/censoring.html" />
    <id>http://stappit.github.io/posts/survival_models/censoring.html</id>
    <published>2018-08-04T00:00:00Z</published>
    <updated>2018-08-04T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 class="post-title">How to deal with right-censored observations</h1>

<div class="card post-meta border-0">
  <div class="card-body post-meta">
    <p class="card-text text-muted text-left">
    Posted on August  4, 2018  by Brian </br>
     Tags: <a href="/tags/censoring.html">censoring</a>, <a href="/tags/stan.html">stan</a>, <a href="/tags/likelihood.html">likelihood</a>, <a href="/tags/survival.html">survival</a>, <a href="/tags/mle.html">mle</a>, <a href="/tags/poisson.html">poisson</a> </br>
     Category: <a href="/categories/survival_models.html">survival_models</a> 
    </p>
  </div>
</div>

<p>I’ve recently been interested in understanding survival models, which model the time to an event of interest (<code>tte</code>) but where we are not always able to wait until that event occurs. This happens, for example, when modelling the time until a customer <a href="https://en.wikipedia.org/wiki/Churn_rate">churns</a>: some of your customers may have cancelled their subscriptions but many hopefully haven’t. Those that haven’t are said to be <code>censored</code> because we haven’t observed them cancel their subscription yet.</p>
<!--more-->
<p>As a first step in that direction, we’ll take a look at modelling censoring when the <code>tte</code> has a Poisson distribution (minor modifications can be made to extend to other distributions). We’ll use <a href="http://mc-stan.org/">Stan</a> to implement our model since Bayesian notation very nicely reflects our statistical understanding. Don’t worry if you aren’t familiar with Stan or Bayesian inference - it should be possible to follow along regardless.</p>
<p>You can download the <a href="./censoring.Rmd">R markdown</a> and the <a href="./censored_poisson.stan">stan model</a> to try it out.</p>
<h2 id="some-theory">Some theory</h2>
<h3 id="the-problem">The Problem</h3>
<p>Let’s generate some data. We will assume that the time to event (<code>tte</code>) is poisson distributed with mean <span class="math inline">\(\mu = 10\)</span>. However, we will also assume that we don’t get to observe the event of interest in every case, i.e. some cases are censored. What we measure is the time to observation (<code>tto</code>).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">N &lt;-<span class="st"> </span><span class="dv">10000</span>
mu &lt;-<span class="st"> </span><span class="dv">10</span>

df &lt;-<span class="st"> </span><span class="kw">tibble</span>(
  <span class="dt">id =</span> <span class="dv">1</span><span class="op">:</span>N,
  <span class="dt">tte =</span> <span class="kw">rpois</span>(N, mu),
  <span class="dt">tto =</span> <span class="kw">pmin</span>(<span class="kw">rpois</span>(N, <span class="dv">12</span>), tte),
  <span class="dt">censored =</span> tto <span class="op">&lt;</span><span class="st"> </span>tte
)

df <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">head</span>() <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">kable</span>() <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">kable_styling</span>()</code></pre></div>
<table class="table" style="margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:right;">
id
</th>
<th style="text-align:right;">
tte
</th>
<th style="text-align:right;">
tto
</th>
<th style="text-align:left;">
censored
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
12
</td>
<td style="text-align:right;">
11
</td>
<td style="text-align:left;">
TRUE
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
7
</td>
<td style="text-align:right;">
7
</td>
<td style="text-align:left;">
FALSE
</td>
</tr>
<tr>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:left;">
FALSE
</td>
</tr>
<tr>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
13
</td>
<td style="text-align:right;">
8
</td>
<td style="text-align:left;">
TRUE
</td>
</tr>
<tr>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:left;">
FALSE
</td>
</tr>
<tr>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
15
</td>
<td style="text-align:right;">
8
</td>
<td style="text-align:left;">
TRUE
</td>
</tr>
</tbody>
</table>
<figure>
<img src="censoring_files/figure-markdown/censoring_count-1.png" />
</figure>
<p>Note that we observe <code>tto</code> but not <code>tte</code>. How might we estimate <span class="math inline">\(\mu\)</span>? One way is to take the mean.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">df <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">select</span>(tte, tto) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">summarise_all</span>(mean) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">kable</span>() <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">kable_styling</span>()</code></pre></div>
<table class="table" style="margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:right;">
tte
</th>
<th style="text-align:right;">
tto
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
9.9916
</td>
<td style="text-align:right;">
8.9525
</td>
</tr>
</tbody>
</table>
<p>This estimate is fairly good for <code>tte</code> but is too low for <code>tto</code>. This was to be expected because we know that <code>tto</code> is smaller than <code>tte</code> for censored observations.</p>
<p>It’s not possible to just filter out the censored values as this also gives biased estimates. In fact, it makes our estimate worse in this case.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">df <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">filter</span>(<span class="op">!</span>censored) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">select</span>(tte, tto) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">summarise_all</span>(mean) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">kable</span>() <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">kable_styling</span>()</code></pre></div>
<table class="table" style="margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:right;">
tte
</th>
<th style="text-align:right;">
tto
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
8.916655
</td>
<td style="text-align:right;">
8.916655
</td>
</tr>
</tbody>
</table>
<h3 id="a-more-sophisticated-way-to-be-wrong">A more sophisticated way to be wrong</h3>
<p>So how can we estimate μ using just <code>tto</code>? The first step is to reinterperet the mean as the estimator that maximises a <a href="https://khakieconomics.github.io/2018/07/14/What-is-a-likelihood-anyway.html">likelihood</a> (the maximum likelihood estimator, or MLE). The likelihood is defined as the probability of the data given the estimate. Under the true model, this probability is <span class="math inline">\(f(tte_i \mid \mu) = \text{Poisson}(tte_i \mid \mu)\)</span> for the <span class="math inline">\(i\)</span>th case, giving the likelihood of the whole dataset as:</p>
<p class="mathjaxWide"><span class="math display">\[
L(\mu) := \prod_{i = 1}^N f(tte_i | \mu)
.
\]</span></p>
<p>The mean maximises this likelihood, which is why the mean of <code>tte</code> is close to the true value.</p>
<p>To further illustrate this point, note that this is the estimate we get when regressing <code>tte</code> on a constant.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">df <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">glm</span>(
    <span class="dt">formula =</span> tte <span class="op">~</span><span class="st"> </span><span class="dv">1</span>,
    <span class="dt">family =</span> <span class="kw">poisson</span>(<span class="dt">link =</span> <span class="st">&#39;log&#39;</span>),
    <span class="dt">data =</span> .
  ) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">tidy</span>() <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">pull</span>(estimate) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">exp</span>()</code></pre></div>
<pre><code>[1] 9.9916</code></pre>
<p>However, we don’t observe <code>tte</code>; we observe <code>tto</code>. Simply replacing <code>tte</code> with <code>tto</code> and maximising</p>
<p class="mathjaxWide"><span class="math display">\[
L(\mu) := \prod_{i = 1}^N f(tto_i \mid \mu)
\]</span></p>
<p>gives us the mean of <code>tto</code>, which is a bad estimate because this ‘likelihood’ does not take censoring into account.</p>
<h3 id="the-correct-likelihood">The correct likelihood</h3>
<p>So what likelihood can we use? Note that in uncensored cases, <span class="math inline">\(f(tto_i \mid \mu) = f(tte_i \mid \mu)\)</span>, just like above.</p>
<p>In the censored cases, all we know is that <code>tte</code> must be larger than what was observed. This means that we need to sum over the probabilities of all possibilities: <span class="math inline">\(S(tto_i \mid \mu) := \sum_{t &gt; tto_i} f(t \mid \mu)\)</span>. The full likelihood is then</p>
<p class="mathjaxWide"><span class="math display">\[
L(\mu) 
:= 
\prod_{i = 1}^N \delta_i f(tto_i \mid \mu)
\times
\prod_{i = 1}^N (1 - \delta_i) S(tto_i \mid \mu)
,
\]</span></p>
<p>where <span class="math inline">\(\delta_i\)</span> is 1 if the event was observed and 0 if censored. Although we’ll stick to the Poisson model in this post, we can use these ideas to create a likelihood for many different choices of distribution by using the appropriate probability/survival functions <span class="math inline">\(f\)</span>, <span class="math inline">\(S\)</span>.</p>
<h2 id="implementation">Implementation</h2>
<p>We will fit this model using Stan because it is relatively easy to write a Bayesian model once we have understood the data generating process. This will require us to define prior distributions (on <span class="math inline">\(\mu\)</span>) just like with any Bayesian method. Since we are mostly interested in understanding the likelihood here, we will not give much consideration to the prior. However, if applying this to a real problem, it would be a good idea to give this more thought in a <a href="https://betanalpha.github.io/assets/case_studies/principled_bayesian_workflow.html">principled Bayesian workflow</a>.</p>
<h3 id="terminology">Terminology</h3>
<p>Stan makes the following abbreviations:</p>
<dl>
<dt><code>pmf</code></dt>
<dd>probability mass function, <span class="math inline">\(f(tto_i \mid \mu) = \text{Poisson}(tto_i \mid \mu)\)</span>
</dd>
<dt><code>lpmf</code></dt>
<dd>log(<code>pmf</code>)
</dd>
<dt><code>ccdf</code></dt>
<dd>survival function, a.k.a. complementary cumulative distribution function, <span class="math inline">\(S(tto_i | \mu) := \sum_{t &gt; tto_i} \text{Poisson}(t | \mu)\)</span>
</dd>
<dt><code>lccdf</code></dt>
<dd>log(<code>ccdf</code>)
</dd>
<dt><code>target</code></dt>
<dd>log(posterior probability density) = log(likelihood x prior).
</dd>
</dl>
<p>Stan uses the log-scale for its calculations, so we will need the log-likelihood:</p>
<p class="mathjaxWide"><span class="math display">\[
\log L(\mu) 
:= 
\sum_{i = 1}^N \delta_i \log f(tto_i \mid \mu)
+
\sum_{i = 1}^N (1 - \delta_i) \log S(tto_i \mid \mu)
.
\]</span></p>
<h3 id="the-model">The model</h3>
<p>In <a href="./censored_poisson.stan">our model</a>, we add the <code>lccdf</code> if the observation is censored and <code>lpmf</code> if not censored. Let’s load our model and take a look.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">model &lt;-<span class="st"> </span><span class="kw">stan_model</span>(<span class="st">&#39;censored_poisson.stan&#39;</span>)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">model</code></pre></div>
<pre><code>S4 class stanmodel &#39;censored_poisson&#39; coded as follows:
data {
  // the input data
  int&lt;lower = 1&gt; n;                      // number of observations
  int&lt;lower = 0&gt; tto[n];                 // tto is a list of ints
  int&lt;lower = 0, upper = 1&gt; censored[n]; // list of 0s and 1s
  
  // parameters for the prior
  real&lt;lower = 0&gt; shape;
  real&lt;lower = 0&gt; rate;
}

parameters {
  // the parameters used in our model
  real&lt;lower = 0&gt; mu; 
}

model {
  // posterior = prior * likelihood
  
  // prior
  mu ~ gamma(shape, rate);
  
  // likelihood
  for (i in 1:n) {
    if (censored[i]) {
      target += poisson_lccdf(tto[i] | mu);  
    } else {
      target += poisson_lpmf(tto[i] | mu);
    }
  }
  
} </code></pre>
<p>The language used in the Stan model is slightly different from R-notation but hopefully intuitive enough to convince yourself that it’s the same model we described above.</p>
<p>Now we can sample from the posterior of our model.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit &lt;-<span class="st"> </span>model <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">sampling</span>(
    <span class="dt">data =</span> <span class="kw">compose_data</span>(df, <span class="dt">shape =</span> <span class="dv">2</span>, <span class="dt">rate =</span> <span class="fl">0.05</span>),
    <span class="dt">iter =</span> <span class="dv">2000</span>,
    <span class="dt">warmup =</span> <span class="dv">500</span>
  ) 

fit</code></pre></div>
<pre><code>Inference for Stan model: censored_poisson.
4 chains, each with iter=2000; warmup=500; thin=1; 
post-warmup draws per chain=1500, total post-warmup draws=6000.

          mean se_mean   sd      2.5%       25%       50%       75%
mu        9.98    0.00 0.03      9.92      9.96      9.98     10.01
lp__ -19613.55    0.01 0.69 -19615.54 -19613.71 -19613.29 -19613.09
         97.5% n_eff Rhat
mu       10.05  2002    1
lp__ -19613.04  2924    1

Samples were drawn using NUTS(diag_e) at Sat Aug  4 15:49:36 2018.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).</code></pre>
<p>Stan has an amazing array of diagnostics to check the quality of the fitted model. Since our model is fairly simple and all checks are in order, I won’t describe them here.</p>
<p>The point estimate for <code>mu</code> is 9.98 and the true value is contained within the 95% credible interval [9.92, 10.05]. We can also plot all the samples from our posterior.</p>
<figure>
<img src="censoring_files/figure-markdown/estimate_histogram-1.png" />
</figure>
<h2 id="conclusion">Conclusion</h2>
<p>We have seen how to change the likelihood to take censored observations into account. Moreover, the same process works for most distributions, so you can swap out the Poisson for Weibull/gamma/lognormal or whatever you want. Using the Bayesian modelling language, Stan, makes it super easy to test your statistical intuitions by turning them into a workable model, so I’ll definitely be exploring Stan more in the future.</p>

<div id="disqus_thread"></div>
<script>
/**
 *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
 *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
 */
/*
   var disqus_config = function () {
   this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
   this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
   };
 */
(function() {  // DON'T EDIT BELOW THIS LINE
 var d = document, s = d.createElement('script');

 s.src = '//stappit-github-io.disqus.com/embed.js';

 s.setAttribute('data-timestamp', +new Date());
 (d.head || d.body).appendChild(s);
 })();
</script>

<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

<script id="dsq-count-scr" src="//stappit-github-io.disqus.com/count.js" async></script>
]]></summary>
</entry>
<entry>
    <title>Ageing and Marathon Running</title>
    <link href="http://stappit.github.io/posts/berlin_marathon/age.html" />
    <id>http://stappit.github.io/posts/berlin_marathon/age.html</id>
    <published>2018-07-02T00:00:00Z</published>
    <updated>2018-07-02T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 class="post-title">Ageing and Marathon Running</h1>

<div class="card post-meta border-0">
  <div class="card-body post-meta">
    <p class="card-text text-muted text-left">
    Posted on July  2, 2018  by Brian </br>
     Tags: <a href="/tags/running.html">running</a>, <a href="/tags/marathon.html">marathon</a>, <a href="/tags/berlin.html">berlin</a>, <a href="/tags/gam.html">gam</a>, <a href="/tags/statistics.html">statistics</a>, <a href="/tags/mgcv.html">mgcv</a>, <a href="/tags/random%20effects.html">random effects</a> </br>
     Category: <a href="/categories/berlin_marathon.html">berlin_marathon</a> 
    </p>
  </div>
</div>

<p>Running is great. One of the worst parts, though, is other people telling you how it’s all downhill after 30. But is it? It’s a persistent idea, so I wanted to see if actual marathon results support it. To get an idea of how age affects running performance, we’ll use the data from the <a href="https://www.bmw-berlin-marathon.com/en/race-day-/results-list.html">Berlin Marathon</a>, after some <a href="https://github.com/stappit/berlin-marathon">cleaning up</a>, focussing on the year 2016.</p>
<!--more-->
<figure>
<img src="./age_files/figure-markdown/bm2016-1.png" alt="Average finishing time in the Berlin marathon 2016" /><figcaption>Average finishing time in the Berlin marathon 2016</figcaption>
</figure>
<p><strong>tl;dr</strong> The performance of the average 20 year old is comparable to that of the average 50 year old. The 34 year old men were the fastest on average for their sex, whereas the 31 year old women were fastest on average for their sex.</p>
<p>Let’s load the data.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># mapping table from country abbreviations to country names</span>
mapping &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="st">&#39;data/country_continent_mapping.csv&#39;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">transmute</span>(
    <span class="dt">country =</span> name,
    <span class="dt">alpha3 =</span> <span class="st">`</span><span class="dt">alpha-3</span><span class="st">`</span>
  )

<span class="co"># read all the data: 2005 to 2016</span>
df0 &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="st">&#39;data/berlin_marathon_times.csv&#39;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">transmute</span>(
    year,
    age,
    <span class="dt">sex =</span> <span class="kw">ordered</span>(sex, <span class="dt">levels =</span> <span class="kw">c</span>(<span class="st">&#39;W&#39;</span>, <span class="st">&#39;M&#39;</span>)),
    <span class="dt">usex =</span> <span class="kw">factor</span>(sex, <span class="dt">levels =</span> <span class="kw">c</span>(<span class="st">&#39;W&#39;</span>, <span class="st">&#39;M&#39;</span>), <span class="dt">ordered =</span> <span class="ot">FALSE</span>), <span class="co"># for technical reasons</span>
    <span class="dt">alpha3 =</span> nationality, <span class="co"># for joining with mapping table</span>
    <span class="dt">net_time =</span> <span class="kw">as.duration</span>(<span class="kw">hms</span>(net_time)),
    <span class="dt">hours =</span> <span class="kw">as.numeric</span>(net_time, <span class="st">&#39;hours&#39;</span>)
  ) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">inner_join</span>(mapping, <span class="dt">by =</span> <span class="st">&#39;alpha3&#39;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">country =</span> <span class="kw">factor</span>(country)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">arrange</span>(hours) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">drop_na</span>() 

<span class="co"># year of interest</span>
df &lt;-<span class="st"> </span>df0 <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">filter</span>(year <span class="op">==</span><span class="st"> </span><span class="dv">2016</span>)

<span class="co"># sample of data</span>
df <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">head</span>() <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">select</span>(<span class="op">-</span>usex, <span class="op">-</span>alpha3, <span class="op">-</span>hours) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">kable</span>() <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">kable_styling</span>(<span class="dt">bootstrap_options =</span> <span class="kw">c</span>(<span class="st">&quot;striped&quot;</span>, <span class="st">&quot;hover&quot;</span>, <span class="st">&quot;responsive&quot;</span>))</code></pre></div>
<table class="table table-striped table-hover table-responsive" style="margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:right;">
year
</th>
<th style="text-align:right;">
age
</th>
<th style="text-align:left;">
sex
</th>
<th style="text-align:right;">
net_time
</th>
<th style="text-align:left;">
country
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
2016
</td>
<td style="text-align:right;">
34
</td>
<td style="text-align:left;">
M
</td>
<td style="text-align:right;">
7383s (~2.05 hours)
</td>
<td style="text-align:left;">
Ethiopia
</td>
</tr>
<tr>
<td style="text-align:right;">
2016
</td>
<td style="text-align:right;">
34
</td>
<td style="text-align:left;">
M
</td>
<td style="text-align:right;">
7393s (~2.05 hours)
</td>
<td style="text-align:left;">
Kenya
</td>
</tr>
<tr>
<td style="text-align:right;">
2016
</td>
<td style="text-align:right;">
28
</td>
<td style="text-align:left;">
M
</td>
<td style="text-align:right;">
7531s (~2.09 hours)
</td>
<td style="text-align:left;">
Kenya
</td>
</tr>
<tr>
<td style="text-align:right;">
2016
</td>
<td style="text-align:right;">
26
</td>
<td style="text-align:left;">
M
</td>
<td style="text-align:right;">
7616s (~2.12 hours)
</td>
<td style="text-align:left;">
Ethiopia
</td>
</tr>
<tr>
<td style="text-align:right;">
2016
</td>
<td style="text-align:right;">
27
</td>
<td style="text-align:left;">
M
</td>
<td style="text-align:right;">
7667s (~2.13 hours)
</td>
<td style="text-align:left;">
Kenya
</td>
</tr>
<tr>
<td style="text-align:right;">
2016
</td>
<td style="text-align:right;">
34
</td>
<td style="text-align:left;">
M
</td>
<td style="text-align:right;">
7769s (~2.16 hours)
</td>
<td style="text-align:left;">
Kenya
</td>
</tr>
</tbody>
</table>
<p>We have the variables <code>age</code>, <code>sex</code>, <code>nationality</code>, and <code>net_time</code> for a cross section of marathon runners to play with. To measure the effect of age on running performance, we should really be looking at multiple measurements of the same runners at different ages, whilst controlling for features more directly related to running performance (e.g. vO2max, training volume). This would be a fairly expensive experiment and I didn’t find any research in that direction (if you find some, please let me know). We’ll do what we can with the data at hand but the results can only be suggestive of interesting questions to pursue.</p>
<h2 id="a-first-look">A first look</h2>
<p>The average finishing time amongst the runners is about 4 hours 12 minutes, and the average age around 42. These numbers are fairly constant over the years.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">df0 <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(year) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">summarise</span>(
    <span class="dt">mean_hours =</span> <span class="kw">mean</span>(hours),
    <span class="dt">mean_age =</span> <span class="kw">mean</span>(age)
  ) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">gather</span>(metric, value, mean_hours, mean_age) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> year, <span class="dt">y =</span> value)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_col</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_x_discrete</span>(<span class="dt">limits =</span> <span class="kw">seq</span>(<span class="dv">2005</span>, <span class="dv">2016</span>, <span class="dv">2</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span><span class="st"> </span>metric, <span class="dt">scales =</span> <span class="st">&#39;free_y&#39;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(
    <span class="dt">x =</span> <span class="st">&#39;Year&#39;</span>,
    <span class="dt">y =</span> <span class="st">&#39;Value&#39;</span>,
    <span class="dt">title =</span> <span class="st">&#39;Average age and finishing time per year&#39;</span>,
    <span class="dt">subtitle =</span> <span class="st">&#39;Berlin Marathon&#39;</span>
  ) <span class="op">+</span>
<span class="st">  </span><span class="ot">NULL</span></code></pre></div>
<figure>
<img src="age_files/figure-markdown/unnamed-chunk-2-1.png" />
</figure>
<p>The results are similar when broken down by sex.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">df0 <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(year, sex) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">summarise</span>(
    <span class="dt">mean_hours =</span> <span class="kw">mean</span>(hours),
    <span class="dt">mean_age =</span> <span class="kw">mean</span>(age)
  ) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">gather</span>(metric, value, mean_hours, mean_age) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> year, <span class="dt">y =</span> value, <span class="dt">fill =</span> sex)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_col</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_x_discrete</span>(<span class="dt">limits =</span> <span class="kw">seq</span>(<span class="dv">2005</span>, <span class="dv">2016</span>, <span class="dv">2</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">facet_grid</span>(metric <span class="op">~</span><span class="st"> </span>sex, <span class="dt">scales =</span> <span class="st">&#39;free_y&#39;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(
    <span class="dt">x =</span> <span class="st">&#39;Year&#39;</span>,
    <span class="dt">y =</span> <span class="st">&#39;Value&#39;</span>,
    <span class="dt">title =</span> <span class="st">&#39;Average age and finishing time per year per sex&#39;</span>,
    <span class="dt">subtitle =</span> <span class="st">&#39;Berlin Marathon&#39;</span>
  ) <span class="op">+</span>
<span class="st">  </span><span class="ot">NULL</span></code></pre></div>
<figure>
<img src="age_files/figure-markdown/unnamed-chunk-3-1.png" />
</figure>
<h3 id="age-and-sex">Age and Sex</h3>
<p>Below is a scatter plot of finishing time against age. The first noticable property is that there is a LOT of variation. Indeed, it would be highly suprising if we could explain running performance with just age, with factors such as <a href="https://en.wikipedia.org/wiki/VVO2max">vvO2max</a> and <a href="https://en.wikipedia.org/wiki/Lactate_threshold">lactate threshold</a> being much more relevant. For some scale of reference, a sub-3h marathon is a challenging goal for many amateur marathon runners - a goal achieved by some in their 60s in this dataset.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">df <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> age, <span class="dt">y =</span> hours)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">alpha =</span> <span class="fl">0.03</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">se =</span> <span class="ot">FALSE</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_y_continuous</span>(<span class="dt">breaks =</span> <span class="kw">seq</span>(<span class="dv">2</span>, <span class="dv">8</span>, <span class="dt">by =</span> <span class="dv">1</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_x_continuous</span>(<span class="dt">breaks =</span> <span class="kw">seq</span>(<span class="dv">20</span>, <span class="dv">80</span>, <span class="dt">by =</span> <span class="dv">5</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(
    <span class="dt">y =</span> <span class="st">&#39;Net finishing time (hours)&#39;</span>,
    <span class="dt">x =</span> <span class="st">&#39;Age&#39;</span>,
    <span class="dt">title =</span> <span class="st">&#39;Finishing time vs. Age&#39;</span>, 
    <span class="dt">subtitle =</span> <span class="st">&#39;Berlin Marathon 2016&#39;</span>
  ) </code></pre></div>
<figure>
<img src="age_files/figure-markdown/age_vs_hours-1.png" />
</figure>
<p>Next up is the trend line, which shows that the runners in their early 20s were actually a bit slower than those in their late 20s. The trend then stays fairly flat until the early 40s, when the trend starts to slow down.</p>
<p>The above chart lumps men and women together, but the following histogram suggests a difference between the sexes.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">df <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(hours, <span class="dt">fill =</span> sex)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_density</span>(<span class="dt">alpha =</span> <span class="fl">0.3</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_x_continuous</span>(<span class="dt">breaks =</span> <span class="kw">seq</span>(<span class="dv">2</span>, <span class="dv">8</span>, <span class="dt">by =</span> <span class="fl">0.5</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_y_continuous</span>(<span class="dt">limits =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="fl">0.65</span>), <span class="dt">breaks =</span> <span class="kw">seq</span>(<span class="dv">0</span>, <span class="fl">0.6</span>, <span class="dt">by =</span> <span class="fl">0.1</span>), <span class="dt">expand =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="fl">0.02</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(
    <span class="dt">x =</span> <span class="st">&#39;Net finishing time (hours)&#39;</span>,
    <span class="dt">y =</span> <span class="st">&#39;Frequency&#39;</span>,
    <span class="dt">title =</span> <span class="st">&#39;Histogram of marathon finishing times&#39;</span>, 
    <span class="dt">subtitle =</span> <span class="st">&#39;Berlin Marathon 2016&#39;</span>
  )</code></pre></div>
<figure>
<img src="age_files/figure-markdown/hours_histogram_by_sex-1.png" />
</figure>
<p>Breaking down the hours-age chart by <code>sex</code> tells a similar story. The main difference is that the trend for women flattens out a bit earlier at around age 25 compared to around age 30 for men.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">df <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> age, <span class="dt">y =</span> hours, <span class="dt">colour =</span> sex)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">alpha =</span> <span class="fl">0.05</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">se =</span> <span class="ot">FALSE</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_y_continuous</span>(<span class="dt">breaks =</span> <span class="kw">seq</span>(<span class="dv">2</span>, <span class="dv">8</span>, <span class="dt">by =</span> <span class="dv">1</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_x_continuous</span>(<span class="dt">breaks =</span> <span class="kw">seq</span>(<span class="dv">20</span>, <span class="dv">80</span>, <span class="dt">by =</span> <span class="dv">5</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(
    <span class="dt">y =</span> <span class="st">&#39;Net finishing time (hours)&#39;</span>,
    <span class="dt">x =</span> <span class="st">&#39;Age&#39;</span>,
    <span class="dt">title =</span> <span class="st">&#39;Finishing time vs. Age&#39;</span>, 
    <span class="dt">subtitle =</span> <span class="st">&#39;Berlin Marathon 2016&#39;</span>
  ) </code></pre></div>
<figure>
<img src="age_files/figure-markdown/age_vs_hours_by_sex-1.png" />
</figure>
<h3 id="nationality">Nationality</h3>
<p>It is also possible that nationality plays some role here. The Berlin marathon is an international event with big prize money at stake and it draws the best of the best from around the world. International travel is also costly and it seems plausible that this could have some influence on the statistical properties of the runners.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ranking_m1 &lt;-<span class="st"> </span>df <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(country) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">summarise</span>(
    <span class="dt">mean_hours =</span> <span class="kw">mean</span>(hours), 
    <span class="dt">mean_age =</span> <span class="kw">mean</span>(age), 
    <span class="dt">runners =</span> <span class="kw">n</span>()
  ) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">arrange</span>(mean_hours) 

<span class="kw">head</span>(ranking_m1) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">kable</span>() <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">kable_styling</span>(<span class="dt">bootstrap_options =</span> <span class="kw">c</span>(<span class="st">&quot;striped&quot;</span>, <span class="st">&quot;hover&quot;</span>, <span class="st">&quot;responsive&quot;</span>))</code></pre></div>
<table class="table table-striped table-hover table-responsive" style="margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
country
</th>
<th style="text-align:right;">
mean_hours
</th>
<th style="text-align:right;">
mean_age
</th>
<th style="text-align:right;">
runners
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Lesotho
</td>
<td style="text-align:right;">
2.451111
</td>
<td style="text-align:right;">
36.00000
</td>
<td style="text-align:right;">
1
</td>
</tr>
<tr>
<td style="text-align:left;">
Eritrea
</td>
<td style="text-align:right;">
2.657500
</td>
<td style="text-align:right;">
28.25000
</td>
<td style="text-align:right;">
4
</td>
</tr>
<tr>
<td style="text-align:left;">
Ethiopia
</td>
<td style="text-align:right;">
2.749514
</td>
<td style="text-align:right;">
28.33333
</td>
<td style="text-align:right;">
12
</td>
</tr>
<tr>
<td style="text-align:left;">
Kenya
</td>
<td style="text-align:right;">
2.963480
</td>
<td style="text-align:right;">
34.94737
</td>
<td style="text-align:right;">
19
</td>
</tr>
<tr>
<td style="text-align:left;">
Kazakhstan
</td>
<td style="text-align:right;">
3.002222
</td>
<td style="text-align:right;">
27.00000
</td>
<td style="text-align:right;">
1
</td>
</tr>
<tr>
<td style="text-align:left;">
Guinea
</td>
<td style="text-align:right;">
3.130556
</td>
<td style="text-align:right;">
26.00000
</td>
<td style="text-align:right;">
1
</td>
</tr>
</tbody>
</table>
<p>However, a breakdown of the 118 countries has the problem that the number of participants for many countries is very small. Small sample sizes lead to high-variance estimates. In other words, we don’t necessarily believe that the average Berlin marathon runner from Lesotho is faster than the average Berlin marathon runner from Kenya on the basis of just one runner. We need some way of using all the information available so that low sample size groups don’t have such extreme estimates. In the next section, we will use random effects to account for this.</p>
<h2 id="models">Models</h2>
<p>This section gets a bit technical; feel free to skip to the end for the pretty charts.</p>
<p>The trend for age doesn’t look linear - it has a U-shape. We could try to model this by adding the quadratic term <code>age^2</code> into our linear models. However, in general we don’t know how many powers we will need to sufficiently model the effect and the effect may not even be polynomial. Instead, we harness the power of general additive models, a.k.a GAMs. Indeed, the trend lines in our plots above actually used GAMs behind the scenes. The advantage of using GAMs is that you don’t have to mess around trying to add the right number of powers into your linear models, or even assume the the trend is polynomial at all.</p>
<p>The <code>s()</code> notation indicates that we want to fit an arbitrary smooth function to the data (not just a linear function). A <strong>smooth</strong> has a number of optional parameters; here <code>bs = re</code> uses random effects as our basis, and <code>by = sex</code> indicates a different age-smooth for each sex.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m &lt;-<span class="st"> </span><span class="kw">gam</span>(
  hours <span class="op">~</span><span class="st"> </span><span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="kw">s</span>(country, <span class="dt">bs =</span> <span class="st">&#39;re&#39;</span>) <span class="op">+</span><span class="st"> </span>usex <span class="op">+</span><span class="st"> </span><span class="kw">s</span>(age) <span class="op">+</span><span class="st"> </span><span class="kw">s</span>(age, <span class="dt">by =</span> sex), 
  <span class="dt">family =</span> <span class="kw">Gamma</span>(),
  <span class="dt">data =</span> df, 
  <span class="dt">method =</span> <span class="st">&quot;REML&quot;</span>
)

<span class="kw">summary</span>(m)</code></pre></div>
<pre><code>Family: Gamma 
Link function: inverse 

Formula:
hours ~ 1 + s(country, bs = &quot;re&quot;) + usex + s(age) + s(age, by = sex)

Parametric coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 0.221962   0.002341   94.81   &lt;2e-16 ***
usexM       0.024254   0.000469   51.72   &lt;2e-16 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Approximate significance of smooth terms:
               edf  Ref.df      F p-value    
s(country)  83.294 117.000 23.306 &lt; 2e-16 ***
s(age)       6.004   6.959 64.706 &lt; 2e-16 ***
s(age):sexM  3.931   4.817  4.439 0.00105 ** 
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

R-sq.(adj) =  0.179   Deviance explained = 17.9%
-REML =  37846  Scale est. = 0.028131  n = 35986</code></pre>
<p>This model achieves an explained deviance of 17.9% (analogous to R^2 for Gaußian models). This is low, as expected, but it is unlikely we can get anything much higher from this dataset.</p>
<p>The observations in the dataset are to some degree not independent since many runners run in packs. This is especially the case for those runners following the official pacers. It is not clear how we could account for this or to what extent it affects the estimates. My guess is that it wouldn’t change them too much.</p>
<p>The residual QQ plot seems acceptable.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">qq.gam</span>(m)</code></pre></div>
<figure>
<img src="age_files/figure-markdown/unnamed-chunk-7-1.png" />
</figure>
<p>The country-level effects are modelled as Gaußian and the QQ plot indicates a decent fit, with the execption of a handful of countries on the extremes. Perhaps a non-normal distribution would model the random effects better, but we’ll roll with this for now.</p>
<p>Note that there are about 83 degrees of freedom although we have included estimates of all 118 countries. This is a direct consequence of using random effects.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(m, <span class="dt">select =</span> <span class="dv">1</span>)</code></pre></div>
<figure>
<img src="age_files/figure-markdown/unnamed-chunk-8-1.png" />
</figure>
<p>An interesting byproduct of using random effects to model country level variance is that we can then rank countries by their effect. Since <code>country</code> has an additive effect and the link function is <code>exp</code>, the effect of <code>country</code> on <code>hours</code> is multiplicative. Here we show the multiplicative effect of country on net finishing time. Note that Lesotho is no longer on top despite having the highest raw average time. Indeed, this country effect is higher for those countries with both faster times and more runners.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">df <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">distinct</span>(country, <span class="dt">.keep_all =</span> <span class="ot">TRUE</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">cbind</span>(
    <span class="kw">predict</span>(m, <span class="dt">newdata =</span> ., <span class="dt">type =</span> <span class="st">&#39;terms&#39;</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">      </span><span class="kw">as_tibble</span>() <span class="op">%&gt;%</span><span class="st">  </span>
<span class="st">      </span><span class="kw">transmute</span>(<span class="dt">country_effect =</span> <span class="kw">exp</span>(<span class="st">`</span><span class="dt">s(country)</span><span class="st">`</span>))
  ) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">inner_join</span>(ranking_m1, <span class="dt">by =</span> <span class="st">&#39;country&#39;</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">select</span>(country, country_effect, runners, mean_hours) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">arrange</span>(<span class="kw">desc</span>(country_effect)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">head</span>() <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">kable</span>() <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">kable_styling</span>(<span class="dt">bootstrap_options =</span> <span class="kw">c</span>(<span class="st">&#39;hover&#39;</span>, <span class="st">&#39;striped&#39;</span>, <span class="st">&#39;responsive&#39;</span>))</code></pre></div>
<table class="table table-hover table-striped table-responsive" style="margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
country
</th>
<th style="text-align:right;">
country_effect
</th>
<th style="text-align:right;">
runners
</th>
<th style="text-align:right;">
mean_hours
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Ethiopia
</td>
<td style="text-align:right;">
1.076819
</td>
<td style="text-align:right;">
12
</td>
<td style="text-align:right;">
2.749514
</td>
</tr>
<tr>
<td style="text-align:left;">
Kenya
</td>
<td style="text-align:right;">
1.070141
</td>
<td style="text-align:right;">
19
</td>
<td style="text-align:right;">
2.963480
</td>
</tr>
<tr>
<td style="text-align:left;">
Eritrea
</td>
<td style="text-align:right;">
1.048144
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
2.657500
</td>
</tr>
<tr>
<td style="text-align:left;">
Algeria
</td>
<td style="text-align:right;">
1.036830
</td>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
3.276991
</td>
</tr>
<tr>
<td style="text-align:left;">
Lithuania
</td>
<td style="text-align:right;">
1.033381
</td>
<td style="text-align:right;">
32
</td>
<td style="text-align:right;">
3.552552
</td>
</tr>
<tr>
<td style="text-align:left;">
Norway
</td>
<td style="text-align:right;">
1.021137
</td>
<td style="text-align:right;">
589
</td>
<td style="text-align:right;">
3.834238
</td>
</tr>
</tbody>
</table>
<p>Feel free to explore more model diagnostics or find a better fitting model.</p>
<h2 id="results">Results</h2>
<p>Let’s create a grid of values of interest and get the fitted estimates from our model.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">add_fit &lt;-<span class="st"> </span><span class="cf">function</span>(df, model) {
  <span class="co"># add columns for model&#39;s predicted response and se</span>
  df <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">cbind</span>(<span class="kw">predict</span>(model, <span class="dt">newdata =</span> ., <span class="dt">type =</span> <span class="st">&#39;response&#39;</span>, <span class="dt">se.fit =</span> <span class="ot">TRUE</span>))
}

<span class="co"># grid of values of interest</span>
mygrid &lt;-<span class="st"> </span><span class="kw">as_tibble</span>(
    <span class="kw">expand.grid</span>(
      <span class="dt">age =</span> <span class="dv">15</span><span class="op">:</span><span class="dv">80</span>,
      <span class="dt">sex =</span> <span class="kw">c</span>(<span class="st">&#39;W&#39;</span>, <span class="st">&#39;M&#39;</span>),
      <span class="dt">country =</span> <span class="st">&#39;Germany&#39;</span>
    ) 
  ) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">usex =</span> sex) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">add_fit</span>(m)</code></pre></div>
<p>We have used Germany as our reference country for a couple of reasons:</p>
<ol type="1">
<li>the mgcv package makes it difficult to exclude random effects from response prediction;</li>
<li>every country has the same curves up to a multiplicative constant; and</li>
<li>Germany has the highest number of participants.</li>
</ol>
<p>Ideally we would set the country effect to zero here, but will proceed regardless. Below is the point estimate with 95% confidence intervals.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mygrid <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> age, <span class="dt">y =</span> fit, <span class="dt">colour =</span> sex)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_ribbon</span>(<span class="kw">aes</span>(<span class="dt">ymin =</span> fit <span class="op">-</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>se.fit, <span class="dt">ymax =</span> fit <span class="op">+</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>se.fit, <span class="dt">fill =</span> sex), <span class="dt">show.legend =</span> <span class="ot">FALSE</span>, <span class="dt">colour =</span> <span class="ot">NA</span>, <span class="dt">alpha =</span> <span class="fl">0.3</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="dt">alpha =</span> <span class="fl">0.5</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">data =</span> mygrid <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">group_by</span>(sex) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">slice</span>(<span class="kw">which.min</span>(fit))) <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_x_continuous</span>(<span class="dt">breaks =</span> <span class="kw">seq</span>(<span class="dv">20</span>, <span class="dv">80</span>, <span class="dv">5</span>), <span class="dt">limits =</span> <span class="kw">c</span>(<span class="dv">18</span>, <span class="dv">80</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_y_continuous</span>(<span class="dt">breaks =</span> <span class="kw">seq</span>(<span class="dv">4</span>, <span class="dv">7</span>, <span class="fl">0.25</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(
    <span class="dt">title =</span> <span class="st">&#39;Average Finish Time by Age and Sex&#39;</span>,
    <span class="dt">subtitle =</span> <span class="st">&#39;The dots indicate the fastest age</span><span class="ch">\n</span><span class="st">The shaded area is a 95% confidence interval for the estimated net finishing time</span><span class="ch">\n</span><span class="st">Berlin Marathon 2016&#39;</span>,
    <span class="dt">x =</span> <span class="st">&#39;Age&#39;</span>,
    <span class="dt">y =</span> <span class="st">&#39;Finish Time (hours)&#39;</span>,
    <span class="dt">colour =</span> <span class="st">&#39;Sex&#39;</span>
  )</code></pre></div>
<figure>
<img src="age_files/figure-markdown/bm2016-1.png" />
</figure>
<p>The 31 years old women are the fastest age group, whereas for men it’s around 34. Moreover, the performance of the 20 year olds is comparable to that of the ~50 year olds.</p>
<p>So far we have only considered the year 2016. We can apply the same analysis to every year since 2005.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mymodel &lt;-<span class="st"> </span><span class="cf">function</span>(df) {
  <span class="co"># fit the desired model to the data</span>
  <span class="kw">gam</span>(
    hours <span class="op">~</span><span class="st"> </span><span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="kw">s</span>(country, <span class="dt">bs =</span> <span class="st">&#39;re&#39;</span>) <span class="op">+</span><span class="st"> </span>usex <span class="op">+</span><span class="st"> </span><span class="kw">s</span>(age) <span class="op">+</span><span class="st"> </span><span class="kw">s</span>(age, <span class="dt">by =</span> sex), 
    <span class="dt">family =</span> <span class="kw">Gamma</span>(),
    <span class="dt">data =</span> df, 
    <span class="dt">method =</span> <span class="st">&quot;REML&quot;</span>
  )
}

<span class="co"># apply the model to each year separately</span>
all_years_modelled &lt;-<span class="st"> </span>df0 <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">nest</span>(<span class="op">-</span>year) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(
    <span class="dt">model =</span> <span class="kw">map</span>(data, mymodel),
    <span class="dt">pred =</span> <span class="kw">map</span>(model, <span class="cf">function</span>(x) {
                        <span class="kw">add_fit</span>(mygrid, x)
                      }
    )
  ) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">unnest</span>(pred)</code></pre></div>
<p>This time we drop the confidence intervals and get a feel for the uncertainty by plotting a line for every year.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">all_years_modelled <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> age, <span class="dt">y =</span> fit, <span class="dt">colour =</span> year, <span class="dt">group =</span> year)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="dt">alpha =</span> <span class="fl">0.3</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">data =</span> all_years_modelled <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">group_by</span>(year, sex) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">slice</span>(<span class="kw">which.min</span>(fit)), <span class="dt">alpha =</span> <span class="fl">0.7</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span><span class="st"> </span>sex) <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_x_continuous</span>(<span class="dt">breaks =</span> <span class="kw">seq</span>(<span class="dv">20</span>, <span class="dv">80</span>, <span class="dv">5</span>), <span class="dt">limits =</span> <span class="kw">c</span>(<span class="dv">18</span>, <span class="dv">80</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_y_continuous</span>(<span class="dt">breaks =</span> <span class="kw">seq</span>(<span class="dv">4</span>, <span class="dv">6</span>, <span class="fl">0.25</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(
    <span class="dt">title =</span> <span class="st">&#39;Average Finish Time by Age and Sex&#39;</span>,
    <span class="dt">subtitle =</span> <span class="st">&#39;Each line is an estimate for different year: 2005 -- 2016</span><span class="ch">\n</span><span class="st">The dots indicate the fastest age&#39;</span>,
    <span class="dt">x =</span> <span class="st">&#39;Age&#39;</span>,
    <span class="dt">y =</span> <span class="st">&#39;Finish Time (hours)&#39;</span>,
    <span class="dt">colour =</span> <span class="st">&#39;Year&#39;</span>
  )</code></pre></div>
<figure>
<img src="age_files/figure-markdown/bm_2005_2016-1.png" />
</figure>
<p>Notice that the fastest ages approximately increase with year, most notably for women. We can display this relationship better by simply plotting estimated fastest age against year.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">all_years_modelled <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(year, sex) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">slice</span>(<span class="kw">which.min</span>(fit)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> year, <span class="dt">y =</span> age, <span class="dt">colour =</span> sex)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_y_continuous</span>(<span class="dt">breaks =</span> <span class="kw">seq</span>(<span class="dv">30</span>, <span class="dv">40</span>, <span class="dv">1</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_x_continuous</span>(<span class="dt">breaks =</span> <span class="kw">seq</span>(<span class="dv">2005</span>, <span class="dv">2015</span>, <span class="dv">2</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span><span class="st"> </span>sex) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> lm) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(
    <span class="dt">x =</span> <span class="st">&#39;Year&#39;</span>,
    <span class="dt">y =</span> <span class="st">&#39;Age&#39;</span>,
    <span class="dt">colour =</span> <span class="st">&#39;Sex&#39;</span>,
    <span class="dt">title =</span> <span class="st">&#39;Fastest age per year&#39;</span>,
    <span class="dt">subtitle =</span> <span class="st">&#39;Berlin Marathon&#39;</span>
  )</code></pre></div>
<figure>
<img src="age_files/figure-markdown/fastest_age_by_year-1.png" />
</figure>
<p>It will be interesting to see if the fastest age for women stays low in future years. It is possible that this change in fastest age for women is due to increasing participation. Indeed, participation is increasing faster for women than for men, although it starts from a much smaller number.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">df0 <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(year, sex) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">summarise</span>(<span class="dt">total =</span> <span class="kw">n</span>()) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">arrange</span>(sex, year) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> year, <span class="dt">y =</span> total, <span class="dt">colour =</span> sex)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> lm) <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_y_continuous</span>(<span class="dt">labels =</span> scales<span class="op">::</span>comma) <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_x_continuous</span>(<span class="dt">breaks =</span> <span class="kw">seq</span>(<span class="dv">2005</span>, <span class="dv">2015</span>, <span class="dv">2</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(
    <span class="dt">x =</span> <span class="st">&#39;Year&#39;</span>,
    <span class="dt">y =</span> <span class="st">&#39;Runners&#39;</span>,
    <span class="dt">colour =</span> <span class="st">&#39;Sex&#39;</span>,
    <span class="dt">title =</span> <span class="st">&#39;Participation per sex per year&#39;</span>,
    <span class="dt">subtitle =</span> <span class="st">&#39;Berlin Marathon&#39;</span>
  )</code></pre></div>
<figure>
<img src="age_files/figure-markdown/finishers_by_year-1.png" />
</figure>
<h2 id="conclusion">Conclusion</h2>
<p>On the basis of our dataset, there is no indication that performance declines at 30. Indeed, those in their early 30s tend to be the fastest age group, both for men and women. Moreover, the performance of the 20 year olds is comparable to that of the ~50 year olds, perhaps suggesting that running performance doesn’t degrade as fast as people often think it does.</p>

<div id="disqus_thread"></div>
<script>
/**
 *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
 *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
 */
/*
   var disqus_config = function () {
   this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
   this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
   };
 */
(function() {  // DON'T EDIT BELOW THIS LINE
 var d = document, s = d.createElement('script');

 s.src = '//stappit-github-io.disqus.com/embed.js';

 s.setAttribute('data-timestamp', +new Date());
 (d.head || d.body).appendChild(s);
 })();
</script>

<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

<script id="dsq-count-scr" src="//stappit-github-io.disqus.com/count.js" async></script>
]]></summary>
</entry>

</feed>
