<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>Thoughts from the Café</title>
    <link href="http://stappit.github.io/atom.xml" rel="self" />
    <link href="http://stappit.github.io" />
    <id>http://stappit.github.io/atom.xml</id>
    <author>
        <name>Brian</name>
        <email>ha@hahaha.com</email>
    </author>
    <updated>2018-11-05T00:00:00Z</updated>
    <entry>
    <title>BDA3 Chapter 5 Exercise 2</title>
    <link href="http://stappit.github.io/posts/bda3/chapter_05_exercise_02.html" />
    <id>http://stappit.github.io/posts/bda3/chapter_05_exercise_02.html</id>
    <published>2018-11-05T00:00:00Z</published>
    <updated>2018-11-05T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 class="post-title">BDA3 Chapter 5 Exercise 2</h1>

<div class="card post-meta border-0">
  <div class="card-body post-meta">
    <p class="card-text text-muted text-left">
    Posted on November  5, 2018  by Brian </br>
     Tags: <a href="/tags/bda%20chapter%205.html">bda chapter 5</a>, <a href="/tags/solutions.html">solutions</a>, <a href="/tags/bayes.html">bayes</a>, <a href="/tags/exchangeability.html">exchangeability</a> </br>
     Category: <a href="/categories/bda3.html">bda3</a> 
    </p>
  </div>
</div>

<p>Here’s my solution to exercise 2, chapter 5, of <a href="https://andrewgelman.com/">Gelman’s</a> <em>Bayesian Data Analysis</em> (BDA), 3rd edition. There are <a href="http://www.stat.columbia.edu/~gelman/book/solutions.pdf">solutions</a> to some of the exercises on the <a href="http://www.stat.columbia.edu/~gelman/book/">book’s webpage</a>.</p>
<!--more-->
<div style="display:none">
<p class="mathjaxWide"><span class="math inline">\(\DeclareMathOperator{\dbinomial}{Binomial}  \DeclareMathOperator{\dbern}{Bernoulli}  \DeclareMathOperator{\dpois}{Poisson}  \DeclareMathOperator{\dnorm}{Normal}  \DeclareMathOperator{\dt}{t}  \DeclareMathOperator{\dcauchy}{Cauchy}  \DeclareMathOperator{\dexponential}{Exp}  \DeclareMathOperator{\duniform}{Uniform}  \DeclareMathOperator{\dgamma}{Gamma}  \DeclareMathOperator{\dinvgamma}{InvGamma}  \DeclareMathOperator{\invlogit}{InvLogit}  \DeclareMathOperator{\dinvchi}{InvChi2}  \DeclareMathOperator{\dsinvchi}{SInvChi2}  \DeclareMathOperator{\dchi}{Chi2}  \DeclareMathOperator{\dnorminvchi}{NormInvChi2}  \DeclareMathOperator{\logit}{Logit}  \DeclareMathOperator{\ddirichlet}{Dirichlet}  \DeclareMathOperator{\dbeta}{Beta}\)</span></p>
</div>
<p>This exercise is an extension of <a href="./chapter_05_exercise_01.html">the previous exercise</a>. Suppose we have a box with <span class="math inline">\(n\)</span> balls, with <span class="math inline">\(B\)</span> black balls and <span class="math inline">\(W\)</span> white balls (<span class="math inline">\(n = B + W\)</span>), where we don’t know how many of each. We pick a ball <span class="math inline">\(y_1\)</span> at random, put it back, then pick another ball <span class="math inline">\(y_2\)</span> at random. In this case, Then <span class="math inline">\(y_1\)</span> and <span class="math inline">\(y_2\)</span> are independent, and therefore also exchangeable.</p>
<p>The draws are no longer independent if we don’t put the first ball back before picking the second ball. They remain exchangeable since the conditional joint probability function does not depend on the order</p>
<p class="mathjaxWide"><span class="math display">\[
  p(y_1, y_2 \mid B, W)
  =
  \begin{cases}
    \frac{BW}{n(n - 1)} &amp; \text{ if } y_1 \ne y_2 \\
    \frac{W(W - 1)}{n (n - 1)} &amp; \text{ if } y_1 = y_2 = \text{white} \\
    \frac{B(B - 1)}{n (n - 1)} &amp; \text{ if } y_1 = y_2 = \text{black,} 
  \end{cases}
\]</span></p>
<p>which implies that the joint probability does not depend on the order</p>
<p class="mathjaxWide"><span class="math display">\[
  p(y_1, y_2)
  =
  \sum_{B + W \ge 2 \\ B \ge 0, W \ge 0}^\infty p(y_1, y_2 \mid B, W) p(B, W)
  .
\]</span></p>
<p>Whether we can treat them as if they were independent depends on the prior <span class="math inline">\(p(B, W)\)</span>. If there is significant probability mass on low values, then we shouldn’t treat them as independent (see <a href="./chapter_05_exercise_01.html">the previous exercise</a>). If the only significant probability mass were on very large values of <span class="math inline">\(B\)</span> and <span class="math inline">\(W\)</span>, then we could treat them as if they were independent. This follows from the fact that</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
  \frac{B-1}{n - 1} 
  &amp;\approx
  \frac{B}{n}
  \approx
  \frac{B}{n-1}
  \\
  \frac{W-1}{n - 1} 
  &amp;\approx
  \frac{W}{n}
  \approx
  \frac{W}{n-1}
  ,
\end{align}
\]</span></p>
<p>when <span class="math inline">\(B, W \gg 0\)</span>.</p>

<div id="disqus_thread"></div>
<script>
/**
 *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
 *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
 */
/*
   var disqus_config = function () {
   this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
   this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
   };
 */
(function() {  // DON'T EDIT BELOW THIS LINE
 var d = document, s = d.createElement('script');

 s.src = '//stappit-github-io.disqus.com/embed.js';

 s.setAttribute('data-timestamp', +new Date());
 (d.head || d.body).appendChild(s);
 })();
</script>

<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

<script id="dsq-count-scr" src="//stappit-github-io.disqus.com/count.js" async></script>
]]></summary>
</entry>
<entry>
    <title>BDA3 Chapter 5 Exercise 1</title>
    <link href="http://stappit.github.io/posts/bda3/chapter_05_exercise_01.html" />
    <id>http://stappit.github.io/posts/bda3/chapter_05_exercise_01.html</id>
    <published>2018-11-05T00:00:00Z</published>
    <updated>2018-11-05T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 class="post-title">BDA3 Chapter 5 Exercise 1</h1>

<div class="card post-meta border-0">
  <div class="card-body post-meta">
    <p class="card-text text-muted text-left">
    Posted on November  5, 2018  by Brian </br>
     Tags: <a href="/tags/bda%20chapter%205.html">bda chapter 5</a>, <a href="/tags/solutions.html">solutions</a>, <a href="/tags/bayes.html">bayes</a>, <a href="/tags/exchangeability.html">exchangeability</a> </br>
     Category: <a href="/categories/bda3.html">bda3</a> 
    </p>
  </div>
</div>

<p>Here’s my solution to exercise 1, chapter 5, of <a href="https://andrewgelman.com/">Gelman’s</a> <em>Bayesian Data Analysis</em> (BDA), 3rd edition. There are <a href="http://www.stat.columbia.edu/~gelman/book/solutions.pdf">solutions</a> to some of the exercises on the <a href="http://www.stat.columbia.edu/~gelman/book/">book’s webpage</a>.</p>
<!--more-->
<div style="display:none">
<p class="mathjaxWide"><span class="math inline">\(\DeclareMathOperator{\dbinomial}{Binomial}  \DeclareMathOperator{\dbern}{Bernoulli}  \DeclareMathOperator{\dpois}{Poisson}  \DeclareMathOperator{\dnorm}{Normal}  \DeclareMathOperator{\dt}{t}  \DeclareMathOperator{\dcauchy}{Cauchy}  \DeclareMathOperator{\dexponential}{Exp}  \DeclareMathOperator{\duniform}{Uniform}  \DeclareMathOperator{\dgamma}{Gamma}  \DeclareMathOperator{\dinvgamma}{InvGamma}  \DeclareMathOperator{\invlogit}{InvLogit}  \DeclareMathOperator{\dinvchi}{InvChi2}  \DeclareMathOperator{\dsinvchi}{SInvChi2}  \DeclareMathOperator{\dchi}{Chi2}  \DeclareMathOperator{\dnorminvchi}{NormInvChi2}  \DeclareMathOperator{\logit}{Logit}  \DeclareMathOperator{\ddirichlet}{Dirichlet}  \DeclareMathOperator{\dbeta}{Beta}\)</span></p>
</div>
<p>Suppose we have a box with one black ball and one white ball inside. We picka ball <span class="math inline">\(y_1\)</span> at random, put it back, then pick another ball <span class="math inline">\(y_2\)</span> at random. Then <span class="math inline">\(y_1\)</span> and <span class="math inline">\(y_2\)</span> are independent, and therefore also exchangeable.</p>
<p>The draws are no longer independent if we don’t put the first ball back before picking the second ball. They remain exchangeable since the joint probability only depends on the equality of the arguments, not on their order. More specifically,</p>
<p class="mathjaxWide"><span class="math display">\[
  p(y_1, y_2)
  =
  \begin{cases}
    \frac{1}{2} &amp; \text{ if } y_1 \ne y_2 \\
    0 &amp; \text{ otherwise.}
  \end{cases}
\]</span></p>
<p>Treating the draws as if they were independent would not be a good idea.</p>
<p>If there were <span class="math inline">\(M\)</span> balls of each colour, where <span class="math inline">\(M \gg 0\)</span> such as <span class="math inline">\(M = 10^6\)</span>, then the draws would still not be independent, but they would be very close to independence. They remain exchangeable. This is clearer from the joint probability function</p>
<p class="mathjaxWide"><span class="math display">\[
  p(y_1, y_2)
  =
  \begin{cases}
    \frac{1}{2}\frac{M}{2M - 1} &amp; \text{ if } y_1 \ne y_2 \\
    \frac{1}{2}\frac{M - 1}{2M - 1} &amp; \text{ otherwise,}
  \end{cases}
\]</span></p>
<p>since <span class="math inline">\(\frac{M}{2M - 1} \approx \frac{1}{2} \approx \frac{M - 1}{2M - 1}\)</span> for <span class="math inline">\(M \gg 0\)</span>.</p>

<div id="disqus_thread"></div>
<script>
/**
 *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
 *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
 */
/*
   var disqus_config = function () {
   this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
   this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
   };
 */
(function() {  // DON'T EDIT BELOW THIS LINE
 var d = document, s = d.createElement('script');

 s.src = '//stappit-github-io.disqus.com/embed.js';

 s.setAttribute('data-timestamp', +new Date());
 (d.head || d.body).appendChild(s);
 })();
</script>

<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

<script id="dsq-count-scr" src="//stappit-github-io.disqus.com/count.js" async></script>
]]></summary>
</entry>
<entry>
    <title>BDA3 Chapter 4 Exercise 1</title>
    <link href="http://stappit.github.io/posts/bda3/chapter_04_exercise_01.html" />
    <id>http://stappit.github.io/posts/bda3/chapter_04_exercise_01.html</id>
    <published>2018-11-03T00:00:00Z</published>
    <updated>2018-11-03T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 class="post-title">BDA3 Chapter 4 Exercise 1</h1>

<div class="card post-meta border-0">
  <div class="card-body post-meta">
    <p class="card-text text-muted text-left">
    Posted on November  3, 2018  by Brian </br>
     Tags: <a href="/tags/bda%20chapter%204.html">bda chapter 4</a>, <a href="/tags/solutions.html">solutions</a>, <a href="/tags/bayes.html">bayes</a>, <a href="/tags/cauchy.html">cauchy</a>, <a href="/tags/normal%20approximation.html">normal approximation</a> </br>
     Category: <a href="/categories/bda3.html">bda3</a> 
    </p>
  </div>
</div>

<p>Here’s my solution to exercise 1, chapter 4, of <a href="https://andrewgelman.com/">Gelman’s</a> <em>Bayesian Data Analysis</em> (BDA), 3rd edition. There are <a href="http://www.stat.columbia.edu/~gelman/book/solutions.pdf">solutions</a> to some of the exercises on the <a href="http://www.stat.columbia.edu/~gelman/book/">book’s webpage</a>.</p>
<!--more-->
<div style="display:none">
<p class="mathjaxWide"><span class="math inline">\(\DeclareMathOperator{\dbinomial}{Binomial} \DeclareMathOperator{\dbern}{Bernoulli} \DeclareMathOperator{\dpois}{Poisson} \DeclareMathOperator{\dnorm}{Normal} \DeclareMathOperator{\dt}{t} \DeclareMathOperator{\dcauchy}{Cauchy} \DeclareMathOperator{\dexponential}{Exp} \DeclareMathOperator{\duniform}{Uniform} \DeclareMathOperator{\dgamma}{Gamma} \DeclareMathOperator{\dinvgamma}{InvGamma} \DeclareMathOperator{\invlogit}{InvLogit} \DeclareMathOperator{\dinvchi}{InvChi2} \DeclareMathOperator{\dnorminvchi}{NormInvChi2} \DeclareMathOperator{\logit}{Logit} \DeclareMathOperator{\ddirichlet}{Dirichlet} \DeclareMathOperator{\dbeta}{Beta}\)</span></p>
</div>
<p>Suppose the likelihood is Cauchy, <span class="math inline">\(p(y_i \mid \theta) \propto (1 + (y_i - \theta)^2)^{-1}\)</span>, with a prior uniform on <span class="math inline">\([0, 1]\)</span>. Then the posterior has the same equation as the likelihood on the support of <span class="math inline">\(\theta\)</span>. Part of the exercise is to find the posterior mode but the hint is more confusing than helpful. Solving for the mode algebraically involves solving a polynomial of degree <span class="math inline">\(2n + 1\)</span>, where <span class="math inline">\(n\)</span> is the number of observations. We’ll use some numerical approximations to find the mode.</p>
<h2 id="posterior-mode">Posterior mode</h2>
<p>The observed data are as follows.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">y &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>, <span class="dv">0</span>, <span class="fl">1.5</span>, <span class="fl">2.5</span>)</code></pre></div>
<p>We could make draws from the posterior by coding this up in stan. However, an estimation of the mode from such values is difficult. An alternative is to use numerical optimisation. For that we’ll use the posterior on the log scale.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">log_likelihood &lt;-<span class="st"> </span><span class="cf">function</span>(y, theta)
  (<span class="dv">1</span> <span class="op">+</span><span class="st"> </span>(y <span class="op">-</span><span class="st"> </span>theta)<span class="op">^</span><span class="dv">2</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">log</span>() <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">sum</span>() <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">map_dbl</span>(<span class="st">`</span><span class="dt">*</span><span class="st">`</span>, <span class="op">-</span><span class="dv">1</span>)

log_posterior_given &lt;-<span class="st"> </span><span class="cf">function</span>(y) {
  <span class="cf">function</span>(theta) {
    <span class="cf">if</span> (theta <span class="op">&lt;</span><span class="st"> </span><span class="dv">0</span> <span class="op">|</span><span class="st"> </span><span class="dv">1</span> <span class="op">&lt;</span><span class="st"> </span>theta) {
      <span class="kw">return</span>(<span class="op">-</span><span class="ot">Inf</span>)
    } <span class="cf">else</span> {
      <span class="kw">return</span>(<span class="kw">log_likelihood</span>(y, theta))
    }
  }
}

log_posterior &lt;-<span class="st"> </span><span class="kw">log_posterior_given</span>(y)</code></pre></div>
<p>Numerical maximisation gives us a value near, but not quite, zero.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mode_numerical &lt;-<span class="st"> </span><span class="kw">optimise</span>(log_posterior, <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">maximum =</span> <span class="ot">TRUE</span>)<span class="op">$</span>maximum
mode_numerical</code></pre></div>
<pre><code>[1] 6.610696e-05</code></pre>
<p>Let’s plot the posterior.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">granularity &lt;-<span class="st"> </span><span class="fl">1e5</span>
grid &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">theta =</span> <span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dt">length.out =</span> granularity)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(
    <span class="dt">id =</span> <span class="dv">1</span><span class="op">:</span><span class="kw">n</span>(),
    <span class="dt">log_unnormalised_density =</span> <span class="kw">map_dbl</span>(theta, log_posterior),
    <span class="dt">unnormalised_density =</span> <span class="kw">exp</span>(log_unnormalised_density),
    <span class="dt">density =</span> granularity <span class="op">*</span><span class="st"> </span>unnormalised_density <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(unnormalised_density),
    <span class="dt">is_mode =</span> <span class="kw">abs</span>(theta <span class="op">-</span><span class="st"> </span><span class="kw">signif</span>(mode_numerical, <span class="dt">digits =</span> <span class="dv">1</span>)) <span class="op">&lt;</span><span class="st"> </span><span class="fl">0.5</span> <span class="op">/</span><span class="st"> </span>granularity
  ) </code></pre></div>
<figure>
<img src="chapter_04_exercise_01_files/figure-markdown/grid_plot-1..svg" />
</figure>
<p>Indeed, it looks like the mode could be 0. Zooming in we see that it is very likely to be zero.</p>
<figure>
<img src="chapter_04_exercise_01_files/figure-markdown/closeup-1..svg" />
</figure>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mode &lt;-<span class="st"> </span><span class="dv">0</span></code></pre></div>
<h2 id="normal-approximation">Normal approximation</h2>
<p>Now let’s calculate the derivatives in order to find the normal approximation. The derivative of the log posterior is</p>
<p class="mathjaxWide"><span class="math display">\[
\frac{d}{d\theta} \log p(y \mid \theta)
=
2 \sum_1^5 \frac{y_i - \theta}{1 + (y_i - \theta)^2}
.
\]</span></p>
<p>The second derivative of the log posterior is</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
\frac{d^2}{d\theta^2} \log p(y \mid \theta)
&amp;=
\sum_1^5
\frac{
  \frac{-2}{1 + (y_i - \theta)^2} - \frac{2(y_i - \theta)}{(1 + (y_i - \theta)^2)^2}\cdot 2(y_i - \theta)
}{
  \left( 1 + (y_i - \theta)^2 \right)^2
}
\\
&amp;=
\sum_1^5
\frac{-2\left( 1 + (y_i - \theta)^2 \right)^2 - 4 (y_i - \theta)^2}{\left( 1 + (y_i - \theta)^2 \right)^4}
\\
&amp;=
-2
\sum_1^5
\frac{3(y_i - \theta)^2 + 2(y_i - \theta) + 1}{\left( 1 + (y_i - \theta)^2 \right)^4}
.
\end{align}
\]</span></p>
<p>Evaluating this at the mode gives</p>
<p class="mathjaxWide"><span class="math display">\[
-2 \sum_1^5 \frac{3y_i^2 + 2y_i + 1}{\left( 1 + y_i^2 \right)^4}
.
\]</span></p>
<p>The means that the observed information is</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">I &lt;-<span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span><span class="kw">sum</span>((<span class="dv">3</span> <span class="op">*</span><span class="st"> </span>y<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>y <span class="op">+</span><span class="st"> </span><span class="dv">1</span>) <span class="op">/</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span>y<span class="op">^</span><span class="dv">2</span>)<span class="op">^</span><span class="dv">4</span>)
I</code></pre></div>
<pre><code>[1] 2.489427</code></pre>
<p>This gives us the normal approximation with</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mu &lt;-<span class="st"> </span>mode
variance &lt;-<span class="st"> </span><span class="dv">1</span> <span class="op">/</span><span class="st"> </span>I
std &lt;-<span class="st"> </span><span class="kw">sqrt</span>(variance)

<span class="kw">c</span>(mu, std)</code></pre></div>
<pre><code>[1] 0.0000000 0.6337972</code></pre>
<p>which gives us the normal approximation <span class="math inline">\(p(\theta \mid y) \approx \dnorm(0, 0.634)\)</span> on <span class="math inline">\([0, 1]\)</span>.</p>
<figure>
<img src="chapter_04_exercise_01_files/figure-markdown/approx_plot-1..svg" />
</figure>
<p>The approximation isn’t very good.</p>

<div id="disqus_thread"></div>
<script>
/**
 *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
 *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
 */
/*
   var disqus_config = function () {
   this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
   this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
   };
 */
(function() {  // DON'T EDIT BELOW THIS LINE
 var d = document, s = d.createElement('script');

 s.src = '//stappit-github-io.disqus.com/embed.js';

 s.setAttribute('data-timestamp', +new Date());
 (d.head || d.body).appendChild(s);
 })();
</script>

<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

<script id="dsq-count-scr" src="//stappit-github-io.disqus.com/count.js" async></script>
]]></summary>
</entry>
<entry>
    <title>BDA3 Chapter 3 Exercise 11</title>
    <link href="http://stappit.github.io/posts/bda3/chapter_03_exercise_11.html" />
    <id>http://stappit.github.io/posts/bda3/chapter_03_exercise_11.html</id>
    <published>2018-10-21T00:00:00Z</published>
    <updated>2018-10-21T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 class="post-title">BDA3 Chapter 3 Exercise 11</h1>

<div class="card post-meta border-0">
  <div class="card-body post-meta">
    <p class="card-text text-muted text-left">
    Posted on October 21, 2018  by Brian </br>
     Tags: <a href="/tags/bda%20chapter%203.html">bda chapter 3</a>, <a href="/tags/solutions.html">solutions</a>, <a href="/tags/bayes.html">bayes</a> </br>
     Category: <a href="/categories/bda3.html">bda3</a> 
    </p>
  </div>
</div>

<p>Here’s my solution to exercise 11, chapter 3, of <a href="https://andrewgelman.com/">Gelman’s</a> <em>Bayesian Data Analysis</em> (BDA), 3rd edition. There are <a href="http://www.stat.columbia.edu/~gelman/book/solutions.pdf">solutions</a> to some of the exercises on the <a href="http://www.stat.columbia.edu/~gelman/book/">book’s webpage</a>.</p>
<!--more-->
<div style="display:none">
<p class="mathjaxWide"><span class="math inline">\(\DeclareMathOperator{\dbinomial}{Binomial} \DeclareMathOperator{\dbern}{Bernoulli} \DeclareMathOperator{\dpois}{Poisson} \DeclareMathOperator{\dnorm}{Normal} \DeclareMathOperator{\dt}{t} \DeclareMathOperator{\dcauchy}{Cauchy} \DeclareMathOperator{\dexponential}{Exp} \DeclareMathOperator{\duniform}{Uniform} \DeclareMathOperator{\dgamma}{Gamma} \DeclareMathOperator{\dinvgamma}{InvGamma} \DeclareMathOperator{\invlogit}{InvLogit} \DeclareMathOperator{\logit}{Logit} \DeclareMathOperator{\ddirichlet}{Dirichlet} \DeclareMathOperator{\dbeta}{Beta}\)</span></p>
</div>
<p>We will analyse <a href="data/chapter_03_exercise_11.csv">the data</a> given in section 3.7 using different priors.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">df &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="st">&#39;data/chapter_03_exercise_11.csv&#39;</span>) </code></pre></div>
<table class="table table-striped table-hover table-responsive" style="margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:right;">
dose_log_g_ml
</th>
<th style="text-align:right;">
animals
</th>
<th style="text-align:right;">
deaths
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
-0.86
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:right;">
-0.30
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
1
</td>
</tr>
<tr>
<td style="text-align:right;">
-0.05
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
3
</td>
</tr>
<tr>
<td style="text-align:right;">
0.73
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
5
</td>
</tr>
</tbody>
</table>
<p>Here is the model specification.</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
  y_i \mid \theta_i 
  &amp;\sim 
  \dbinomial(n_i, \theta_i)
  \\
  \logit(\theta_i)
  &amp;=
  \alpha + \beta x_i
  \\
  \alpha
  &amp;\sim
  \dnorm(0, 2^2)
  \\
  \beta
  &amp;\sim
  \dnorm(10, 10^2)
\end{align}
\]</span></p>
<p>We won’t use a grid approximation to the posterior but instead just use <a href="https://www.rdocumentation.org/packages/rstanarm/versions/2.17.4/topics/stan_glm">Stan</a> because it is a lot simpler.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m &lt;-<span class="st"> </span>rstanarm<span class="op">::</span><span class="kw">stan_glm</span>(
  <span class="kw">cbind</span>(deaths, animals <span class="op">-</span><span class="st"> </span>deaths)  <span class="op">~</span><span class="st"> </span><span class="dv">1</span> <span class="op">+</span><span class="st"> </span>dose_log_g_ml,
  <span class="dt">family =</span> <span class="kw">binomial</span>(<span class="dt">link =</span> logit),
  <span class="dt">data =</span> df,
  <span class="dt">prior_intercept =</span> <span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">2</span>),
  <span class="dt">prior =</span> <span class="kw">normal</span>(<span class="dv">10</span>, <span class="dv">10</span>),
  <span class="dt">warmup =</span> <span class="dv">500</span>,
  <span class="dt">iter =</span> <span class="dv">4000</span>
)

<span class="kw">summary</span>(m)</code></pre></div>
<pre><code>Model Info:

 function:     stan_glm
 family:       binomial [logit]
 formula:      cbind(deaths, animals - deaths) ~ 1 + dose_log_g_ml
 algorithm:    sampling
 priors:       see help(&#39;prior_summary&#39;)
 sample:       14000 (posterior sample size)
 observations: 4
 predictors:   2

Estimates:
                mean   sd   2.5%   25%   50%   75%   97.5%
(Intercept)    1.3    1.0 -0.5    0.6   1.2   1.9   3.4   
dose_log_g_ml 10.9    5.1  3.4    7.1  10.2  13.9  22.7   
mean_PPD       2.3    0.4  1.5    2.0   2.2   2.5   3.2   
log-posterior -5.6    1.1 -8.4   -6.0  -5.2  -4.8  -4.6   

Diagnostics:
              mcse Rhat n_eff
(Intercept)   0.0  1.0   6000
dose_log_g_ml 0.1  1.0   4565
mean_PPD      0.0  1.0  10222
log-posterior 0.0  1.0   3733

For each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).</code></pre>
<p>The <a href="https://mjskay.github.io/tidybayes/articles/tidybayes.html">tidybayes package</a> offers convenient functions for drawing from the posterior. We’ll also add in our <code>LD50</code> estimate.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">draws &lt;-<span class="st"> </span>m <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span>tidybayes<span class="op">::</span><span class="kw">spread_draws</span>(<span class="st">`</span><span class="dt">(Intercept)</span><span class="st">`</span>, dose_log_g_ml) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">rename</span>(
    <span class="dt">alpha =</span> <span class="st">`</span><span class="dt">(Intercept)</span><span class="st">`</span>,
    <span class="dt">beta =</span> dose_log_g_ml
  ) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">LD50 =</span> <span class="op">-</span>alpha <span class="op">/</span><span class="st"> </span>beta)</code></pre></div>
<figure>
<img src="chapter_03_exercise_11_files/figure-markdown/joint_posterior_plot-1..svg" />
</figure>
<p>The estimates look much the same with the more informative priors as with the uninformative priors. The posterior probability that <span class="math inline">\(\beta &gt; 0\)</span> is:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">draws <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">positive =</span> beta <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">summarise</span>(<span class="kw">mean</span>(positive)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">pull</span>() <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">percent</span>()</code></pre></div>
<pre><code>[1] &quot;100%&quot;</code></pre>
<p>The posterior LD50 estimate (conditional on <span class="math inline">\(\beta &gt; 0\)</span>) is as follows:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">draws <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">filter</span>(beta <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">ggplot</span>() <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">aes</span>(LD50) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">bins =</span> <span class="dv">50</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> <span class="dv">0</span>, <span class="dt">linetype =</span> <span class="st">&#39;dashed&#39;</span>, <span class="dt">colour =</span> <span class="st">&#39;chocolate&#39;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(
    <span class="dt">y =</span> <span class="st">&#39;Count&#39;</span>,
    <span class="dt">title =</span> <span class="st">&#39;Histogram of posterior LD50 estimate&#39;</span>,
    <span class="dt">subtitle =</span> <span class="st">&#39;Conditional on β &gt; 0&#39;</span>
  )</code></pre></div>
<figure>
<img src="chapter_03_exercise_11_files/figure-markdown/unnamed-chunk-2-1..svg" />
</figure>

<div id="disqus_thread"></div>
<script>
/**
 *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
 *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
 */
/*
   var disqus_config = function () {
   this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
   this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
   };
 */
(function() {  // DON'T EDIT BELOW THIS LINE
 var d = document, s = d.createElement('script');

 s.src = '//stappit-github-io.disqus.com/embed.js';

 s.setAttribute('data-timestamp', +new Date());
 (d.head || d.body).appendChild(s);
 })();
</script>

<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

<script id="dsq-count-scr" src="//stappit-github-io.disqus.com/count.js" async></script>
]]></summary>
</entry>
<entry>
    <title>BDA3 Chapter 3 Exercise 10</title>
    <link href="http://stappit.github.io/posts/bda3/chapter_03_exercise_10.html" />
    <id>http://stappit.github.io/posts/bda3/chapter_03_exercise_10.html</id>
    <published>2018-10-21T00:00:00Z</published>
    <updated>2018-10-21T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 class="post-title">BDA3 Chapter 3 Exercise 10</h1>

<div class="card post-meta border-0">
  <div class="card-body post-meta">
    <p class="card-text text-muted text-left">
    Posted on October 21, 2018  by Brian </br>
     Tags: <a href="/tags/bda%20chapter%203.html">bda chapter 3</a>, <a href="/tags/solutions.html">solutions</a>, <a href="/tags/bayes.html">bayes</a>, <a href="/tags/f.html">f</a>, <a href="/tags/scaled%20inverse%20chi2.html">scaled inverse chi2</a>, <a href="/tags/inverse%20chi2.html">inverse chi2</a>, <a href="/tags/chi2.html">chi2</a> </br>
     Category: <a href="/categories/bda3.html">bda3</a> 
    </p>
  </div>
</div>

<p>Here’s my solution to exercise 10, chapter 3, of <a href="https://andrewgelman.com/">Gelman’s</a> <em>Bayesian Data Analysis</em> (BDA), 3rd edition. There are <a href="http://www.stat.columbia.edu/~gelman/book/solutions.pdf">solutions</a> to some of the exercises on the <a href="http://www.stat.columbia.edu/~gelman/book/">book’s webpage</a>.</p>
<!--more-->
<div style="display:none">
<p class="mathjaxWide"><span class="math inline">\(\DeclareMathOperator{\dbinomial}{Binomial}  \DeclareMathOperator{\dbern}{Bernoulli}  \DeclareMathOperator{\dpois}{Poisson}  \DeclareMathOperator{\dnorm}{Normal}  \DeclareMathOperator{\dt}{t}  \DeclareMathOperator{\dcauchy}{Cauchy}  \DeclareMathOperator{\dexponential}{Exp}  \DeclareMathOperator{\duniform}{Uniform}  \DeclareMathOperator{\dgamma}{Gamma}  \DeclareMathOperator{\dinvgamma}{InvGamma}  \DeclareMathOperator{\invlogit}{InvLogit}  \DeclareMathOperator{\dinvchi}{InvChi2}  \DeclareMathOperator{\dsinvchi}{SInvChi2}  \DeclareMathOperator{\dchi}{Chi2}  \DeclareMathOperator{\dnorminvchi}{NormInvChi2}  \DeclareMathOperator{\logit}{Logit}  \DeclareMathOperator{\ddirichlet}{Dirichlet}  \DeclareMathOperator{\dbeta}{Beta}\)</span></p>
</div>
<p>For <span class="math inline">\(j = 1, 2\)</span>, let</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
y_j \mid \mu_j \sigma_j^2 
&amp;\sim
\dnorm(\mu_j, \sigma_j^2)
\\
p(\mu_j, \log \sigma_j^2)
&amp;\propto
1.
\end{align}
\]</span></p>
<p>We show that</p>
<p class="mathjaxWide"><span class="math display">\[
\frac{s_1^2 \sigma_2^2}{s_2^2 \sigma_1^2}
\sim
F(n_1 - 1, n_2 - 1)
.
\]</span></p>
<p>Equation 3.5 in the book shows that <span class="math inline">\(\sigma_j^2 \mid y \sim \dinvchi(n_j - 1, s_j^2)\)</span>. It follows that <span class="math inline">\(\frac{\sigma_j^2}{(n_j - 1) s_j^2} \sim \dinvChi(n_j - 1)\)</span>. Thus, <span class="math inline">\(\frac{(n_j - 1) s_j^2}{\sigma_j^2} \sim \dchi(n_j - 1)\)</span>. The result follows from <a href="https://en.wikipedia.org/wiki/F-distribution#Characterization">the fact</a> that the ratio of two <span class="math inline">\(\chi^2\)</span> random variables (divided by the ratio of their degrees of freedom) has an <span class="math inline">\(F\)</span>-distribution.</p>

<div id="disqus_thread"></div>
<script>
/**
 *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
 *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
 */
/*
   var disqus_config = function () {
   this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
   this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
   };
 */
(function() {  // DON'T EDIT BELOW THIS LINE
 var d = document, s = d.createElement('script');

 s.src = '//stappit-github-io.disqus.com/embed.js';

 s.setAttribute('data-timestamp', +new Date());
 (d.head || d.body).appendChild(s);
 })();
</script>

<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

<script id="dsq-count-scr" src="//stappit-github-io.disqus.com/count.js" async></script>
]]></summary>
</entry>
<entry>
    <title>BDA3 Chapter 3 Exercise 9</title>
    <link href="http://stappit.github.io/posts/bda3/chapter_03_exercise_09.html" />
    <id>http://stappit.github.io/posts/bda3/chapter_03_exercise_09.html</id>
    <published>2018-10-21T00:00:00Z</published>
    <updated>2018-10-21T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 class="post-title">BDA3 Chapter 3 Exercise 9</h1>

<div class="card post-meta border-0">
  <div class="card-body post-meta">
    <p class="card-text text-muted text-left">
    Posted on October 21, 2018  by Brian </br>
     Tags: <a href="/tags/bda%20chapter%203.html">bda chapter 3</a>, <a href="/tags/solutions.html">solutions</a>, <a href="/tags/bayes.html">bayes</a>, <a href="/tags/conjugate%20prior.html">conjugate prior</a>, <a href="/tags/normal.html">normal</a>, <a href="/tags/inverse%20chi2.html">inverse chi2</a>, <a href="/tags/normal%20inverse%20chi2.html">normal inverse chi2</a> </br>
     Category: <a href="/categories/bda3.html">bda3</a> 
    </p>
  </div>
</div>

<p>Here’s my solution to exercise 9, chapter 3, of <a href="https://andrewgelman.com/">Gelman’s</a> <em>Bayesian Data Analysis</em> (BDA), 3rd edition. There are <a href="http://www.stat.columbia.edu/~gelman/book/solutions.pdf">solutions</a> to some of the exercises on the <a href="http://www.stat.columbia.edu/~gelman/book/">book’s webpage</a>.</p>
<!--more-->
<div style="display:none">
<p class="mathjaxWide"><span class="math inline">\(\DeclareMathOperator{\dbinomial}{Binomial}  \DeclareMathOperator{\dbern}{Bernoulli}  \DeclareMathOperator{\dpois}{Poisson}  \DeclareMathOperator{\dnorm}{Normal}  \DeclareMathOperator{\dt}{t}  \DeclareMathOperator{\dcauchy}{Cauchy}  \DeclareMathOperator{\dexponential}{Exp}  \DeclareMathOperator{\duniform}{Uniform}  \DeclareMathOperator{\dgamma}{Gamma}  \DeclareMathOperator{\dinvgamma}{InvGamma}  \DeclareMathOperator{\invlogit}{InvLogit}  \DeclareMathOperator{\dinvchi}{InvChi2}  \DeclareMathOperator{\dnorminvchi}{NormInvChi2}  \DeclareMathOperator{\logit}{Logit}  \DeclareMathOperator{\ddirichlet}{Dirichlet}  \DeclareMathOperator{\dbeta}{Beta}\)</span></p>
</div>
<p>Suppose we have a normal likelihood <span class="math inline">\(y \mid \mu, \sigma \sim \dnorm(\mu, \sigma)\)</span> with conjugate priors</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
  \sigma^2 &amp;\sim \dinvchi(\nu_0, \sigma_0^2)
  \\
  \mu \mid \sigma^2 &amp;\sim \dnorm\left(\mu_0, \frac{\sigma^2}{\kappa_0}\right)
  .
\end{align}
\]</span></p>
<p>We need to show that the posterior is</p>
<p class="mathjaxWide"><span class="math display">\[
\mu, \sigma^2 \mid y \sim \dnorminvchi\left(\mu_n, \frac{\sigma_n^2}{\kappa_n}, \nu_n, \sigma_n^2\right)
\]</span></p>
<p>where</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
  \mu_n &amp;= \frac{\kappa_0}{\kappa_0 + n}\mu_0 + \frac{n}{\kappa_0 + n} \bar y
  \\
  \kappa_n &amp;= \kappa_0 + n
  \\
  \nu_n &amp;= \nu_0 + n
  \\
  \nu_n \sigma_n^2 &amp;= \nu_0 \sigma_0^2 + (n - 1) s^2 + \frac{\kappa_0 n}{\kappa_0 + n}(\bar y - \mu_0)^2
  .
\end{align}
\]</span></p>
<p>Using the calculations on pages 67/68, we can compare the factors in front of the exponentials and the exponents of the exponentials, to see that it is sufficient to show that</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
  \frac{1}{\sigma(\sigma^2)^{-(\nu_n / 2 - 1)}}
  &amp;=
  \frac{1}{\sigma (\sigma^2)^{-(\nu_0 / 2 + 1)} (\sigma^2)^{-n / 2}}
  \\
  \nu_n \sigma_n^2 + \kappa_n (\mu_n - \mu)^2
  &amp;=
  \nu_0 \sigma_0^2 + \kappa_0 (\mu - \mu_0)^2 + (n - 1)s^2 + n(\bar y - \mu)^2
.
\end{align}
\]</span></p>
<p>The first identity is straight forward so we focus on the second. We will expand the left hand side and drop any terms we find that match those on the right. Expanding the LHS in terms on the hyperpriors, we get</p>
<p class="mathjaxWide"><span class="math display">\[
\nu_0 \sigma_0^2 + (n - 1) s^2 + \frac{\kappa_0 n}{\kappa_0 + n}(\bar y - \mu_0)^2
+
(\kappa_0 + n) \left(\frac{\kappa_0}{\kappa_0 + n}\mu_0 + \frac{n}{\kappa_0 + n} \bar y- \mu\right)^2
-
\text{RHS}
\\
=
\frac{\kappa_0 n}{\kappa_0 + n}(\bar y - \mu_0)^2
+
(\kappa_0 + n) \left(\frac{\kappa_0}{\kappa_0 + n}\mu_0 + \frac{n}{\kappa_0 + n} \bar y- \mu\right)^2
-
\kappa_0 (\mu - \mu_0)^2 - n(\bar y - \mu)^2
.
\]</span></p>
<p>Moving the <span class="math inline">\(\kappa_0 + n\)</span> denominator of the second term out of the brackets we obtain</p>
<p class="mathjaxWide"><span class="math display">\[
\frac{\kappa_0 n}{\kappa_0 + n}(\bar y - \mu_0)^2
+
\frac{1}{(\kappa_0 + n)} \left(\kappa_0\mu_0 + n \bar y- (\kappa_0 + n)\mu\right)^2
-
\kappa_0 (\mu - \mu_0)^2 - n(\bar y - \mu)^2
.
\]</span></p>
<p>Simplifying and multiplying out the brackets of the second term gives</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
  \left(\kappa_0\mu_0 + n \bar y- (\kappa_0 + n)\mu\right)^2
  &amp;=
  \left( \kappa_0 (\mu_0 - \bar y) + (\kappa_0 + n)(\bar y - \mu) \right)^2
  \\
  &amp;=
  \kappa_0^2 (\bar y - \mu_0)^2 + (\kappa_0 + n)^2 (\bar y - \mu)^2 + 2\kappa_0(\kappa_0 + n)(\bar y - \mu)(\mu_0 - \bar y)
. 
\end{align}
\]</span></p>
<p>Substituting this back in, we can combine the first terms of each and multiply out all the brackets to get</p>
<span class="math display">\[\begin{align}
  \frac{\kappa_0 n}{\kappa_0 + n}(\bar y - \mu_0)^2
  +
  \frac{1}{\kappa_0 + n} 
  \left( 
    \kappa_0^2 (\bar y - \mu_0)^2 
    + 
    (\kappa_0 + n)^2 (\bar y - \mu)^2 
    + 
    2\kappa_0 (\kappa_0 + n) (\bar y - \mu) (\mu_0 - \bar y) 
  \right)
  \\
  -
  \kappa_0 (\mu - \mu_0)^2 
  - 
  n(\bar y - \mu)^2
  
  \\
  =
  
  \kappa_0 (\bar y - \mu_0)^2 
  + 
  (\kappa_0 + n) (\bar y - \mu)^2 
  + 
  2\kappa_0 (\bar y - \mu) (\mu_0 - \bar y) 
  \\
  -
  \kappa_0 (\mu - \mu_0)^2 
  - 
  n(\bar y - \mu)^2
  
  \\
  =
  
  \color{red}{ \kappa_0 \bar y^2  }
  + 
  \color{blue}{\kappa_0 \mu_0^2}
  - 
  \color{green}{2\kappa_0 \mu_0 \bar y}
  +
  \color{red}{\kappa_0 \bar y^2}
  +
  \color{orange}{\kappa_0 \mu^2}
  -
  \color{black}{2 \kappa_0 \mu \bar y}
  +
  \color{green}{2 \kappa_0 \mu_0 \bar y}
  \\
  +
  \color{black}{2 \kappa_0 \mu \bar y}
  -
  \color{purple}{2 \kappa_0 \mu_0 \mu}
  -
  \color{red}{2 \kappa_0 \bar y^2}
  -
  \color{orange}{\kappa_0 \mu^2}
  -
  \color{blue}{\kappa_0 \mu_0^2}
  +
  \color{purple}{2 \kappa_0 \mu_0 \mu}
  ,
\end{align}\]</span>
<p>which cancel to 0.</p>

<div id="disqus_thread"></div>
<script>
/**
 *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
 *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
 */
/*
   var disqus_config = function () {
   this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
   this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
   };
 */
(function() {  // DON'T EDIT BELOW THIS LINE
 var d = document, s = d.createElement('script');

 s.src = '//stappit-github-io.disqus.com/embed.js';

 s.setAttribute('data-timestamp', +new Date());
 (d.head || d.body).appendChild(s);
 })();
</script>

<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

<script id="dsq-count-scr" src="//stappit-github-io.disqus.com/count.js" async></script>
]]></summary>
</entry>
<entry>
    <title>BDA3 Chapter 3 Exercise 8</title>
    <link href="http://stappit.github.io/posts/bda3/chapter_03_exercise_08.html" />
    <id>http://stappit.github.io/posts/bda3/chapter_03_exercise_08.html</id>
    <published>2018-10-21T00:00:00Z</published>
    <updated>2018-10-21T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 class="post-title">BDA3 Chapter 3 Exercise 8</h1>

<div class="card post-meta border-0">
  <div class="card-body post-meta">
    <p class="card-text text-muted text-left">
    Posted on October 21, 2018  by Brian </br>
     Tags: <a href="/tags/bda%20chapter%203.html">bda chapter 3</a>, <a href="/tags/solutions.html">solutions</a>, <a href="/tags/bayes.html">bayes</a>, <a href="/tags/poisson.html">poisson</a> </br>
     Category: <a href="/categories/bda3.html">bda3</a> 
    </p>
  </div>
</div>

<p>Here’s my solution to exercise 8, chapter 3, of <a href="https://andrewgelman.com/">Gelman’s</a> <em>Bayesian Data Analysis</em> (BDA), 3rd edition. There are <a href="http://www.stat.columbia.edu/~gelman/book/solutions.pdf">solutions</a> to some of the exercises on the <a href="http://www.stat.columbia.edu/~gelman/book/">book’s webpage</a>.</p>
<!--more-->
<div style="display:none">
<p class="mathjaxWide"><span class="math inline">\(\DeclareMathOperator{\dbinomial}{Binomial} \DeclareMathOperator{\dbern}{Bernoulli} \DeclareMathOperator{\dpois}{Poisson} \DeclareMathOperator{\dnorm}{Normal} \DeclareMathOperator{\dt}{t} \DeclareMathOperator{\dcauchy}{Cauchy} \DeclareMathOperator{\dexponential}{Exp} \DeclareMathOperator{\duniform}{Uniform} \DeclareMathOperator{\dgamma}{Gamma} \DeclareMathOperator{\dinvgamma}{InvGamma} \DeclareMathOperator{\invlogit}{InvLogit} \DeclareMathOperator{\logit}{Logit} \DeclareMathOperator{\ddirichlet}{Dirichlet} \DeclareMathOperator{\dbeta}{Beta}\)</span></p>
</div>
<p>You can download the <a href="data/chapter_03_exercise_08.csv">full dataset shown in table 3.3</a>. Let’s load it into a dataframe and select just the residential data, as suggested.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">df0 &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="st">&#39;data/chapter_03_exercise_08.csv&#39;</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(
    <span class="dt">type =</span> <span class="kw">as_factor</span>(
      type, 
      <span class="dt">levels =</span> <span class="kw">c</span>(<span class="st">&#39;residential&#39;</span>, <span class="st">&#39;fairly_busy&#39;</span>, <span class="st">&#39;busy&#39;</span>), 
      <span class="dt">ordered =</span> <span class="ot">TRUE</span>
    ),
    <span class="dt">bikes =</span> <span class="kw">as.integer</span>(bikes),
    <span class="dt">other =</span> <span class="kw">as.integer</span>(other)
  )

df &lt;-<span class="st"> </span>df0 <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">filter</span>(type <span class="op">==</span><span class="st"> &#39;residential&#39;</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(
    <span class="dt">total =</span> bikes <span class="op">+</span><span class="st"> </span>other,
    <span class="dt">bike_fraction =</span> bikes <span class="op">/</span><span class="st"> </span>total,
    <span class="dt">other_fraction =</span> other <span class="op">/</span><span class="st"> </span>total
  )</code></pre></div>
<p>Here are the first few rows with each value of <code>bike_route</code>.</p>
<table class="table table-hover table-striped table-responsive" style="margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
type
</th>
<th style="text-align:left;">
bike_route
</th>
<th style="text-align:right;">
bikes
</th>
<th style="text-align:right;">
other
</th>
<th style="text-align:right;">
total
</th>
<th style="text-align:right;">
bike_fraction
</th>
<th style="text-align:right;">
other_fraction
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
residential
</td>
<td style="text-align:left;">
TRUE
</td>
<td style="text-align:right;">
16
</td>
<td style="text-align:right;">
58
</td>
<td style="text-align:right;">
74
</td>
<td style="text-align:right;">
0.2162162
</td>
<td style="text-align:right;">
0.7837838
</td>
</tr>
<tr>
<td style="text-align:left;">
residential
</td>
<td style="text-align:left;">
TRUE
</td>
<td style="text-align:right;">
9
</td>
<td style="text-align:right;">
90
</td>
<td style="text-align:right;">
99
</td>
<td style="text-align:right;">
0.0909091
</td>
<td style="text-align:right;">
0.9090909
</td>
</tr>
<tr>
<td style="text-align:left;">
residential
</td>
<td style="text-align:left;">
TRUE
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
48
</td>
<td style="text-align:right;">
58
</td>
<td style="text-align:right;">
0.1724138
</td>
<td style="text-align:right;">
0.8275862
</td>
</tr>
<tr>
<td style="text-align:left;">
residential
</td>
<td style="text-align:left;">
FALSE
</td>
<td style="text-align:right;">
12
</td>
<td style="text-align:right;">
113
</td>
<td style="text-align:right;">
125
</td>
<td style="text-align:right;">
0.0960000
</td>
<td style="text-align:right;">
0.9040000
</td>
</tr>
<tr>
<td style="text-align:left;">
residential
</td>
<td style="text-align:left;">
FALSE
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
18
</td>
<td style="text-align:right;">
19
</td>
<td style="text-align:right;">
0.0526316
</td>
<td style="text-align:right;">
0.9473684
</td>
</tr>
<tr>
<td style="text-align:left;">
residential
</td>
<td style="text-align:left;">
FALSE
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
14
</td>
<td style="text-align:right;">
16
</td>
<td style="text-align:right;">
0.1250000
</td>
<td style="text-align:right;">
0.8750000
</td>
</tr>
</tbody>
</table>
<p>We’ll use an uninformative gamma prior with a Poisson likelihood for the counts. The posterior can then be calculated as follows.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">draws &lt;-<span class="st"> </span><span class="dv">10000</span>

shape_prior &lt;-<span class="st"> </span><span class="dv">2</span>
rate_prior &lt;-<span class="st"> </span><span class="dv">0</span>

posterior &lt;-<span class="st"> </span><span class="cf">function</span>(data, <span class="dt">draws =</span> <span class="dv">10000</span>) {
  
  bikes &lt;-<span class="st"> </span>data <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">pull</span>(bikes)
  other &lt;-<span class="st"> </span>data <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">pull</span>(other)
  n &lt;-<span class="st"> </span>data <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">pull</span>(n)
  
  <span class="kw">tibble</span>(<span class="dt">draw =</span> <span class="dv">1</span><span class="op">:</span>draws) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">mutate</span>(
      <span class="dt">theta_bike =</span> <span class="kw">rgamma</span>(draws, bikes, n),
      <span class="dt">theta_other =</span> <span class="kw">rgamma</span>(draws, other, n),
      <span class="dt">mu =</span> <span class="kw">rpois</span>(draws, theta_bike),
      <span class="dt">p =</span> theta_bike <span class="op">/</span><span class="st"> </span>(theta_bike <span class="op">+</span><span class="st"> </span>theta_other)
    )
  
}

posterior_draws &lt;-<span class="st"> </span>df <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(bike_route) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">summarise</span>(
    <span class="dt">bikes =</span> <span class="kw">sum</span>(bikes) <span class="op">+</span><span class="st"> </span>shape_prior,
    <span class="dt">other =</span> <span class="kw">sum</span>(other) <span class="op">+</span><span class="st"> </span>shape_prior,
    <span class="dt">n =</span> <span class="kw">n</span>() <span class="op">+</span><span class="st"> </span>rate_prior
  ) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">nest</span>(<span class="op">-</span>bike_route) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">draws =</span> <span class="kw">map</span>(data, posterior, draws)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">unnest</span>(draws)</code></pre></div>
<p>Plotting posterior predictive draws of <span class="math inline">\(\theta_y\)</span> and <span class="math inline">\(\theta_z\)</span>, we can see that there seems to be quite a difference.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">posterior_draws <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">ggplot</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">aes</span>(mu, <span class="dt">fill =</span> bike_route) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_bar</span>(<span class="dt">position =</span> <span class="st">&#39;identity&#39;</span>, <span class="dt">alpha =</span> <span class="fl">0.75</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(
    <span class="dt">x =</span> <span class="st">&#39;Bike count&#39;</span>,
    <span class="dt">y =</span> <span class="st">&#39;Count&#39;</span>,
    <span class="dt">fill =</span> <span class="st">&#39;Has bike route?&#39;</span>,
    <span class="dt">title =</span> <span class="st">&#39;Posterior expectation of bike count&#39;</span>
  )</code></pre></div>
<figure>
<img src="chapter_03_exercise_08_files/figure-markdown/posterior_predictive_plot-1..svg" />
</figure>
<p>To quantify this difference, we’ll have to match up our posterior draws for <span class="math inline">\(\theta_y\)</span> and <span class="math inline">\(\theta_z\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">difference &lt;-<span class="st"> </span>posterior_draws <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">select</span>(draw, bike_route, mu) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">spread</span>(bike_route, mu) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">difference =</span> <span class="st">`</span><span class="dt">TRUE</span><span class="st">`</span> <span class="op">-</span><span class="st"> `</span><span class="dt">FALSE</span><span class="st">`</span>) </code></pre></div>
<figure>
<img src="chapter_03_exercise_08_files/figure-markdown/difference_plot-1..svg" />
</figure>
<p>The difference <span class="math inline">\(\mu_y - \mu_z\)</span> has the following 95% credible interval:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">difference <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">pull</span>(difference) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">quantile</span>(<span class="dt">probs =</span> <span class="kw">c</span>(<span class="fl">0.05</span>, <span class="fl">0.5</span>, <span class="fl">0.95</span>))</code></pre></div>
<pre><code> 5% 50% 95% 
  6  15  24</code></pre>

<div id="disqus_thread"></div>
<script>
/**
 *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
 *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
 */
/*
   var disqus_config = function () {
   this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
   this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
   };
 */
(function() {  // DON'T EDIT BELOW THIS LINE
 var d = document, s = d.createElement('script');

 s.src = '//stappit-github-io.disqus.com/embed.js';

 s.setAttribute('data-timestamp', +new Date());
 (d.head || d.body).appendChild(s);
 })();
</script>

<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

<script id="dsq-count-scr" src="//stappit-github-io.disqus.com/count.js" async></script>
]]></summary>
</entry>
<entry>
    <title>BDA3 Chapter 3 Exercise 7</title>
    <link href="http://stappit.github.io/posts/bda3/chapter_03_exercise_07.html" />
    <id>http://stappit.github.io/posts/bda3/chapter_03_exercise_07.html</id>
    <published>2018-10-21T00:00:00Z</published>
    <updated>2018-10-21T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 class="post-title">BDA3 Chapter 3 Exercise 7</h1>

<div class="card post-meta border-0">
  <div class="card-body post-meta">
    <p class="card-text text-muted text-left">
    Posted on October 21, 2018  by Brian </br>
     Tags: <a href="/tags/bda%20chapter%203.html">bda chapter 3</a>, <a href="/tags/solutions.html">solutions</a>, <a href="/tags/bayes.html">bayes</a>, <a href="/tags/poisson.html">poisson</a>, <a href="/tags/binomial.html">binomial</a>, <a href="/tags/unsolved.html">unsolved</a> </br>
     Category: <a href="/categories/bda3.html">bda3</a> 
    </p>
  </div>
</div>

<p>Here’s my solution to exercise 7, chapter 3, of <a href="https://andrewgelman.com/">Gelman’s</a> <em>Bayesian Data Analysis</em> (BDA), 3rd edition. There are <a href="http://www.stat.columbia.edu/~gelman/book/solutions.pdf">solutions</a> to some of the exercises on the <a href="http://www.stat.columbia.edu/~gelman/book/">book’s webpage</a>.</p>
<!--more-->
<div style="display:none">
<p class="mathjaxWide"><span class="math inline">\(\DeclareMathOperator{\dbinomial}{Binomial} \DeclareMathOperator{\dbern}{Bernoulli} \DeclareMathOperator{\dpois}{Poisson} \DeclareMathOperator{\dnorm}{Normal} \DeclareMathOperator{\dt}{t} \DeclareMathOperator{\dcauchy}{Cauchy} \DeclareMathOperator{\dexponential}{Exp} \DeclareMathOperator{\duniform}{Uniform} \DeclareMathOperator{\dgamma}{Gamma} \DeclareMathOperator{\dinvgamma}{InvGamma} \DeclareMathOperator{\invlogit}{InvLogit} \DeclareMathOperator{\logit}{Logit} \DeclareMathOperator{\ddirichlet}{Dirichlet} \DeclareMathOperator{\dbeta}{Beta}\)</span></p>
</div>
<p>Suppose we observe <span class="math inline">\(b\)</span> bikes and <span class="math inline">\(v\)</span> other vehicles passing a section of road within an hour. We can model the counts as Poisson distributed</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
  b \mid \theta_b &amp;\sim \dpois(\theta_b)
  \\
  v \mid \theta_v &amp;\sim \dpois(\theta_v)
\end{align}
\]</span></p>
<p>or as binomial distributed</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
  b \mid n, p &amp;\sim \dbinomial(n, p)
\end{align}
\]</span></p>
<p>where <span class="math inline">\(n\)</span> is the number of trials and <span class="math inline">\(p\)</span> is the probability of observing a bike. Let</p>
<p class="mathjaxWide"><span class="math display">\[
p := \frac{\theta_b}{\theta_b + \theta_v}
.
\]</span></p>
<p>We are supposed to show that this definition of <span class="math inline">\(p\)</span> gives the two models the same likelihood, but I’m stuck. At best I can show that the expectations are different</p>
<p class="mathjaxWide"><span class="math display">\[
\mathbb E (b \mid \theta_b) = \theta_b
\\
\mathbb E (b \mid n, p) = np = n\frac{\theta_b}{\theta_b + \theta_v}
\]</span></p>
<p>which suggests the conditioning should be done differently.</p>

<div id="disqus_thread"></div>
<script>
/**
 *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
 *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
 */
/*
   var disqus_config = function () {
   this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
   this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
   };
 */
(function() {  // DON'T EDIT BELOW THIS LINE
 var d = document, s = d.createElement('script');

 s.src = '//stappit-github-io.disqus.com/embed.js';

 s.setAttribute('data-timestamp', +new Date());
 (d.head || d.body).appendChild(s);
 })();
</script>

<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

<script id="dsq-count-scr" src="//stappit-github-io.disqus.com/count.js" async></script>
]]></summary>
</entry>
<entry>
    <title>BDA3 Chapter 3 Exercise 6</title>
    <link href="http://stappit.github.io/posts/bda3/chapter_03_exercise_06.html" />
    <id>http://stappit.github.io/posts/bda3/chapter_03_exercise_06.html</id>
    <published>2018-10-07T00:00:00Z</published>
    <updated>2018-10-07T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 class="post-title">BDA3 Chapter 3 Exercise 6</h1>

<div class="card post-meta border-0">
  <div class="card-body post-meta">
    <p class="card-text text-muted text-left">
    Posted on October  7, 2018  by Brian </br>
     Tags: <a href="/tags/bda%20chapter%203.html">bda chapter 3</a>, <a href="/tags/solutions.html">solutions</a>, <a href="/tags/bayes.html">bayes</a>, <a href="/tags/hierarchical.html">hierarchical</a>, <a href="/tags/grid%20approximation.html">grid approximation</a>, <a href="/tags/binomial.html">binomial</a>, <a href="/tags/log-factorial.html">log-factorial</a>, <a href="/tags/stirling%27s%20approximation.html">stirling&#39;s approximation</a> </br>
     Category: <a href="/categories/bda3.html">bda3</a> 
    </p>
  </div>
</div>

<p>Here’s my solution to exercise 6, chapter 3, of <a href="https://andrewgelman.com/">Gelman’s</a> <em>Bayesian Data Analysis</em> (BDA), 3rd edition. There are <a href="http://www.stat.columbia.edu/~gelman/book/solutions.pdf">solutions</a> to some of the exercises on the <a href="http://www.stat.columbia.edu/~gelman/book/">book’s webpage</a>.</p>
<!--more-->
<div style="display:none">
<p class="mathjaxWide"><span class="math inline">\(\DeclareMathOperator{\dbinomial}{Binomial} \DeclareMathOperator{\dbern}{Bernoulli} \DeclareMathOperator{\dpois}{Poisson} \DeclareMathOperator{\dnorm}{Normal} \DeclareMathOperator{\dt}{t} \DeclareMathOperator{\dcauchy}{Cauchy} \DeclareMathOperator{\dexponential}{Exp} \DeclareMathOperator{\duniform}{Uniform} \DeclareMathOperator{\dgamma}{Gamma} \DeclareMathOperator{\dinvgamma}{InvGamma} \DeclareMathOperator{\invlogit}{InvLogit} \DeclareMathOperator{\logit}{Logit} \DeclareMathOperator{\ddirichlet}{Dirichlet} \DeclareMathOperator{\dbeta}{Beta}\)</span></p>
</div>
<h2 id="theory">Theory</h2>
<p>Suppose we have binomial data where both the number of trials <span class="math inline">\(N\)</span> and the success rate <span class="math inline">\(\theta\)</span> are unknown and to be estimated. <a href="http://pluto.huji.ac.il/~galelidan/52558/Material/Raftery.pdf">Rafferty</a> suggests the following hierarchical model:</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
  y \mid N, \theta &amp;\sim \dbinomial(N, \theta)
  \\
  \theta &amp;\sim \duniform(0, 1)
  \\
  N \mid \mu &amp;\sim \dpois(\mu)
\end{align}
\]</span></p>
<p>where the parameter <span class="math inline">\(\mu\)</span> is also unknown. Instead of putting a prior on <span class="math inline">\(\mu\)</span>, it is suggested to define</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
  \lambda &amp;:= \mu \theta
  \\
  p(\lambda) &amp;\propto \frac{1}{\lambda}
\end{align}
\]</span></p>
<p>and to put a prior on <span class="math inline">\(\lambda\)</span>. This is advantageous since <span class="math inline">\(\lambda\)</span> is the expected number of successes, which is easier to reason about than <span class="math inline">\(N\)</span> since we can actually observe it. The hierarchical structure allows us to be vague about the prior magnitude of <span class="math inline">\(N\)</span>; an unconditional Poisson prior on <span class="math inline">\(N\)</span> could be a good idea if we had reason to believe it lies in the plausible range as defined by that prior. It is improper since its integral is the logarithm evaluated where it goes to infinity.</p>
<p>Let’s find the corresponding prior on <span class="math inline">\(N\)</span>. First note that</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
  p(\mu, \theta)
  &amp;\propto
  p(\lambda, \theta) \cdot
  \begin{vmatrix}
    \theta &amp; \mu \\
    0 &amp; 1
  \end{vmatrix}
  \\
  &amp;=
  \frac{1}{\lambda}
  \cdot
  \theta
  \\
  &amp;=
  \frac{1}{\mu}.
\end{align}
\]</span></p>
<p>It follows that <span class="math inline">\(p(\mu) \propto \mu^{-1}\)</span>. Thus,</p>
<span class="math display">\[\begin{align}
  p(N, \mu)
  &amp;=
  p(N \mid \mu) p(\mu)
  \\
  &amp;\propto
  \frac{\mu^N e^{-\mu}}{N!}\frac{1}{\mu}
  \\
  &amp;=
  \frac{\mu^{N-1} e^{-\mu}}{N!}
  
\end{align}\]</span>
<p>From the definition of the <a href="https://en.wikipedia.org/wiki/Gamma_function#Main_definition">Gamma function</a>, it follows that</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
  p(N)
  &amp;\propto
  \int_0^\infty \frac{\mu^{N-1} e^{-\mu}}{N!} d\mu
  \\
  &amp;=
  \frac{1}{N!} \Gamma (N)
  \\
  &amp;=
  \frac{1}{N!} (N - 1)!
  \\
  &amp;=
  \frac{1}{N}
  .
\end{align}
\]</span></p>
<p>The joint posterior is</p>
<p class="mathjaxWide"><span class="math display">\[
p(N, \theta \mid y)
\propto
\frac{1}{N}\cdot \prod_{i = 1}^n \binom{N}{y_i} \cdot \theta^{\sum_{i = 1}^n y_i} (1 - \theta)^{nN - \sum_{i = 1}^n y_i}
.
\]</span></p>
<p>and the marginal posterior is</p>
<p class="mathjaxWide"><span class="math display">\[
\begin{align}
  p(N \mid y)
  &amp;=
  \int_0^1
  p(N, \theta \mid y) d\theta
  \\
  &amp;\propto
  \frac{1}{N}\cdot \prod_{i = 1}^n \binom{N}{y_i} 
  \int_0^1 \theta^{\sum_{i = 1}^n y_i} (1 - \theta)^{nN - \sum_{i = 1}^n y_i} d\theta
  \\
  &amp;=
  \frac{1}{N}\cdot \prod_{i = 1}^n \binom{N}{y_i} 
  \cdot
  \frac{\Gamma(1 + \sum_1^n y_i)\Gamma(1 + nN - \sum_1^n y_i)}{\Gamma(2 + nN)}
  \\
  &amp;=
  \frac{1}{N}\cdot \prod_{i = 1}^n \binom{N}{y_i} 
  \cdot
  \frac{\left( \sum_1^n y_i \right)!\left( nN - \sum_1^n y_i \right)!}{(1 + nN)!}
\end{align}
\]</span></p>
<h2 id="example">Example</h2>
<p>Suppose we observe the following counts.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">counts &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">count =</span> <span class="kw">c</span>(<span class="dv">53</span>, <span class="dv">57</span>, <span class="dv">66</span>, <span class="dv">67</span>, <span class="dv">72</span>))</code></pre></div>
<p>We’d like to estimate the probability that <span class="math inline">\(N &gt; 100\)</span>. Stan can’t help us here because the algorithm it uses requires parameters to be continuous. I learned a lot from approximating the posterior on a suitable grid, so let’s do that.</p>
<p>Here’s our first computational problem: overflow.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">counts<span class="op">$</span>count <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">sum</span>() <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">factorial</span>() <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">log</span>()</code></pre></div>
<pre><code>Warning in factorial(.): value out of range in &#39;gammafn&#39;

[1] Inf</code></pre>
<p>The factorial function gets big very fast, but computers can only deal with a finite range. John Cook has a <a href="https://www.johndcook.com/blog/2010/08/16/how-to-compute-log-factorial/">very useful post</a> with several solutions to get around this problem. I couldn’t find a definitive answer on how to make efficient hash tables in base R, so I’ll just use vectors.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># slow but exact</span>
logfactorial0 &lt;-<span class="st"> </span><span class="cf">function</span>(k)
  <span class="dv">1</span><span class="op">:</span>k <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">log</span>() <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">sum</span>()

<span class="co"># precalculated values</span>
lookup &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="dv">256</span> <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">map</span>(logfactorial0)

<span class="co"># stirling approximation</span>
stirling &lt;-<span class="st"> </span><span class="cf">function</span>(k) 
  (k <span class="op">+</span><span class="st"> </span><span class="fl">0.5</span>) <span class="op">*</span><span class="st"> </span><span class="kw">log</span>(k) <span class="op">-</span><span class="st"> </span>k <span class="op">+</span><span class="st"> </span><span class="fl">0.5</span> <span class="op">*</span><span class="st"> </span><span class="kw">log</span>(<span class="dv">2</span> <span class="op">*</span><span class="st"> </span>pi) <span class="op">+</span><span class="st"> </span><span class="dv">1</span> <span class="op">/</span><span class="st"> </span>(<span class="dv">12</span> <span class="op">*</span><span class="st"> </span>k)

<span class="co"># &#39;efficient&#39; implementation</span>
logfactorial &lt;-<span class="st"> </span><span class="cf">function</span>(k) {
  <span class="cf">if</span> (k <span class="op">==</span><span class="st"> </span><span class="dv">0</span>) {
    result &lt;-<span class="st"> </span><span class="dv">0</span>
  } <span class="cf">else</span> <span class="cf">if</span> (k <span class="op">&lt;=</span><span class="st"> </span><span class="dv">256</span>) {
    result &lt;-<span class="st"> </span>lookup[[k]]
  } <span class="cf">else</span> {
    result &lt;-<span class="st"> </span><span class="kw">stirling</span>(k)
  }
  <span class="kw">return</span>(result)
}</code></pre></div>
<p>To convince ourselves our functions work correctly, we can look at a few examples.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="dv">1</span><span class="op">:</span><span class="dv">4</span> <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">map</span>(logfactorial) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">map</span>(exp)</code></pre></div>
<pre><code>[[1]]
[1] 1

[[2]]
[1] 2

[[3]]
[1] 6

[[4]]
[1] 24</code></pre>
<p>Now we can calculate the log of the factorial. The factorial itself though is still too large to be represented as a finite number.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">counts<span class="op">$</span>count <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">sum</span>() <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">logfactorial</span>()</code></pre></div>
<pre><code>[1] 1500.856</code></pre>
<p>The log-factorial is enough for us to calculate the (unnormalised) log-density.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">util &lt;-<span class="st"> </span><span class="cf">function</span>(N)
  counts <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">mutate</span>(
      <span class="dt">logfac =</span> <span class="kw">map</span>(count, logfactorial) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">unlist</span>(),
      <span class="dt">nlogfac =</span> <span class="kw">map</span>(N <span class="op">-</span><span class="st"> </span>count, logfactorial) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">unlist</span>(),
      <span class="dt">logperm =</span> <span class="kw">logfactorial</span>(N) <span class="op">-</span><span class="st"> </span>logfac <span class="op">-</span><span class="st"> </span>nlogfac
    ) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">summarise</span>(
      <span class="dt">n =</span> <span class="kw">n</span>(),
      <span class="dt">sum_count =</span> <span class="kw">sum</span>(count),
      <span class="dt">sum_logperm =</span> <span class="kw">sum</span>(logperm),
      <span class="dt">max_count =</span> <span class="kw">max</span>(count)
    ) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">as.list</span>()
  
joint_posterior &lt;-<span class="st"> </span><span class="cf">function</span>(N, theta) {
  p &lt;-<span class="st"> </span><span class="kw">util</span>(N)
  <span class="cf">if</span> (N <span class="op">&lt;</span><span class="st"> </span>p<span class="op">$</span>max_count) {
    density &lt;-<span class="st"> </span><span class="dv">0</span>
  } <span class="cf">else</span> {
    logdensity &lt;-<span class="st"> </span><span class="op">-</span><span class="kw">log</span>(N) <span class="op">+</span><span class="st"> </span>p<span class="op">$</span>sum_logperm <span class="op">+</span><span class="st"> </span>p<span class="op">$</span>sum_count <span class="op">*</span><span class="st"> </span><span class="kw">log</span>(theta) <span class="op">+</span><span class="st"> </span>(p<span class="op">$</span>n <span class="op">*</span><span class="st"> </span>N <span class="op">-</span><span class="st"> </span>p<span class="op">$</span>sum_count) <span class="op">*</span><span class="st"> </span><span class="kw">log</span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>theta)
    density &lt;-<span class="st"> </span><span class="kw">exp</span>(logdensity)
  }
  <span class="kw">return</span>(density)
}</code></pre></div>
<p>In order to approximate the posterior on a grid, we need a suitable grid. It’s only worth calculating this where the posterior is non-negligible. For a given <span class="math inline">\(N\)</span>, the posterior can only have non-negligible density when <span class="math inline">\(\theta N\)</span> is near the range 50-75. We’ll broaden that a bit and restrict it to be in the range 35-90.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">grid_joint &lt;-<span class="st"> </span><span class="kw">expand.grid</span>(
    <span class="dt">N =</span> <span class="kw">max</span>(counts<span class="op">$</span>count)<span class="op">:</span><span class="dv">5000</span>,
    <span class="dt">theta =</span> <span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="fl">0.01</span>)
  ) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">as_tibble</span>() <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">filter</span>(<span class="dv">35</span> <span class="op">&lt;</span><span class="st"> </span>N <span class="op">*</span><span class="st"> </span>theta <span class="op">&amp;</span><span class="st"> </span>N <span class="op">*</span><span class="st"> </span>theta <span class="op">&lt;</span><span class="st"> </span><span class="dv">90</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(
    <span class="dt">unnormalised_density =</span> <span class="kw">map2</span>(N, theta, joint_posterior) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">unlist</span>(),
    <span class="dt">normalising_constant =</span> <span class="kw">sum</span>(unnormalised_density),
    <span class="dt">density =</span> unnormalised_density <span class="op">/</span><span class="st"> </span>normalising_constant
  ) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">select</span>(<span class="op">-</span>unnormalised_density, <span class="op">-</span>normalising_constant)</code></pre></div>
<figure>
<img src="chapter_03_exercise_06_files/figure-markdown/joint_plot-1..svg" />
</figure>
<p>We can also calculate the marginal posterior of <span class="math inline">\(N\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">marginal &lt;-<span class="st"> </span>grid_joint <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(N) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">summarise</span>(<span class="dt">mass =</span> <span class="kw">sum</span>(density)) </code></pre></div>
<figure>
<img src="chapter_03_exercise_06_files/figure-markdown/marginal_plot-1..svg" />
</figure>
<p>The posterior probability that <span class="math inline">\(N &gt; 100\)</span> is then:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">grid_joint <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">filter</span>(N <span class="op">&gt;</span><span class="st"> </span><span class="dv">100</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">summarise</span>(<span class="dt">mass =</span> <span class="kw">sum</span>(density)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">pull</span>() <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">percent</span>()</code></pre></div>
<pre><code>[1] &quot;95.6%&quot;</code></pre>

<div id="disqus_thread"></div>
<script>
/**
 *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
 *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
 */
/*
   var disqus_config = function () {
   this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
   this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
   };
 */
(function() {  // DON'T EDIT BELOW THIS LINE
 var d = document, s = d.createElement('script');

 s.src = '//stappit-github-io.disqus.com/embed.js';

 s.setAttribute('data-timestamp', +new Date());
 (d.head || d.body).appendChild(s);
 })();
</script>

<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

<script id="dsq-count-scr" src="//stappit-github-io.disqus.com/count.js" async></script>
]]></summary>
</entry>
<entry>
    <title>BDA3 Chapter 3 Exercise 5</title>
    <link href="http://stappit.github.io/posts/bda3/chapter_03_exercise_05.html" />
    <id>http://stappit.github.io/posts/bda3/chapter_03_exercise_05.html</id>
    <published>2018-10-06T00:00:00Z</published>
    <updated>2018-10-06T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 class="post-title">BDA3 Chapter 3 Exercise 5</h1>

<div class="card post-meta border-0">
  <div class="card-body post-meta">
    <p class="card-text text-muted text-left">
    Posted on October  6, 2018  by Brian </br>
     Tags: <a href="/tags/bda%20chapter%203.html">bda chapter 3</a>, <a href="/tags/solutions.html">solutions</a>, <a href="/tags/bayes.html">bayes</a>, <a href="/tags/rounding%20error.html">rounding error</a>, <a href="/tags/marginal%20posterior.html">marginal posterior</a>, <a href="/tags/measurement%20error.html">measurement error</a>, <a href="/tags/noninformative%20prior.html">noninformative prior</a>, <a href="/tags/normal.html">normal</a> </br>
     Category: <a href="/categories/bda3.html">bda3</a> 
    </p>
  </div>
</div>

<p>Here’s my solution to exercise 5, chapter 3, of <a href="https://andrewgelman.com/">Gelman’s</a> <em>Bayesian Data Analysis</em> (BDA), 3rd edition. There are <a href="http://www.stat.columbia.edu/~gelman/book/solutions.pdf">solutions</a> to some of the exercises on the <a href="http://www.stat.columbia.edu/~gelman/book/">book’s webpage</a>.</p>
<!--more-->
<div style="display:none">
<p class="mathjaxWide"><span class="math inline">\(\DeclareMathOperator{\dbinomial}{Binomial} \DeclareMathOperator{\dbern}{Bernoulli} \DeclareMathOperator{\dpois}{Poisson} \DeclareMathOperator{\dnorm}{Normal} \DeclareMathOperator{\dt}{t} \DeclareMathOperator{\dcauchy}{Cauchy} \DeclareMathOperator{\dexponential}{Exp} \DeclareMathOperator{\duniform}{Uniform} \DeclareMathOperator{\dgamma}{Gamma} \DeclareMathOperator{\dinvgamma}{InvGamma} \DeclareMathOperator{\invlogit}{InvLogit} \DeclareMathOperator{\logit}{Logit} \DeclareMathOperator{\ddirichlet}{Dirichlet} \DeclareMathOperator{\dbeta}{Beta}\)</span></p>
</div>
<p>Suppose we weigh an object 5 times with measurements</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">measurements &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">10</span>, <span class="dv">10</span>, <span class="dv">12</span>, <span class="dv">11</span>, <span class="dv">9</span>)</code></pre></div>
<p>all rounded to the nearest kilogram. Assuming the unrounded measurements are normally distributed, we wish to estimate the weight of the object. We will use the uniform non-informative prior <span class="math inline">\(p(\mu, \log \sigma) \propto 1\)</span>.</p>
<p>First, let’s assume the measurments are not rounded. Then the marginal posterior mean is <span class="math inline">\(\mu \mid y \sim t_{n - 1}(\bar y, s / \sqrt{n}) = t_4(10.4, 0.51)\)</span>.</p>
<figure>
<img src="chapter_03_exercise_05_files/figure-markdown/mpm_plot-1..svg" />
</figure>
<p>Now, let’s find the posterior assuming rounded measurements. The probability of getting the rounded measurements <span class="math inline">\(y\)</span> is</p>
<p class="mathjaxWide"><span class="math display">\[
p(y \mid \mu, \sigma) = \prod_{i = 1}^n \Phi_{\mu, \sigma} (y_i + 0.5) - \Phi_{\mu, \sigma} (y_i - 0.5)
\]</span></p>
<p>where <span class="math inline">\(\Phi_{\mu, \sigma}\)</span> is the CDF of the <span class="math inline">\(\dnorm(\mu, \sigma)\)</span> distribution. This implies that the posterior is</p>
<p class="mathjaxWide"><span class="math display">\[
p(\mu, \sigma \mid y) \propto \frac{1}{\sigma^2} \prod_{i = 1}^n \Phi_{\mu, \sigma} (y_i + 0.5) - \Phi_{\mu, \sigma} (y_i - 0.5) .
\]</span></p>
<p>Calculating this marginal posterior mean is pretty difficult, so we’ll use <a href="http://mc-stan.org/">Stan</a> to draw samples. My <a href="src/ex_03_05.stan">first attempt</a> at writing the model was a direct translation of the maths above. However, it doesn’t allow us to infer the unrounded values, as required in part d. The model can be expressed differently by considering the unrounded values as uniformly distributed around the rounded values, i.e. <span class="math inline">\(z_i \sim \duniform (y_i - 0.5, y_i + 0.5)\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">model &lt;-<span class="st"> </span>rstan<span class="op">::</span><span class="kw">stan_model</span>(<span class="st">&#39;src/ex_03_05_d.stan&#39;</span>)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">model</code></pre></div>
<pre><code>S4 class stanmodel &#39;ex_03_05_d&#39; coded as follows:
data {
  int&lt;lower = 1&gt; n;
  vector[n] y; // rounded measurements
}

parameters {
  real mu; // &#39;true&#39; weight of the object
  real&lt;lower = 0&gt; sigma; // measurement error
  vector&lt;lower = -0.5, upper = 0.5&gt;[n] err; // rounding error
}

transformed parameters {
  // unrounded values are the rounded values plus some rounding error
  vector[n] z = y + err; // unrounded measurements
}

model {
  target += -2 * log(sigma); // prior
  z ~ normal(mu, sigma);
  // other parameters are uniform
} </code></pre>
<p>Note that Stan assumes parameters are uniform on their range unless specified otherwise.</p>
<p>Let’s also load a model that assumes the measurements are unrounded.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">model_unrounded &lt;-<span class="st"> </span>rstan<span class="op">::</span><span class="kw">stan_model</span>(<span class="st">&#39;src/ex_03_05_unrounded.stan&#39;</span>)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">model_unrounded</code></pre></div>
<pre><code>S4 class stanmodel &#39;ex_03_05_unrounded&#39; coded as follows:
data {
  int&lt;lower = 1&gt; n;
  vector[n] y; 
}

parameters {
  real mu; 
  real&lt;lower = 0&gt; sigma; 
}

model {
  target += -2 * log(sigma); 
  y ~ normal(mu, sigma);
} </code></pre>
<p>Now we can fit the models to the data.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">data  =<span class="st"> </span><span class="kw">list</span>(
  <span class="dt">n =</span> <span class="kw">length</span>(measurements),
  <span class="dt">y =</span> measurements
)
 
fit &lt;-<span class="st"> </span>model <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span>rstan<span class="op">::</span><span class="kw">sampling</span>(
    <span class="dt">data =</span> data,
    <span class="dt">warmup =</span> <span class="dv">1000</span>,
    <span class="dt">iter =</span> <span class="dv">5000</span>
  ) 

fit_unrounded &lt;-<span class="st"> </span>model_unrounded <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span>rstan<span class="op">::</span><span class="kw">sampling</span>(
    <span class="dt">data =</span> data,
    <span class="dt">warmup =</span> <span class="dv">1000</span>,
    <span class="dt">iter =</span> <span class="dv">5000</span>
  ) </code></pre></div>
<p>We’ll also need some draws from the posteriors to make our comparisons.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">draws &lt;-<span class="st"> </span>fit <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span>tidybayes<span class="op">::</span><span class="kw">spread_draws</span>(mu, sigma, z[index]) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="co"># spread out z&#39;s so that</span>
<span class="st">  </span><span class="co"># there is one row per draw</span>
<span class="st">  </span><span class="kw">ungroup</span>() <span class="op">%&gt;%</span><span class="st">  </span>
<span class="st">  </span><span class="kw">mutate</span>(
    <span class="dt">index =</span> <span class="kw">paste0</span>(<span class="st">&#39;z&#39;</span>, <span class="kw">as.character</span>(index)),
    <span class="dt">model =</span> <span class="st">&#39;rounded&#39;</span>
  ) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">spread</span>(index, z)

draws_unrounded &lt;-<span class="st"> </span>fit_unrounded <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span>tidybayes<span class="op">::</span><span class="kw">spread_draws</span>(mu, sigma) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">model =</span> <span class="st">&#39;unrounded&#39;</span>) 

draws_all &lt;-<span class="st"> </span>draws <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">bind_rows</span>(draws_unrounded)</code></pre></div>
<table class="table table-striped table-hover table-responsive" style="margin-left: auto; margin-right: auto;">
<caption>
First few draws from each model
</caption>
<thead>
<tr>
<th style="text-align:right;">
mu
</th>
<th style="text-align:right;">
sigma
</th>
<th style="text-align:left;">
model
</th>
<th style="text-align:right;">
z1
</th>
<th style="text-align:right;">
z2
</th>
<th style="text-align:right;">
z3
</th>
<th style="text-align:right;">
z4
</th>
<th style="text-align:right;">
z5
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
9.687131
</td>
<td style="text-align:right;">
1.4458022
</td>
<td style="text-align:left;">
rounded
</td>
<td style="text-align:right;">
9.72795
</td>
<td style="text-align:right;">
10.265646
</td>
<td style="text-align:right;">
11.94897
</td>
<td style="text-align:right;">
11.06143
</td>
<td style="text-align:right;">
9.116118
</td>
</tr>
<tr>
<td style="text-align:right;">
10.517837
</td>
<td style="text-align:right;">
0.5648234
</td>
<td style="text-align:left;">
rounded
</td>
<td style="text-align:right;">
10.33178
</td>
<td style="text-align:right;">
9.797315
</td>
<td style="text-align:right;">
11.54332
</td>
<td style="text-align:right;">
10.60547
</td>
<td style="text-align:right;">
9.153462
</td>
</tr>
<tr>
<td style="text-align:right;">
11.381591
</td>
<td style="text-align:right;">
2.1127547
</td>
<td style="text-align:left;">
rounded
</td>
<td style="text-align:right;">
10.09489
</td>
<td style="text-align:right;">
10.176548
</td>
<td style="text-align:right;">
12.44288
</td>
<td style="text-align:right;">
10.92263
</td>
<td style="text-align:right;">
8.700090
</td>
</tr>
<tr>
<td style="text-align:right;">
9.965578
</td>
<td style="text-align:right;">
1.2079231
</td>
<td style="text-align:left;">
unrounded
</td>
<td style="text-align:right;">
NA
</td>
<td style="text-align:right;">
NA
</td>
<td style="text-align:right;">
NA
</td>
<td style="text-align:right;">
NA
</td>
<td style="text-align:right;">
NA
</td>
</tr>
<tr>
<td style="text-align:right;">
10.062636
</td>
<td style="text-align:right;">
1.2355530
</td>
<td style="text-align:left;">
unrounded
</td>
<td style="text-align:right;">
NA
</td>
<td style="text-align:right;">
NA
</td>
<td style="text-align:right;">
NA
</td>
<td style="text-align:right;">
NA
</td>
<td style="text-align:right;">
NA
</td>
</tr>
<tr>
<td style="text-align:right;">
9.922399
</td>
<td style="text-align:right;">
1.7637659
</td>
<td style="text-align:left;">
unrounded
</td>
<td style="text-align:right;">
NA
</td>
<td style="text-align:right;">
NA
</td>
<td style="text-align:right;">
NA
</td>
<td style="text-align:right;">
NA
</td>
<td style="text-align:right;">
NA
</td>
</tr>
</tbody>
</table>
<p>The contour plots look very similar but with <span class="math inline">\(\sigma\)</span> shifted upward when we treat the observations as unrounded measurements. This is contrary to my intuition about what should happen: by introducing uncertainty into our measurments, I would have thought we’d see more uncertainty in our parameter estimates.</p>
<figure>
<img src="chapter_03_exercise_05_files/figure-markdown/contour_plot-1..svg" />
</figure>
<p>The density for <span class="math inline">\(\mu \mid y\)</span> look much the same in both models. This is expected because the rounded measurement is the mean of all possible unrounded measurements.</p>
<figure>
<img src="chapter_03_exercise_05_files/figure-markdown/mu_plot-1..svg" />
</figure>
<p>The marginal posterior for <span class="math inline">\(\sigma\)</span> again shows a decrease when taking rounding error into account. I’m not sure why that would happen.</p>
<figure>
<img src="chapter_03_exercise_05_files/figure-markdown/sigma_plot-1..svg" />
</figure>
<table class="table table-striped table-hover table-responsive" style="margin-left: auto; margin-right: auto;">
<caption>
Quantiles for σ | y
</caption>
<thead>
<tr>
<th style="text-align:left;">
model
</th>
<th style="text-align:right;">
5%
</th>
<th style="text-align:right;">
50%
</th>
<th style="text-align:right;">
95%
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
rounded
</td>
<td style="text-align:right;">
0.6044134
</td>
<td style="text-align:right;">
1.025455
</td>
<td style="text-align:right;">
2.073181
</td>
</tr>
<tr>
<td style="text-align:left;">
unrounded
</td>
<td style="text-align:right;">
0.6804532
</td>
<td style="text-align:right;">
1.095101
</td>
<td style="text-align:right;">
2.133040
</td>
</tr>
</tbody>
</table>
<p>Finally, let’s calculate the posterior for <span class="math inline">\(\theta := (z_1 - z_2)^2\)</span> (assuming we observe rounded measurements).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sims &lt;-<span class="st"> </span>draws <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">theta =</span> (z1 <span class="op">-</span><span class="st"> </span>z2)<span class="op">^</span><span class="dv">2</span>) </code></pre></div>
<figure>
<img src="chapter_03_exercise_05_files/figure-markdown/sims_plot-1..svg" />
</figure>
<p>There is a lot of mass near 0 because the observed rounded measurments are the same for <span class="math inline">\(z_1\)</span> and <span class="math inline">\(z_2\)</span>. The probability density is also entirely less than 1 because the rounding is off by at most 0.5 in any direction.</p>

<div id="disqus_thread"></div>
<script>
/**
 *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
 *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
 */
/*
   var disqus_config = function () {
   this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
   this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
   };
 */
(function() {  // DON'T EDIT BELOW THIS LINE
 var d = document, s = d.createElement('script');

 s.src = '//stappit-github-io.disqus.com/embed.js';

 s.setAttribute('data-timestamp', +new Date());
 (d.head || d.body).appendChild(s);
 })();
</script>

<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

<script id="dsq-count-scr" src="//stappit-github-io.disqus.com/count.js" async></script>
]]></summary>
</entry>

</feed>
